---
---

@inproceedings{shi2021deeply,
  title={Deeply-debiased off-policy interval estimation},
  author={Shi, Chengchun and Wan, Runzhe and Chernozhukov, Victor and Song, Rui},
  booktitle={International Conference on Machine Learning},
  pages={9580--9591},
  year={2021},
  organization={PMLR}
}


@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{thomas2015safe,
  title={Safe reinforcement learning},
  author={Thomas, Philip S},
  journal={Doctoral Dissertations at University of Massachusetts Amherst},
  year={2015}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}



@inproceedings{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5356--5366},
  year={2018}
}


@article{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang M and Voloshin, Cameron and Yue, Yisong},
  journal={arXiv preprint arXiv:1903.08738},
  year={2019}
}

@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}


@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016}
}

@inproceedings{tang2019doubly,
	title={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},
	author={Tang, Ziyang and Feng, Yihao and Li, Lihong and Zhou, Dengyong and Liu, Qiang},
	booktitle={International Conference on Learning Representations},
	year={2019}
}



@article{uehara2019minimax,
  title={Minimax weight and q-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.12809},
  year={2019}
}


@article{kallus2019efficiently,
	title={Efficiently Breaking the Curse of Horizon in Off-Policy Evaluation with Double Reinforcement Learning},
	author={Kallus, Nathan and Uehara, Masatoshi},
	journal={arXiv preprint arXiv:1909.05850},
	year={2019}
}


@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{shi2020reinforcement,
  title={A REINFORCEMENT LEARNING FRAMEWORK FOR TIME DEPENDENT CAUSAL EFFECTS EVALUATION IN A/B TESTING},
  author={Shi, Chengchun and Wang, Xiaoyu and Luo, Shikai and Song, Rui and Zhu, Hongtu and Ye, Jieping},
  year={2020}
}

@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  year={2005},
  publisher={Microtome Publishing, Brookline, United States-Massachusetts}
}