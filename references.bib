---
---

@inproceedings{shi2021deeply,
  title={Deeply-debiased off-policy interval estimation},
  author={Shi, Chengchun and Wan, Runzhe and Chernozhukov, Victor and Song, Rui},
  booktitle={International Conference on Machine Learning},
  pages={9580--9591},
  year={2021},
  organization={PMLR}
}


@article{precup2000eligibility,
  title={Eligibility traces for off-policy policy evaluation},
  author={Precup, Doina},
  journal={Computer Science Department Faculty Publication Series},
  pages={80},
  year={2000}
}

@article{thomas2015safe,
  title={Safe reinforcement learning},
  author={Thomas, Philip S},
  journal={Doctoral Dissertations at University of Massachusetts Amherst},
  year={2015}
}

@inproceedings{jiang2016doubly,
  title={Doubly robust off-policy value evaluation for reinforcement learning},
  author={Jiang, Nan and Li, Lihong},
  booktitle={International Conference on Machine Learning},
  pages={652--661},
  year={2016},
  organization={PMLR}
}



@inproceedings{liu2018breaking,
  title={Breaking the curse of horizon: Infinite-horizon off-policy estimation},
  author={Liu, Qiang and Li, Lihong and Tang, Ziyang and Zhou, Dengyong},
  booktitle={Advances in Neural Information Processing Systems},
  pages={5356--5366},
  year={2018}
}


@article{le2019batch,
  title={Batch policy learning under constraints},
  author={Le, Hoang M and Voloshin, Cameron and Yue, Yisong},
  journal={arXiv preprint arXiv:1903.08738},
  year={2019}
}

@article{voloshin2019empirical,
  title={Empirical study of off-policy policy evaluation for reinforcement learning},
  author={Voloshin, Cameron and Le, Hoang M and Jiang, Nan and Yue, Yisong},
  journal={arXiv preprint arXiv:1911.06854},
  year={2019}
}


@inproceedings{thomas2016data,
  title={Data-efficient off-policy policy evaluation for reinforcement learning},
  author={Thomas, Philip and Brunskill, Emma},
  booktitle={International Conference on Machine Learning},
  pages={2139--2148},
  year={2016}
}

@inproceedings{tang2019doubly,
	title={Doubly Robust Bias Reduction in Infinite Horizon Off-Policy Estimation},
	author={Tang, Ziyang and Feng, Yihao and Li, Lihong and Zhou, Dengyong and Liu, Qiang},
	booktitle={International Conference on Learning Representations},
	year={2019}
}



@article{uehara2019minimax,
  title={Minimax weight and q-function learning for off-policy evaluation},
  author={Uehara, Masatoshi and Huang, Jiawei and Jiang, Nan},
  journal={arXiv preprint arXiv:1910.12809},
  year={2019}
}


@article{kallus2019efficiently,
	title={Efficiently Breaking the Curse of Horizon in Off-Policy Evaluation with Double Reinforcement Learning},
	author={Kallus, Nathan and Uehara, Masatoshi},
	journal={arXiv preprint arXiv:1909.05850},
	year={2019}
}


@book{puterman2014markov,
  title={Markov decision processes: discrete stochastic dynamic programming},
  author={Puterman, Martin L},
  year={2014},
  publisher={John Wiley \& Sons}
}

@book{sutton2018reinforcement,
  title={Reinforcement learning: An introduction},
  author={Sutton, Richard S and Barto, Andrew G},
  year={2018},
  publisher={MIT press}
}

@article{shi2020reinforcement,
  title={A REINFORCEMENT LEARNING FRAMEWORK FOR TIME DEPENDENT CAUSAL EFFECTS EVALUATION IN A/B TESTING},
  author={Shi, Chengchun and Wang, Xiaoyu and Luo, Shikai and Song, Rui and Zhu, Hongtu and Ye, Jieping},
  year={2020}
}

@article{ernst2005tree,
  title={Tree-based batch mode reinforcement learning},
  author={Ernst, Damien and Geurts, Pierre and Wehenkel, Louis},
  journal={Journal of Machine Learning Research},
  volume={6},
  year={2005},
  publisher={Microtome Publishing, Brookline, United States-Massachusetts}
}

@article{shi2021minimax,
  title={A minimax learning approach to off-policy evaluation in partially observable markov decision processes},
  author={Shi, Chengchun and Uehara, Masatoshi and Jiang, Nan},
  journal={arXiv preprint arXiv:2111.06784},
  year={2021}
}

@article{hu2020dtr,
  title={Dtr bandit: Learning to make response-adaptive decisions with low regret},
  author={Hu, Yichun and Kallus, Nathan},
  journal={arXiv preprint arXiv:2005.02791},
  year={2020}
}

@article{shi2022off,
  title={Off-policy confidence interval estimation with confounded markov decision process},
  author={Shi, Chengchun and Zhu, Jin and Ye, Shen and Luo, Shikai and Zhu, Hongtu and Song, Rui},
  journal={Journal of the American Statistical Association},
  pages={1--12},
  year={2022},
  publisher={Taylor \& Francis}
}



@article{shi2020statistical,
  title={Statistical inference of the value function for reinforcement learning in infinite horizon settings},
  author={Shi, Chengchun and Zhang, Sheng and Lu, Wenbin and Song, Rui},
  journal={arXiv preprint arXiv:2001.04515},
  year={2020}
}

@article{vlassis2012bayesian,
  title={Bayesian reinforcement learning},
  author={Vlassis, Nikos and Ghavamzadeh, Mohammad and Mannor, Shie and Poupart, Pascal},
  journal={Reinforcement Learning: State-of-the-Art},
  pages={359--386},
  year={2012},
  publisher={Springer}
}

@article{spaan2012partially,
  title={Partially observable Markov decision processes},
  author={Spaan, Matthijs TJ},
  journal={Reinforcement learning: State-of-the-art},
  pages={387--414},
  year={2012},
  publisher={Springer}
}


@article{zhu2017improving,
  title={On improving deep reinforcement learning for pomdps},
  author={Zhu, Pengfei and Li, Xin and Poupart, Pascal and Miao, Guanghui},
  journal={arXiv preprint arXiv:1704.07978},
  year={2017}
}

@inproceedings{meng2021memory,
  title={Memory-based deep reinforcement learning for pomdps},
  author={Meng, Lingheng and Gorbet, Rob and Kuli{\'c}, Dana},
  booktitle={2021 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)},
  pages={5619--5626},
  year={2021},
  organization={IEEE}
}

@article{sutton1988learning,
  title={Learning to predict by the methods of temporal differences},
  author={Sutton, Richard S},
  journal={Machine learning},
  volume={3},
  pages={9--44},
  year={1988},
  publisher={Springer}
}

@article{singh1996reinforcement,
  title={Reinforcement learning with replacing eligibility traces},
  author={Singh, Satinder P and Sutton, Richard S},
  journal={Machine learning},
  volume={22},
  number={1-3},
  pages={123--158},
  year={1996},
  publisher={Springer}
}


@article{mnih2015human,
  title={Human-level control through deep reinforcement learning},
  author={Mnih, Volodymyr and Kavukcuoglu, Koray and Silver, David and Rusu, Andrei A and Veness, Joel and Bellemare, Marc G and Graves, Alex and Riedmiller, Martin and Fidjeland, Andreas K and Ostrovski, Georg and others},
  journal={Nature},
  volume={518},
  number={7540},
  pages={529--533},
  year={2015},
  publisher={Nature Publishing Group}
}

@inproceedings{van2016deep,
  title={Deep reinforcement learning with double q-learning},
  author={Van Hasselt, Hado and Guez, Arthur and Silver, David},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={30},
  number={1},
  year={2016}
}

@inproceedings{schulman2015trust,
  title={Trust region policy optimization},
  author={Schulman, John and Levine, Sergey and Abbeel, Pieter and Jordan, Michael and Moritz, Philipp},
  booktitle={International conference on machine learning},
  pages={1889--1897},
  year={2015},
  organization={PMLR}
}

@article{schulman2017proximal,
  title={Proximal policy optimization algorithms},
  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},
  journal={arXiv preprint arXiv:1707.06347},
  year={2017}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}

@inproceedings{haarnoja2018soft,
  title={Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with a stochastic actor},
  author={Haarnoja, Tuomas and Zhou, Aurick and Abbeel, Pieter and Levine, Sergey},
  booktitle={International conference on machine learning},
  pages={1861--1870},
  year={2018},
  organization={PMLR}
}

@inproceedings{mnih2016asynchronous,
  title={Asynchronous methods for deep reinforcement learning},
  author={Mnih, Volodymyr and Badia, Adria Puigdomenech and Mirza, Mehdi and Graves, Alex and Lillicrap, Timothy and Harley, Tim and Silver, David and Kavukcuoglu, Koray},
  booktitle={International conference on machine learning},
  pages={1928--1937},
  year={2016},
  organization={PMLR}
}


@article{williams1992simple,
  title={Simple statistical gradient-following algorithms for connectionist reinforcement learning},
  author={Williams, Ronald J},
  journal={Reinforcement learning},
  pages={5--32},
  year={1992},
  publisher={Springer}
}


@article{watkins1992q,
  title={Q-learning},
  author={Watkins, Christopher JCH and Dayan, Peter},
  journal={Machine learning},
  volume={8},
  pages={279--292},
  year={1992},
  publisher={Springer}
}