{"cells":[{"cell_type":"markdown","id":"112864aa","metadata":{"id":"112864aa"},"source":["# Heterogeneous Treatment Effect Estimation (Single Stage)\n","In the previous section, we've introduced the estimation of average treatment effect, where we aims to estimate the difference of potential outcomes by executing action $A=1$ v.s. $A=0$. That is, \n","$$\n","\\text{ATE}=\\mathbb{E}[R(1)-R(0)].\n","$$\n","\n","In this section, we will focus on the estimation of heterogeneous treatment effect (HTE), which is also one of the main focuses in causal inference.\n","\n","\\\\\n","\n","## Main Idea\n","Let's first consider the single stage setup, where the observed data can be written as a state-action-reward triplet $\\{S_i,A_i,R_i\\}_{i=1}^n$ with a total of $n$ trajectories. Heterogeneous treatment effect, as we can imagine from its terminology, aims to measure the heterogeneity of the treatment effect for different subjects. Specifically, we define HTE as $\\tau(s)$, where\n","$$\n","\\tau(s)=\\mathbb{E}[R(1)-R(0)|S=s],\n","$$\n","where $S=s$ denotes the state information of a subject. \n","\n","The estimation of HTE is widely used in a lot of real cases such as precision medicine, advertising, recommendation systems, etc. For example, in adversiting system, the company would like to know the impact (such as annual income) of exposing an ad to a group of customers. In this case, $S$ contains all of the information of a specific customer, $A$ denotes the status of ads exposure ($A=1$ means exposed and $A=0$ means not), and $R$ denotes the reward one can observe when assigned to policy $A$. \n","\n","Suppose the ad is a picture of a dress that can lead the customers to a detail page on a shopping website. In this case, females are more likely to be interested to click the picture and look at the detail page of a dress, resulting in a higher conversion rate than males. The difference of customers preference in clothes can be regarded as the heterogeneity of the treatment effect. By looking at the HTE for each customer, we can clearly estimate the reward of ads exposure from a granular level. \n","\n","Another related concept is conditional averge treatment effect, which is defined as\n","$$\n","\\text{CATE}=\\mathbb{E}[R(1)-R(0)|Z],\n","$$\n","where $Z$ is a collection of states with some specific characsteristics. For example, if the company is interested in the treatment effect of exposing the dress to female customers, $Z$ can be defined as ``female\", and the problem can be addressed under the structure CATE estimation.\n","\n","\\\\\n","\n","## Different approaches in single-stage HTE estimation\n","Next, let's briefly summarize some state-of-the-art approaches in estimating the heterogeneous treatment effect. \n"]},{"cell_type":"markdown","source":["### **1. S-learner**\n","\n","\n","The first estimator we would like to introduce is the S-learner, also known as a ``single learner\". This is one of the most foundamental learners in HTE esitmation, and is very easy to implement.\n","\n","Under three common assumptions in causal inference, i.e. (1) consistency, (2) no unmeasured confounders (NUC), (3) positivity assumption, the heterogeneous treatment effect can be identified by the observed data, where\n","$$\n","\\tau(s)=\\mathbb{E}[R|S,A=1]-\\mathbb{E}[R|S,A=0].\n","$$\n","The basic idea of S-learner is to fit a model for $\\mathbb{E}[R|S,A]$, and then construct a plug-in estimator based on the expression above. Specifically, the algorithm can be summarized as below:\n","\n","**Step 1:**  Estimate the combined response function $\\mu(s,a):=\\mathbb{E}[R|S=s,A=a]$ with any regression algorithm or supervised machine learning methods;\n","\\\n","**Step 2:**  Estimate HTE by \n","$$\n","\\hat{\\tau}_{\\text{S-learner}}(s)=\\hat\\mu(s,1)-\\hat\\mu(s,0).\n","$$\n"],"metadata":{"id":"yfiI-lSZRuOP"},"id":"yfiI-lSZRuOP"},{"cell_type":"markdown","source":["\n","### **2. T-learner**\n","The second learner is called T-learner, which denotes ``two learners\". Instead of fitting a single model to estimate the potential outcomes under both treatment and control groups, T-learner aims to learn different models for $\\mathbb{E}[R(1)|S]$ and $\\mathbb{E}[R(0)|S]$ separately, and finally combines them to obtain a final HTE estimtor.\n","\n","Define the control response function as $\\mu_0(s)=\\mathbb{E}[R(0)|S=s]$, and the treatment response function as $\\mu_1(s)=\\mathbb{E}[R(1)|S=s]$. The algorithm of T-learner is summarized below:\n","\n","**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods;\n","\\\n","**Step 2:**  Estimate HTE by \n","$$\n","\\hat{\\tau}_{\\text{T-learner}}(s)=\\hat\\mu_1(s)-\\hat\\mu_0(s).\n","$$\n","\n"],"metadata":{"id":"cMny8Ri7RvqC"},"id":"cMny8Ri7RvqC"},{"cell_type":"markdown","source":["### **3. X-learner**\n","Next, let's introduce the X-learner. As a combination of S-learner and T-learner, the X-learner can use information from the control(treatment) group to derive better estimators for the treatment(control) group, which is provably more efficient than the above two.\n","\n","The basic\n","\n","\n","**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods (same as T-learner);\n","\\\n","**Step 2:**  Obtain the imputed treatment effects for individuals\n","$$\n","\\tilde{\\Delta}_i^1:=R_i^1-\\hat\\mu_0(S_i^1), \\quad \\tilde{\\Delta}_i^0:=R_i^0-\\hat\\mu_0(S_i^0).\n","$$\n","\\\n","**Step 3:**  Fit the imputed treatment effects to obtain $\\hat\\tau_1(s):=\\mathbb{E}[\\tilde{\\Delta}_i^1|S=s]$ and $\\hat\\tau_0(s):=\\mathbb{E}[\\tilde{\\Delta}_i^0|S=s]$;\n","\\\n","**Step 4:**  The final HTE estimator is given by\n","$$\n","\\hat{\\tau}_{\\text{X-learner}}(s)=g(s)\\hat\\tau_0(s)+(1-g(s))\\hat\\tau_1(s),\n","$$\n","where $g(s)$ is a weight function."],"metadata":{"id":"8lwheJQ8RxAw"},"id":"8lwheJQ8RxAw"},{"cell_type":"markdown","source":["### **4. R learner**\n","\n","\n"],"metadata":{"id":"32szzPY4RyWO"},"id":"32szzPY4RyWO"},{"cell_type":"markdown","source":["### **5. DR-learner**\n","\n","\n","\n"],"metadata":{"id":"J2z2JRumRzdo"},"id":"J2z2JRumRzdo"},{"cell_type":"markdown","source":["### **6. Lp-R-learner**\n","\n"],"metadata":{"id":"YgKc3F0cR0Y4"},"id":"YgKc3F0cR0Y4"},{"cell_type":"markdown","source":["### **7. Causal Forest**\n","\n","\n","\n"],"metadata":{"id":"SgWh47pKR1XR"},"id":"SgWh47pKR1XR"},{"cell_type":"markdown","source":["## Demo Code\n","In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."],"metadata":{"id":"I6Z2one4R3Aq"},"id":"I6Z2one4R3Aq"},{"cell_type":"markdown","id":"e3559b08","metadata":{"id":"e3559b08"},"source":["### 1. Meta-Leaners"]},{"cell_type":"code","execution_count":null,"id":"6be9bfe4","metadata":{"id":"6be9bfe4"},"outputs":[],"source":["# import learner\n","from causaldm._util_causaldm import *\n","from causaldm.learners import QLearning"]},{"cell_type":"code","execution_count":null,"id":"8e46da2b","metadata":{"id":"8e46da2b"},"outputs":[],"source":["# get the data\n","S,A,R = get_data(target_col = 'spend', binary_trt = False)"]},{"cell_type":"code","execution_count":null,"id":"b4c2e64d","metadata":{"id":"b4c2e64d"},"outputs":[],"source":["#1. specify the model you would like to use\n","# If want to include all the variable in S and A with no specific model structure, then use \"Y~.\"\n","# Otherwise, specify the model structure by hand\n","# Note: if the action space is not binary, use C(A) in the model instead of A\n","model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n","              'action_space':{'A':[0,1,2]}}]"]},{"cell_type":"markdown","id":"46e83e1f","metadata":{"id":"46e83e1f"},"source":["By specifing the model_info, we assume a regression model that:\n","\\begin{align}\n","Q(s,a,\\beta) &= \\beta_{00}+\\beta_{01}*recency+\\beta_{02}*history\\\\\n","&+I(a=1)*\\{\\beta_{10}+\\beta_{11}*recency+\\beta_{12}*history\\} \\\\\n","&+I(a=2)*\\{\\beta_{20}+\\beta_{21}*recency+\\beta_{22}*history\\} \n","\\end{align}"]},{"cell_type":"code","execution_count":null,"id":"b7df28e8","metadata":{"id":"b7df28e8"},"outputs":[],"source":["#2. initialize the learner\n","QLearn = QLearning.QLearning()\n","#3. train the policy\n","QLearn.train(S, A, R, model_info, T=1)"]},{"cell_type":"code","execution_count":null,"id":"840f98ff","metadata":{"id":"840f98ff"},"outputs":[],"source":["#4. recommend action\n","opt_d = QLearn.recommend_action(S).value_counts()\n","#5. get the estimated value of the optimal regime\n","V_hat = QLearn.predict_value(S)\n","print(\"fitted model:\",QLearn.fitted_model[0].params)\n","print(\"opt regime:\",opt_d)\n","print(\"opt value:\",V_hat)"]},{"cell_type":"markdown","id":"c6111442","metadata":{"id":"c6111442"},"source":["####**Result Interpretation:** "]},{"cell_type":"markdown","id":"1098b550","metadata":{"id":"1098b550"},"source":["## References\n","1. Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 4156â€“4165."]},{"cell_type":"markdown","id":"45df2637","metadata":{"id":"45df2637"},"source":["!!Functions are already tested with the data and results provided in the DTR book\n","\n","TODO: \n","    1. estimate the standard error for the binary case with sandwich formula;\n","    2. inference for the estimated optimal regime: projected confidence interval? m-out-of-n CI?...."]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"collapsed_sections":["e3559b08"]}},"nbformat":4,"nbformat_minor":5}