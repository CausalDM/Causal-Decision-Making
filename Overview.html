
<!DOCTYPE html>

<html lang="en">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Overview &#8212; Causal Decision Making</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="_static/styles/bootstrap.css?digest=796348d33e8b1d947c94" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=796348d33e8b1d947c94" rel="stylesheet">

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=796348d33e8b1d947c94" rel="stylesheet">
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2">
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=4ec06e9971c5264fbd345897d5258098f11cc577" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94">
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=8bf782fb4ee92b3d3646425e50f299c4e1fd152d"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'Overview';</script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Causal Structure Learning (CSL)" href="0_Motivating_Examples/CSL.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="docsearch:language" content="en">
  </head>
  
  
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="180" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>

  
  <input type="checkbox" class="sidebar-toggle" name="__primary" id="__primary">
  <label class="overlay overlay-primary" for="__primary"></label>

  
  <input type="checkbox" class="sidebar-toggle" name="__secondary" id="__secondary">
  <label class="overlay overlay-secondary" for="__secondary"></label>

  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
      
<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
  </div>

  
  <nav class="bd-header navbar navbar-expand-lg bd-navbar" id="navbar-main"><div class="bd-header__inner bd-page-width">
  <label class="sidebar-toggle primary-toggle" for="__primary">
      <span class="fa-solid fa-bars"></span>
  </label>
  <div id="navbar-start">
    
    
  


<a class="navbar-brand logo" href="#">

  
  
  
  
  
  
  

  
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    
  </div>

  
  <div class="col-lg-9 navbar-header-items">
    <div id="navbar-center" class="mr-auto">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="0_Motivating_Examples/CSL.html">
                        Causal Structure Learning (CSL)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="0_Motivating_Examples/CEL.html">
                        Causal Effect Learning (CEL)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="0_Motivating_Examples/CPL.html">
                        Causal Policy Learning (CPL)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="1_Preliminary/Causal%20Inference%20Preliminary.html">
                        Causal Inference Preliminary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="2_Causal_Structure_Learning/Preliminaries%20of%20Causal%20Graphs.html">
                        Preliminaries of Causal Graphs
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="2_Causal_Structure_Learning/Causal%20Discovery.html">
                        Causal Discovery
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="2_Causal_Structure_Learning/Causal%20Mediation%20Analysis%20with%20Causal%20Discovery.html">
                        Causal Mediation Analysis with Causal Discovery
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html">
                        Single Stage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="3_Causal_Effect_Learning/Scenario%202/underMDP.html">
                        Markov Decision Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="3_Causal_Effect_Learning/Scenario%203/Panel%20Data.html">
                        Panel Data
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/Single%20Stage.html">
                        Single Stage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/Discrete.html">
                        Discrete Action Space
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/Continuous.html">
                        Continuous Action Space
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/PlanToDo.html">
                        Plan To Do
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario2/preliminary_MDP-potential-outcome.html">
                        Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario2/Evaluation.html">
                        Policy Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario2/Optimization.html">
                        Policy Optimization
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/Multi%20Stage.html">
                        Multiple Stages (DTR)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/Q-learning_Multiple.html">
                        Q-Learning (Multiple Stages)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/A-learning_Multiple.html">
                        A-Learning (Multiple Stages)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/MediatedQ-learning_Multiple.html">
                        MediatedQ-Learning (Multiple Stages)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Bandits.html">
                        Overview: Bandits ALgorithm
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/MAB/MAB.html">
                        Multi-Armed Bandits (MAB)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Contextual_Bandits/Contextual_Bandits.html">
                        Contextual Bandits
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Meta_Bandits/Meta_Bandits.html">
                        Meta Bandits
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Structured_Bandit.html">
                        Structured Bandit (Slate Recommendation)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/OnlineEval/Online%20Policy%20Evaluation.html">
                        Online Policy Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario5/OnlineRL_Markov.html">
                        Ooline Policy Learning and Evaluation in Markovian Environments
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario6/OnlineRL_non_Markov.html">
                        Ooline Policy Learning in Non-Markov Environments
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MIMIC3/MIMIC3-Demo.html">
                        Mimic3 Demo
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MIMIC3/MIMIC3_DTR.html">
                        CPL: 3-Stage DTR
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MIMIC3/MIMIC3_MediationAnalysis.html">
                        Multi-Stage Mediation Analysis
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MovieLens/Case_Study_2.html">
                        MovieLens
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
    </div>

    <div id="navbar-end">
      
        <div class="navbar-end-item navbar-persistent--container">
          
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
        </div>
      
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
  </div>


  
  
    <div class="navbar-persistent--mobile">
<button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-toggle="tooltip">
  <i class="fa-solid fa-magnifying-glass"></i>
</button>
    </div>
  

  
  <label class="sidebar-toggle secondary-toggle" for="__secondary">
      <span class="fa-solid fa-outdent"></span>
  </label>
  

</div>
  </nav>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        
  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
      
      <div class="navbar-center-item">
        <nav class="navbar-nav">
    <p class="sidebar-header-items__title" role="heading" aria-level="1" aria-label="Site Navigation">
        Site Navigation
    </p>
    <ul id="navbar-main-elements" class="navbar-nav">
        
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="0_Motivating_Examples/CSL.html">
                        Causal Structure Learning (CSL)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="0_Motivating_Examples/CEL.html">
                        Causal Effect Learning (CEL)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="0_Motivating_Examples/CPL.html">
                        Causal Policy Learning (CPL)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="1_Preliminary/Causal%20Inference%20Preliminary.html">
                        Causal Inference Preliminary
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="2_Causal_Structure_Learning/Preliminaries%20of%20Causal%20Graphs.html">
                        Preliminaries of Causal Graphs
                      </a>
                    </li>
                
            <div class="nav-item dropdown">
                <button class="btn dropdown-toggle nav-item" type="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">
                    More
                </button>
                <div class="dropdown-menu">
                    
                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="2_Causal_Structure_Learning/Causal%20Discovery.html">
                        Causal Discovery
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="2_Causal_Structure_Learning/Causal%20Mediation%20Analysis%20with%20Causal%20Discovery.html">
                        Causal Mediation Analysis with Causal Discovery
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html">
                        Single Stage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="3_Causal_Effect_Learning/Scenario%202/underMDP.html">
                        Markov Decision Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="3_Causal_Effect_Learning/Scenario%203/Panel%20Data.html">
                        Panel Data
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/Single%20Stage.html">
                        Single Stage
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/Discrete.html">
                        Discrete Action Space
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/Continuous.html">
                        Continuous Action Space
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario1/PlanToDo.html">
                        Plan To Do
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario2/preliminary_MDP-potential-outcome.html">
                        Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario2/Evaluation.html">
                        Policy Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario2/Optimization.html">
                        Policy Optimization
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/Multi%20Stage.html">
                        Multiple Stages (DTR)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/Q-learning_Multiple.html">
                        Q-Learning (Multiple Stages)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/A-learning_Multiple.html">
                        A-Learning (Multiple Stages)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario3/MediatedQ-learning_Multiple.html">
                        MediatedQ-Learning (Multiple Stages)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Bandits.html">
                        Overview: Bandits ALgorithm
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/MAB/MAB.html">
                        Multi-Armed Bandits (MAB)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Contextual_Bandits/Contextual_Bandits.html">
                        Contextual Bandits
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Meta_Bandits/Meta_Bandits.html">
                        Meta Bandits
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Structured_Bandit.html">
                        Structured Bandit (Slate Recommendation)
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario4/OnlineEval/Online%20Policy%20Evaluation.html">
                        Online Policy Evaluation
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario5/OnlineRL_Markov.html">
                        Ooline Policy Learning and Evaluation in Markovian Environments
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="4_Causal_Policy_Learning/Scenario6/OnlineRL_non_Markov.html">
                        Ooline Policy Learning in Non-Markov Environments
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MIMIC3/MIMIC3-Demo.html">
                        Mimic3 Demo
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MIMIC3/MIMIC3_DTR.html">
                        CPL: 3-Stage DTR
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MIMIC3/MIMIC3_MediationAnalysis.html">
                        Multi-Stage Mediation Analysis
                      </a>
                    </li>
                

                    <li class="nav-item">
                      <a class="nav-link nav-internal" href="5_Case_Study/MovieLens/Case_Study_2.html">
                        MovieLens
                      </a>
                    </li>
                
                </div>
            </div>
            
    </ul>
</nav>
      </div>
      
      </div>
    

    
    
    <div class="sidebar-header-items__end">
      
      <div class="navbar-end-item">
        <button class="theme-switch-button btn btn-sm btn-outline-primary navbar-btn rounded-circle" title="light/dark" aria-label="light/dark" data-toggle="tooltip">
    <span class="theme-switch" data-mode="light"><i class="fa-solid fa-sun"></i></span>
    <span class="theme-switch" data-mode="dark"><i class="fa-solid fa-moon"></i></span>
    <span class="theme-switch" data-mode="auto"><i class="fa-solid fa-circle-half-stroke"></i></span>
</button>
      </div>
      
      <div class="navbar-end-item">
        <ul id="navbar-icon-links" class="navbar-nav" aria-label="Icon Links">
      </ul>
      </div>
      
    </div>
    
  </div>

  
  <div class="sidebar-start-items sidebar-primary__section">
    <div class="sidebar-start-items__item">
  


<a class="navbar-brand logo" href="#">

  
  
  
  
  
  
  

  
    <img src="_static/logo.png" class="logo__image only-light" alt="Logo image">
    <img src="_static/logo.png" class="logo__image only-dark" alt="Logo image">
  
  
</a>
    </div>
    <div class="sidebar-start-items__item">
<form class="bd-search d-flex align-items-center" action="search.html" method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false">
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
    </div>
    <div class="sidebar-start-items__item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1 current active">
                <a class="reference internal" href="#">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Motivating Examples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="0_Motivating_Examples/CSL.html"><em>Causal Structure Learning (CSL)</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="0_Motivating_Examples/CEL.html"><em>Causal Effect Learning (CEL)</em></a></li>
<li class="toctree-l1"><a class="reference internal" href="0_Motivating_Examples/CPL.html"><em>Causal Policy Learning (CPL)</em></a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="1_Preliminary/Causal%20Inference%20Preliminary.html">Causal Inference Preliminary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Structure Learning (CSL)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="2_Causal_Structure_Learning/Preliminaries%20of%20Causal%20Graphs.html">Preliminaries of Causal Graphs</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="2_Causal_Structure_Learning/Causal%20Discovery.html">Causal Discovery</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="2_Causal_Structure_Learning/Testing-based%20Learner.html">Testing-based Learner</a></li>
<li class="toctree-l2"><a class="reference internal" href="2_Causal_Structure_Learning/Functional-based%20Learner.html">Functional-based Learner</a></li>
<li class="toctree-l2"><a class="reference internal" href="2_Causal_Structure_Learning/Score-based%20Learner.html">Score-based Learner</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="2_Causal_Structure_Learning/Causal%20Mediation%20Analysis%20with%20Causal%20Discovery.html">Causal Mediation Analysis with Causal Discovery</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Effect Learning (CEL)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html"><strong>Single Stage</strong></a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/ATE.html">ATE Estimation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/HTE.html">HTE Estimation</a><input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/S-learner.html"><strong>1. S-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/T-learner.html"><strong>2. T-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/X-learner.html"><strong>3. X-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/R-Learner.html"><strong>4. R learner</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/DR-Learner.html"><strong>5. DR-learner</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/Lp-R-Learner.html"><strong>6. Lp-R-learner</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/GRF.html"><strong>7. Generalized Random Forest</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%201/Dragonnet.html"><strong>8. Dragon Net</strong></a></li>

</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%202/underMDP.html">Markov Decision Processes</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/Panel%20Data.html">Panel Data</a><input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-4"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/ATE.html">ATE</a><input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-5"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/Synthetic%20Control.html"><strong>Synthetic Control</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/Synthetic%20Learner.html"><strong>Synthetic Learner</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/Synthetic%20DiD.html"><strong>Synthetic DiD</strong></a></li>

</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/HTE.html">HTE</a><input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-6"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/DiD.html"><strong>Difference in Difference</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/R-DiD.html"><strong>R-DiD</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/Synthetic%20X-Learner.html"><strong>Synthetic X-Learner</strong></a></li>

<li class="toctree-l3"><a class="reference internal" href="3_Causal_Effect_Learning/Scenario%203/H1SL_H2SL.html"><strong>H1SL and H2SL</strong></a></li>

</ul>
</li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Single%20Stage.html">Single Stage</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Discrete.html">Discrete Action Space</a><input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-7"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Q-learning_Single.html">Q-Learning (Single Stage)</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/A-learning_Single.html">A-Learning (Single Stage)</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Classification.html">Reduction to Classification Problems</a><input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-8"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Classification/O-Learning.html">Outcome Weighted Learning</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Classification/E-learning.html">Entropy learning</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Quantile/QuantileOTR_test.html"><strong>Quantile Optimal Treatment Regime</strong></a></li>

</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Continuous.html">Continuous Action Space</a><input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-9"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Continuous/Deep%20Jump%20Learner.html">Deep Jump Learner for Continuous Actions</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Continuous/Kernel-Based%20Learner.html">Kernel-Based Learner</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/Continuous/Outcome%20Learning.html">Outcome Learning</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario1/PlanToDo.html">Plan To Do</a><input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-10"><i class="fa-solid fa-chevron-down"></i></label><ul class="simple">
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 2</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/preliminary_MDP-potential-outcome.html">Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/Evaluation.html">Policy Evaluation</a><input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-11"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/FQE.html">Fitted-Q Evaluation</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/IPW_Infinite.html">Importance Sampling for Policy Evaluation (Infinite Horizon)</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/DR_Infinite.html">Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/Deeply_Debiased.html">Deeply-Debiased Off-Policy Evaluation</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/Optimization.html">Policy Optimization</a><input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-12"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario2/FQI.html">Fitted-Q Iteration</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario3/Multi%20Stage.html">Multiple Stages (DTR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario3/Q-learning_Multiple.html">Q-Learning (Multiple Stages)</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario3/A-learning_Multiple.html">A-Learning (Multiple Stages)</a></li>
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario3/MediatedQ-learning_Multiple.html">MediatedQ-Learning (Multiple Stages)</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Bandits.html">Overview: Bandits ALgorithm</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/MAB/MAB.html">Multi-Armed Bandits (MAB)</a><input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-13"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/MAB/Epsilon_Greedy.html"><span class="math notranslate nohighlight">\(\epsilon\)</span>-Greedy</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/MAB/UCB.html">UCB</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/MAB/TS.html">TS</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Contextual_Bandits/Contextual_Bandits.html">Contextual Bandits</a><input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-14"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Contextual_Bandits/LinUCB.html">LinUCB</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Contextual_Bandits/LinTS.html">LinTS</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Meta_Bandits/Meta_Bandits.html">Meta Bandits</a><input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-15"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Meta_Bandits/Meta_TS.html">Meta Thompson Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Meta_Bandits/MTTS.html">Multi-Task Thompson Sampling (MTTS)</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Structured_Bandit.html">Structured Bandit (Slate Recommendation)</a><input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-16"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Cascade/Learning%20to%20rank.html">Online Learning to Rank (Cascading Bandit)</a><input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-17"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Cascade/TS_Cascade.html">TS_Cascade</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Cascade/CascadeLinTS.html">CascadeLinTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Cascade/MTSS_Cascade.html">MTSS_Cascade</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Combinatorial-Semi/Combinatorial%20Optimization.html">Online Combinatorial Optimization (Combinatorial Semi-Bandit)</a><input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-18"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Combinatorial-Semi/CombTS.html">CombTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Combinatorial-Semi/CombLinTS.html">CombLinTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/Combinatorial-Semi/MTSS_Comb.html">MTSS_Comb</a></li>
</ul>
</li>
<li class="toctree-l2 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/MNL/Assortment%20Optimization.html">Dynamic Assortment Optimization (Multinomial Logit Bandit)</a><input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-19"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/MNL/TS_MNL_Beta.html">TS_MNL</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/MNL/TS_Contextual_MNL.html">TS_Contextual_MNL</a></li>
<li class="toctree-l3"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/Structured_Bandits/MNL/MTSS_MNL.html">MTSS_MNL</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/OnlineEval/Online%20Policy%20Evaluation.html">Online Policy Evaluation</a><input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-20"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/OnlineEval/Direct%20Online%20Policy%20Evaluator.html">Direct Online Policy Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/OnlineEval/Inverse%20Probability%20Weighted%20Online%20Policy%20Evaluator.html">Inverse Probability Weighted Online Policy Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario4/OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html">Doubly Robust Online Policy Evaluator</a></li>
</ul>
</li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario5/OnlineRL_Markov.html">Ooline Policy Learning and Evaluation in Markovian Environments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="4_Causal_Policy_Learning/Scenario6/OnlineRL_non_Markov.html">Ooline Policy Learning in Non-Markov Environments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="5_Case_Study/MIMIC3/MIMIC3-Demo.html">Mimic3 Demo</a></li>


<li class="toctree-l1"><a class="reference internal" href="5_Case_Study/MIMIC3/MIMIC3_DTR.html">CPL: 3-Stage DTR</a></li>


<li class="toctree-l1"><a class="reference internal" href="5_Case_Study/MIMIC3/MIMIC3_MediationAnalysis.html">Multi-Stage Mediation Analysis</a></li>
<li class="toctree-l1"><a class="reference internal" href="5_Case_Study/MovieLens/Case_Study_2.html">MovieLens</a></li>
</ul>

    </div>
</nav>
    </div>
  </div>
  

  
  <div class="sidebar-end-items sidebar-primary__section">
    <div class="sidebar-end-items__item">
    </div>
  </div>

  
  <div id="rtd-footer-container"></div>

      </div>
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

        <div class="bd-content">
          <div class="bd-article-container">
            
            <div class="bd-header-article">
                



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        <label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" data-toggle="tooltip" data-placement="right" title="Toggle primary sidebar">
            <span class="fa-solid fa-bars"></span>
        </label>
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="btn btn-sm"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<div class="dropdown dropdown-repository-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">repository</span>
</a>
</a>
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2FOverview.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">open issue</span>
</a>
</a>
      
  </ul>
</div>



<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      <li><a href="_sources/Overview.md" target="_blank"
   class="btn btn-sm dropdown-item"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.md</span>
</a>
</a>
      
      <li>
<button onclick="printPdf(this)"
  class="btn btn-sm dropdown-item"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</a>
      
  </ul>
</div>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary" data-toggle="tooltip" data-placement="left" title="Toggle secondary sidebar">
            <span class="fa-solid fa-list"></span>
        </label>
    </div>
</div>
            </div>
            
            

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Overview</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Overview
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-to-expect">
     What to expect?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-sl-a-causal-structure-learning-csl">
     <a name="SL">
     </a>
     Causal Structure Learning (CSL)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-ml-a-causal-effect-learning-cel">
     <a name="ML">
     </a>
     Causal Effect Learning (CEL)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-1-i-i-d">
       <a name="Case1">
       </a>
       Paradigm 1: I.I.D
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-2-markovian-state-transition">
       <a name="Case1">
       </a>
       Paradigm 2: Markovian State Transition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-3-panel-data">
       <a name="Case1">
       </a>
       Paradigm 3: Panel Data
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-pl-a-causal-policy-learning-cpl">
     <a name="PL">
     </a>
     Causal Policy Learning (CPL)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-1-fixed-policy-with-independent-states">
       <a name="Case1">
       </a>
       Paradigm 1: Fixed Policy with Independent States
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case2-a-paradigm-2-fixed-policy-with-markovian-state-transition">
       <a name="Case2">
       </a>
       Paradigm 2: Fixed Policy with Markovian State Transition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case3-a-paradigm-3-fixed-policy-with-non-markovian-state-transition">
       <a name="Case3">
       </a>
       Paradigm 3: Fixed Policy with Non-Markovian State Transition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case4-a-paradigm-4-adaptive-policy-with-independent-states">
       <a name="Case4">
       </a>
       Paradigm 4: Adaptive Policy with Independent States
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case5-a-paradigm-5-adaptive-policy-with-markovian-state-transition">
       <a name="Case5">
       </a>
       Paradigm 5: Adaptive Policy with Markovian State Transition
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-upon-the-mdp-structure-when-an-adaptive-policy-is-applied-the-paradigm-5-clearly-depicts-the-data-generating-process-in-which-s-t-follows-the-markovian-state-transition-and-a-t-would-be-affected-by-all-previous-observations-h-t-1">
   Building upon the MDP structure, when an adaptive policy is applied, the Paradigm 5 clearly depicts the data-generating process, in which
   <span class="math notranslate nohighlight">
    \(S_t\)
   </span>
   follows the Markovian state transition and
   <span class="math notranslate nohighlight">
    \(A_t\)
   </span>
   would be affected by all previous observations
   <span class="math notranslate nohighlight">
    \(H_{t-1}\)
   </span>
   .
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case3-a-scenario-3-fixed-policy-with-non-markovian-state-transition">
     <a name="Case3">
     </a>
     Scenario 3: Fixed Policy with Non-Markovian State Transition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case4-a-scenario-4-adaptive-policy-with-independent-states">
     <a name="Case4">
     </a>
     Scenario 4: Adaptive Policy with Independent States
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case5-a-scenario-5-adaptive-policy-with-markovian-state-transition">
     <a name="Case5">
     </a>
     Scenario 5: Adaptive Policy with Markovian State Transition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case6-a-paradigm-6-adaptive-policy-with-non-markovian-state-transition">
     <a name="Case6">
     </a>
     Paradigm 6: Adaptive Policy with Non-Markovian State Transition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#appendix">
     Appendix
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-singeldtr-a-a-paradigm-1">
       <a name="SingelDTR">
       </a>
       A. Paradigm 1
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-mdp-a-b-paradigm-2">
       <a name="MDP">
       </a>
       B. Paradigm 2
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-multidtr-a-c-paradigm-3">
       <a name="MultiDTR">
       </a>
       C. Paradigm 3
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-bandits-a-d-paradigm-4">
       <a name="Bandits">
       </a>
       D. Paradigm 4
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reference">
     Reference
    </a>
   </li>
  </ul>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>

            <article class="bd-article" role="main">
              
  <section class="tex2jax_ignore mathjax_ignore" id="overview">
<h1>Overview<a class="headerlink" href="#overview" title="Permalink to this heading">#</a></h1>
<section id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this heading">#</a></h2>
</section>
<hr class="docutils" />
<section id="what-to-expect">
<h2>What to expect?<a class="headerlink" href="#what-to-expect" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<figure class="align-center" id="id1">
<a class="reference internal image-reference" href="_images/Workflow_CausalDM.png"><img alt="_images/Workflow_CausalDM.png" src="_images/Workflow_CausalDM.png" style="width: 800px;" /></a>
<figcaption>
<p><span class="caption-number">Fig. 1 </span><span class="caption-text">Workflow of the Causal Decision Making.</span><a class="headerlink" href="#id1" title="Permalink to this image">#</a></p>
</figcaption>
</figure>
<p>The Fig 1 depicts the overall structure of this book, which is comprised of three primary components: <strong>Causal Structure Learning</strong>, <strong>Causal Policy Learning</strong>, and <strong>Causal Effect Learning</strong>. Specifically, in the chapter <span class="xref myst"><strong>Causal Structure Learning (CSL)</strong></span>, we present state-of-the-art techniques for learning the skeleton of causal relationships among input variables. When a causal structure is known, the second chapter <span class="xref myst"><strong>Causal Effect Learning (CEL)</strong></span> introduces approaches for treatment effect identification, estimation and inference. Finally, the <span class="xref myst"><strong>Causal Policy Learning (CPL)</strong></span> chapter introduces diverse policy learners to learn optimal policies and evaluate various policies of interest.</p>
<p>Following is a brief summary of the contents of each chapter.</p>
</section>
<section id="a-name-sl-a-causal-structure-learning-csl">
<h2><a name="SL"></a> Causal Structure Learning (CSL)<a class="headerlink" href="#a-name-sl-a-causal-structure-learning-csl" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<p>The main goal of causal structure learning is to learn the unknown causal relationships among different variables.</p>
<a class="reference internal image-reference" href="_images/CSL_aim.png"><img alt="Scenario1" class="align-center" src="_images/CSL_aim.png" style="width: 500px;" /></a>
<p>The classical causal structure learning methods can be categorized into three types.</p>
<a class="reference internal image-reference" href="_images/CSL_type.png"><img alt="Scenario1" class="align-center" src="_images/CSL_type.png" style="width: 500px;" /></a>
<p>This chapter discusses three classical techniques for learning causal graphs, each with its own merits and downsides.</p>
<table class="table">
<thead>
<tr class="row-odd"><th class="head"><p>Learners      Type</p></th>
<th class="head"><p>Supported Model</p></th>
<th class="head"><p>Noise Required for Training</p></th>
<th class="head"><p>Complexity</p></th>
<th class="head"><p>Scale-Free?</p></th>
<th class="head"><p>Learners Example</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Testing based</p></td>
<td><p>Models 1</p></td>
<td><p>Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(O(p^q)\)</span></p></td>
<td><p>Yes</p></td>
<td><p>PC</p></td>
</tr>
<tr class="row-odd"><td><p>Functional based</p></td>
<td><p>Models 1 &amp; 2</p></td>
<td><p>non-Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(O(p^3)\)</span></p></td>
<td><p>Yes</p></td>
<td><p>LiNGAM</p></td>
</tr>
<tr class="row-even"><td><p>Score based</p></td>
<td><p>Models 1 &amp; 3</p></td>
<td><p>Gaussian/non-Gaussian</p></td>
<td><p><span class="math notranslate nohighlight">\(O(p^3)\)</span></p></td>
<td><p>No</p></td>
<td><p>NOTEARS</p></td>
</tr>
</tbody>
</table>
<p><em><span class="math notranslate nohighlight">\(p\)</span> is the number of nodes in <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>, and <span class="math notranslate nohighlight">\(q\)</span> is the max number of nodes adjacent to any nodes in <span class="math notranslate nohighlight">\(\mathcal{G}\)</span>.</em></p>
</section>
<section id="a-name-ml-a-causal-effect-learning-cel">
<h2><a name="ML"></a> Causal Effect Learning (CEL)<a class="headerlink" href="#a-name-ml-a-causal-effect-learning-cel" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<p>Causal effect learning, as we’ve mentioned at the beginning, aims to infer on the effect of a specific treatment in the context of causal inference. According to the data structure, we mainly divide the problem settings of CEL into three categories: independent states, Markovian state transition, and non-Markovian state transition.</p>
<p><img alt="Table_of_Fixed_Policy.png" src="_images/Table_of_Fixed_Policy.png" /></p>
<ol class="arabic simple">
<li><p><strong>Independent states</strong>, or Paradigm 1, denotes the single-stage setup where the full data can be summarized as a number of state-action-reward triplet with size <span class="math notranslate nohighlight">\(n\)</span>, i.e. <span class="math notranslate nohighlight">\((S_i,A_i,R_i)_{1\leq i\leq n}\)</span>. Due to the simplicity of data structure, there are quite a few methods proposed to handle the estimation of both average treatment effect and heterogeneous treatment effect, ranging from basic learners to deep-learning related approaches.</p></li>
<li><p><strong>Markovian state transition</strong>, or Paradigm 2, denotes the case where the data contains <span class="math notranslate nohighlight">\(T\)</span> stages, i.e.  <span class="math notranslate nohighlight">\((S_{i,t},A_{i,t},R_{i,t})_{1\leq i\leq n,0\leq t\leq T}\)</span>, and the transition of stages follows a Markov decision process.</p></li>
<li><p><strong>Non-Markovian state transition</strong>, or Paradigm 3, denotes other miscellaneous cases where the data have a relatively complex data structure, while the transition of stages doesn’t follow the Markov assumption. In this section, we will mainly discuss some representative methods to deal with panel data.</p></li>
</ol>
<section id="a-name-case1-a-paradigm-1-i-i-d">
<h3><a name="Case1"></a> Paradigm 1: I.I.D<a class="headerlink" href="#a-name-case1-a-paradigm-1-i-i-d" title="Permalink to this heading">#</a></h3>
<p>In Paradigm 1, we consider the standard case where all observations are i.i.d.. The full data of interest is <span class="math notranslate nohighlight">\((S_i,A_i,R_i)\)</span> where <span class="math notranslate nohighlight">\(i\in\{1,\dots,N\}\)</span>.</p>
<a class="reference internal image-reference" href="_images/CEL-IID.png"><img alt="Scenario1" class="align-center" src="_images/CEL-IID.png" style="width: 400px;" /></a>
</section>
<section id="a-name-case1-a-paradigm-2-markovian-state-transition">
<h3><a name="Case1"></a> Paradigm 2: Markovian State Transition<a class="headerlink" href="#a-name-case1-a-paradigm-2-markovian-state-transition" title="Permalink to this heading">#</a></h3>
<p>In Paradigm 2, the data we observed can be denoted as <span class="math notranslate nohighlight">\((S_{i,t},A_{i,t},R_{i,t})_{1\leq i\leq n,0\leq t\leq T}\)</span>, where <span class="math notranslate nohighlight">\(n\)</span> is the number of trajectories, and <span class="math notranslate nohighlight">\(T\)</span> is the number of stages. This data structure is widely named as Markov decision processes (MDPs).</p>
<p>In causal effect learning, we focus on estimating the difference of the effect between a specific target policy (or treatment) and control at all stages. Due to the long-stage or even infinite-horizon structure of the data, most of the existing approaches paid attention to evaluating the expected reward of any given policy (treatment), and then do subtraction to obtain the effect of treatment versus control.</p>
<p>In observational data analysis, the data we obtained does not come from the target policy (or treatment) we wish to evaluate, resulting in the shift of data distribution. This problem is widely known as offline policy evaluation (OPE) under MDPs. The figure below depicts several groups of methods to address this problem.</p>
<a class="reference internal image-reference" href="_images/CEL-Markovian.png"><img alt="Scenario2" class="align-center" src="_images/CEL-Markovian.png" style="width: 200px;" /></a>
<p>Since this problem can be regarded as a special case of causal policy learning, we leave the detailed introduction of this part to Paradigm 2 of chapter 3 (Causal Policy Learning).</p>
</section>
<section id="a-name-case1-a-paradigm-3-panel-data">
<h3><a name="Case1"></a> Paradigm 3: Panel Data<a class="headerlink" href="#a-name-case1-a-paradigm-3-panel-data" title="Permalink to this heading">#</a></h3>
<p>In Paradigm 3, we consider the panel data where samples are measured over time. This type of data can be found in economics, social sciences, medicine and epidemiology, finance, and the physical sciences. The outcome of interest is defined as <span class="math notranslate nohighlight">\(R_{i,t}\)</span>, which denotes the reward of observation <span class="math notranslate nohighlight">\(i\)</span>  at time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Consider a (either experimental or observational) study with <span class="math notranslate nohighlight">\(N = m + n\)</span> units and <span class="math notranslate nohighlight">\(T = T_0 + T_1\)</span> time periods in total. Without loss of generality, we assume that the first m units are treated units and the last n units are control units. Each unit <span class="math notranslate nohighlight">\(i\)</span> is associated with a <span class="math notranslate nohighlight">\(d\)</span>-dimensional time-invariant feature vector <span class="math notranslate nohighlight">\(S_i\in \mathbb{R}^d\)</span>,  and receives an unit-level outcome <span class="math notranslate nohighlight">\(R_{i,t}\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>. The full data structure is given below:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\left[
\begin{array}{ccc:ccc}
S_{1} &amp; \cdots &amp; S_{m} &amp; S_{m+1} &amp; \cdots &amp; S_{n} \\
\hline
R_{1,1} &amp; \cdots &amp; R_{m,1} &amp; R_{m+1,1} &amp; \cdots &amp; R_{m+n,1} \\
\vdots &amp; &amp; \vdots &amp;\vdots &amp; &amp; \vdots \\
R_{1,T_0} &amp; \cdots &amp; R_{m,T_0} &amp; R_{m+1,T_0} &amp; \cdots &amp; R_{m+n,T_0} \\
\hdashline
R_{1,T_0+1} &amp; \cdots &amp; R_{m,T_0+1} &amp; R_{m+1,T_0+1} &amp; \cdots &amp; R_{m+n,T_0+1} \\
\vdots &amp; &amp; \vdots &amp;\vdots &amp; &amp; \vdots \\
R_{1,T} &amp; \cdots &amp; R_{m,T} &amp; R_{m+1,T} &amp; \cdots &amp; R_{m+n,T} \\
\end{array}
\right]
\end{split}\]</div>
<p>The current literature in dealing with panel data can be roughly divided into two categories: Difference-in-difference and synthetic control.</p>
<a class="reference internal image-reference" href="_images/CEL-PanelData.png"><img alt="Scenario3" class="align-center" src="_images/CEL-PanelData.png" style="width: 400px;" /></a>
<p>In general, DiD methods are applied in cases where we have a substantial number of units that are exposed to the policy, and researchers are willing to make a “parallel trends” assumption which implies that we can adequately control for selection effects by accounting for additive unit-specific and time-specific fixed effects. In contrast, synthetic control (SC) methods, introduced in a setting with only a single (or small number) of units exposed, seek to compensate for the lack of parallel trends by re-weighting units to match their pre-exposure trends. [2]</p>
</section>
</section>
<section id="a-name-pl-a-causal-policy-learning-cpl">
<h2><a name="PL"></a> Causal Policy Learning (CPL)<a class="headerlink" href="#a-name-pl-a-causal-policy-learning-cpl" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<p>This chapter focuses on six common data dependence structures in decision making, including <span class="xref myst"><strong>Fixed Policy with Independent States</strong></span>, <span class="xref myst"><strong>Fixed Policy with Markovian State Transition</strong></span>, <span class="xref myst"><strong>Fixed Policy with Non-Markovian State Transition</strong></span>, <span class="xref myst"><strong>Adaptive Policy with Independent States</strong></span>, <span class="xref myst"><strong>Adaptive Policy with Markovian State Transition</strong></span>, and <span class="xref myst"><strong>Adaptive Policy with Non-Markovian State Transition</strong></span>. The similarities and differences between six paradigms are summarized as follows.</p>
<p><img alt="Table_of_Six_Scenarios.png" src="_images/Table_of_Six_Scenarios.png" /></p>
<section id="a-name-case1-a-paradigm-1-fixed-policy-with-independent-states">
<h3><a name="Case1"></a> Paradigm 1: Fixed Policy with Independent States<a class="headerlink" href="#a-name-case1-a-paradigm-1-fixed-policy-with-independent-states" title="Permalink to this heading">#</a></h3>
<p>As the figure illustrated, observations in Paradigm 1 are i.i.d. sampled. For each observation, there are three components, <span class="math notranslate nohighlight">\(S_i\)</span> is the context information if there is any, <span class="math notranslate nohighlight">\(A_i\)</span> is the action taken, and <span class="math notranslate nohighlight">\(R_i\)</span> is the reward received. When there is contextual information, the action would be affected by the contextual information, while the final reward would be affected by both the contextual information and the action. A classical class of problems that are widely studied in this context is the <strong>Single-Stage Dynamic Treatment Regime (DTR)</strong>[1]. In this book, we mainly focus on methods for policy evaluation and policy optimization for Single-Stage DTR, with a detailed map in <span class="xref myst">Appendix A</span></p>
</section>
<section id="a-name-case2-a-paradigm-2-fixed-policy-with-markovian-state-transition">
<h3><a name="Case2"></a> Paradigm 2: Fixed Policy with Markovian State Transition<a class="headerlink" href="#a-name-case2-a-paradigm-2-fixed-policy-with-markovian-state-transition" title="Permalink to this heading">#</a></h3>
<p>The Paradigm 2 is well-known as Markov Decision Process (MDP), whose main characteristic is the Markovian state transition. In particular, while <span class="math notranslate nohighlight">\(A_t\)</span> is only affected by <span class="math notranslate nohighlight">\(S_t\)</span>, both <span class="math notranslate nohighlight">\(R_t\)</span> and <span class="math notranslate nohighlight">\(S_{t+1}\)</span> would be affected by <span class="math notranslate nohighlight">\((S_t,A_t)\)</span>. Given <span class="math notranslate nohighlight">\(S_{t}, A_t\)</span>, a standard assumption of MDP problems is that <span class="math notranslate nohighlight">\(R_t\)</span> and <span class="math notranslate nohighlight">\(S_{t+1}\)</span> are independent of previous observations. A list of related learning methods will be introduced, with a map in <span class="xref myst">Appendix B</span>.</p>
<p>&lt;&lt;&lt;&lt;&lt;&lt;&lt; Updated upstream</p>
</section>
<section id="a-name-case3-a-paradigm-3-fixed-policy-with-non-markovian-state-transition">
<h3><a name="Case3"></a> Paradigm 3: Fixed Policy with Non-Markovian State Transition<a class="headerlink" href="#a-name-case3-a-paradigm-3-fixed-policy-with-non-markovian-state-transition" title="Permalink to this heading">#</a></h3>
<p>When a history-independent policy is applied, the Paradigm 3 takes all the possible causal relationships into account and is well-known as the multiple-stage DTR problem [1]. In this book, we introduce two classical learning methods, including Q-learning and A-learning (See a map in <span class="xref myst">Appendix C</span>)</p>
</section>
<section id="a-name-case4-a-paradigm-4-adaptive-policy-with-independent-states">
<h3><a name="Case4"></a> Paradigm 4: Adaptive Policy with Independent States<a class="headerlink" href="#a-name-case4-a-paradigm-4-adaptive-policy-with-independent-states" title="Permalink to this heading">#</a></h3>
<p>The Paradigm 4 setting is widely examined in the online decision making literature, especially the bandits, where the treatment policy is time-adaptive. Specifically, <span class="math notranslate nohighlight">\(H_{t-1}\)</span> includes all the previous observations up to time <span class="math notranslate nohighlight">\(t-1\)</span> (include observations at time <span class="math notranslate nohighlight">\(t-1\)</span>) and is used to update the action policy at time <span class="math notranslate nohighlight">\(t\)</span>, and therefore affect the action <span class="math notranslate nohighlight">\(A_t\)</span>. While <span class="math notranslate nohighlight">\(S_t\)</span> is i.i.d sampled from the correponding distribution, <span class="math notranslate nohighlight">\(R_t\)</span> is influenced by both <span class="math notranslate nohighlight">\(A_t\)</span> and <span class="math notranslate nohighlight">\(S_t\)</span>. Finally, the new observation <span class="math notranslate nohighlight">\((S_t,A_t,R_t)\)</span>, in conjunction with all previous observations, would then be formulated as <span class="math notranslate nohighlight">\(H_{t+1}\)</span> and affect <span class="math notranslate nohighlight">\(A_{t+1}\)</span> only. A structure that lacks contextual information <span class="math notranslate nohighlight">\(S_t\)</span> is also very common. In this book, a list of bandits algorithms would be introduced, with a detailed map in <span class="xref myst">Appendix D</span>.</p>
</section>
<section id="a-name-case5-a-paradigm-5-adaptive-policy-with-markovian-state-transition">
<h3><a name="Case5"></a> Paradigm 5: Adaptive Policy with Markovian State Transition<a class="headerlink" href="#a-name-case5-a-paradigm-5-adaptive-policy-with-markovian-state-transition" title="Permalink to this heading">#</a></h3>
</section>
</section>
</section>
<section class="tex2jax_ignore mathjax_ignore" id="building-upon-the-mdp-structure-when-an-adaptive-policy-is-applied-the-paradigm-5-clearly-depicts-the-data-generating-process-in-which-s-t-follows-the-markovian-state-transition-and-a-t-would-be-affected-by-all-previous-observations-h-t-1">
<h1>Building upon the MDP structure, when an adaptive policy is applied, the Paradigm 5 clearly depicts the data-generating process, in which  <span class="math notranslate nohighlight">\(S_t\)</span> follows the Markovian state transition and <span class="math notranslate nohighlight">\(A_t\)</span> would be affected by all previous observations <span class="math notranslate nohighlight">\(H_{t-1}\)</span>.<a class="headerlink" href="#building-upon-the-mdp-structure-when-an-adaptive-policy-is-applied-the-paradigm-5-clearly-depicts-the-data-generating-process-in-which-s-t-follows-the-markovian-state-transition-and-a-t-would-be-affected-by-all-previous-observations-h-t-1" title="Permalink to this heading">#</a></h1>
<section id="a-name-case3-a-scenario-3-fixed-policy-with-non-markovian-state-transition">
<h2><a name="Case3"></a> Scenario 3: Fixed Policy with Non-Markovian State Transition<a class="headerlink" href="#a-name-case3-a-scenario-3-fixed-policy-with-non-markovian-state-transition" title="Permalink to this heading">#</a></h2>
<p>When a history-independent policy is applied, the Scenario 3 takes all the possible causal relationships into account and is well-known as the multiple-stage DTR problem [1].
In this book, we introduce two classical learning methods, including Q-learning and A-learning (See a map in <span class="xref myst">Appendix C</span>)</p>
</section>
<section id="a-name-case4-a-scenario-4-adaptive-policy-with-independent-states">
<h2><a name="Case4"></a> Scenario 4: Adaptive Policy with Independent States<a class="headerlink" href="#a-name-case4-a-scenario-4-adaptive-policy-with-independent-states" title="Permalink to this heading">#</a></h2>
<p>The Scenario 4 setting is widely examined in the online decision making literature, where the data collection policy is adaptive to the data it has seen. Specifically, <span class="math notranslate nohighlight">\(H_{t-1}\)</span> includes all the previous observations up to time <span class="math notranslate nohighlight">\(t-1\)</span> (include observations at time <span class="math notranslate nohighlight">\(t-1\)</span>) and is used to update the action policy at time <span class="math notranslate nohighlight">\(t\)</span>, and therefore affect the action <span class="math notranslate nohighlight">\(A_t\)</span>. While <span class="math notranslate nohighlight">\(S_t\)</span> is i.i.d sampled from the correponding distribution, <span class="math notranslate nohighlight">\(R_t\)</span> is influenced by both <span class="math notranslate nohighlight">\(A_t\)</span> and <span class="math notranslate nohighlight">\(S_t\)</span>. Finally, the new observation <span class="math notranslate nohighlight">\((S_t,A_t,R_t)\)</span>, in conjunction with all previous observations, would then be formulated as <span class="math notranslate nohighlight">\(H_{t+1}\)</span> and affect <span class="math notranslate nohighlight">\(A_{t+1}\)</span> only. A structure that lacks contextual information <span class="math notranslate nohighlight">\(S_t\)</span> is also very common. In this book, a list of bandits algorithms would be introduced, with a detailed map in <span class="xref myst">Appendix D</span>.</p>
</section>
<section id="a-name-case5-a-scenario-5-adaptive-policy-with-markovian-state-transition">
<h2><a name="Case5"></a> Scenario 5: Adaptive Policy with Markovian State Transition<a class="headerlink" href="#a-name-case5-a-scenario-5-adaptive-policy-with-markovian-state-transition" title="Permalink to this heading">#</a></h2>
<p>Building upon the MDP structure, when an adaptive policy is applied, the Scenario 5 clearly depicts the data-generating process, in which  <span class="math notranslate nohighlight">\(S_t\)</span> follows the Markovian state transition and <span class="math notranslate nohighlight">\(A_t\)</span> would be affected by all previous observations <span class="math notranslate nohighlight">\(H_{t-1}\)</span>.
This corresponds to the typical online RL setup.</p>
<blockquote>
<div><blockquote>
<div><blockquote>
<div><blockquote>
<div><blockquote>
<div><blockquote>
<div><blockquote>
<div><p>Stashed changes</p>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</div></blockquote>
</section>
<section id="a-name-case6-a-paradigm-6-adaptive-policy-with-non-markovian-state-transition">
<h2><a name="Case6"></a> Paradigm 6: Adaptive Policy with Non-Markovian State Transition<a class="headerlink" href="#a-name-case6-a-paradigm-6-adaptive-policy-with-non-markovian-state-transition" title="Permalink to this heading">#</a></h2>
<p><strong>Extensions.</strong> Along the y-axis, we can further consider the case where the data collection policy depends on some unobservable variables, which correspond to the <em>confounded</em> problems.</p>
</section>
<section id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this heading">#</a></h2>
<hr class="docutils" />
<section id="a-name-singeldtr-a-a-paradigm-1">
<h3><a name="SingelDTR"></a> A. Paradigm 1<a class="headerlink" href="#a-name-singeldtr-a-a-paradigm-1" title="Permalink to this heading">#</a></h3>
<a class="reference internal image-reference" href="_images/CPL_Paradigm1.png"><img alt="Scenario1" class="align-center" src="_images/CPL_Paradigm1.png" style="width: 500px;" /></a>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Algorithm</p></th>
<th class="head text-center"><p>Treatment Type</p></th>
<th class="head text-center"><p>Outcome Type</p></th>
<th class="head text-center"><p>Single Stage?</p></th>
<th class="head text-center"><p>Multiple Stages?</p></th>
<th class="head text-center"><p>Infinite Horizon?</p></th>
<th class="head text-center"><p>Evaluation?</p></th>
<th class="head text-center"><p>Optimization?</p></th>
<th class="head text-center"><p>C.I.?</p></th>
<th class="head text-center"><p>Advantages</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://www.jmlr.org/papers/volume6/murphy05a/murphy05a.pdf">Q-Learning</a></p></td>
<td class="text-center"><p>Discrete</p></td>
<td class="text-center"><p>Continuous (Mean)</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://www.researchgate.net/profile/Eric-Laber/publication/221665211_Q-_and_A-Learning_Methods_for_Estimating_Optimal_Dynamic_Treatment_Regimes/links/58825d074585150dde402268/Q-and-A-Learning-Methods-for-Estimating-Optimal-Dynamic-Treatment-Regimes.pdf">A-Learning</a></p></td>
<td class="text-center"><p>Discrete</p></td>
<td class="text-center"><p>Continuous (Mean)</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://www.tandfonline.com/doi/pdf/10.1080/01621459.2012.695674?casa_token=bwkVvffpyFcAAAAA:hlN58Fbz59blLj5npZFjEQD-HkPeMevEN5pWWLu_vuIVxPWl5aYShgCVHUVeODAfj6Pr8DpzGFlPZ1E">OWL</a></p></td>
<td class="text-center"><p>Discrete</p></td>
<td class="text-center"><p>Continuous (Mean)</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❗BOWL</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>❗TODO</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://doi.org/10.1080/01621459.2017.1330204">Quatile-OTR</a></p></td>
<td class="text-center"><p>Discrete</p></td>
<td class="text-center"><p>Continuous (Quantiles)</p></td>
<td class="text-center"><p>✅✏️</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>✅✏️</p></td>
<td class="text-center"><p>✅✏️</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2021/file/816b112c6105b3ebd537828a39af4818-Paper.pdf">Deep Jump Learner</a></p></td>
<td class="text-center"><p>Continuous</p></td>
<td class="text-center"><p>Continuous/Discrete</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>Flexible to implement &amp; Fast to Converge</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>Kernel-Based Learner</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Outcome Learning</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</section>
<section id="a-name-mdp-a-b-paradigm-2">
<h3><a name="MDP"></a> B. Paradigm 2<a class="headerlink" href="#a-name-mdp-a-b-paradigm-2" title="Permalink to this heading">#</a></h3>
<a class="reference internal image-reference" href="_images/CPL_Paradigm2.png"><img alt="Scenario2" class="align-center" src="_images/CPL_Paradigm2.png" style="width: 400px;" /></a>
</section>
<section id="a-name-multidtr-a-c-paradigm-3">
<h3><a name="MultiDTR"></a> C. Paradigm 3<a class="headerlink" href="#a-name-multidtr-a-c-paradigm-3" title="Permalink to this heading">#</a></h3>
<a class="reference internal image-reference" href="_images/CPL_Paradigm3.png"><img alt="Scenario3" class="align-center" src="_images/CPL_Paradigm3.png" style="width: 300px;" /></a>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>Algorithm</p></th>
<th class="head text-center"><p>Treatment Type</p></th>
<th class="head text-center"><p>Outcome Type</p></th>
<th class="head text-center"><p>Evaluation?</p></th>
<th class="head text-center"><p>Optimization?</p></th>
<th class="head text-center"><p>C.I.?</p></th>
<th class="head text-center"><p>Advantages</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://www.jmlr.org/papers/volume6/murphy05a/murphy05a.pdf">Q-Learning</a></p></td>
<td class="text-center"><p>Discrete</p></td>
<td class="text-center"><p>Continuous (Mean)</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://www.researchgate.net/profile/Eric-Laber/publication/221665211_Q-_and_A-Learning_Methods_for_Estimating_Optimal_Dynamic_Treatment_Regimes/links/58825d074585150dde402268/Q-and-A-Learning-Methods-for-Estimating-Optimal-Dynamic-Treatment-Regimes.pdf">A-Learning</a></p></td>
<td class="text-center"><p>Discrete</p></td>
<td class="text-center"><p>Continuous (Mean)</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
</tbody>
</table>
</section>
<section id="a-name-bandits-a-d-paradigm-4">
<h3><a name="Bandits"></a> D. Paradigm 4<a class="headerlink" href="#a-name-bandits-a-d-paradigm-4" title="Permalink to this heading">#</a></h3>
<a class="reference internal image-reference" href="_images/CPL_Paradigm4.png"><img alt="Scenario4" class="align-center" src="_images/CPL_Paradigm4.png" style="width: 900px;" /></a>
<table class="table">
<thead>
<tr class="row-odd"><th class="head text-left"><p>algorithm</p></th>
<th class="head text-center"><p>Reward</p></th>
<th class="head text-center"><p>with features?</p></th>
<th class="head text-center"><p>Advantage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-left"><p><strong>Multi-Armed Bandits</strong></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><span class="xref myst"><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy</span></p></td>
<td class="text-center"><p>Binary/Gaussian</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>Simple</p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/sampling/TS_Tutorial.pdf">TS</a></p></td>
<td class="text-center"><p>Binary/Guaasian</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf">UCB1</a></p></td>
<td class="text-center"><p>Binary/Gaussian</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Contextual Bandits</strong></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>LinTS</p></td>
<td class="text-center"><p><a class="reference external" href="http://proceedings.mlr.press/v108/kveton20a/kveton20a.pdf">GLM</a>/<a class="reference external" href="http://proceedings.mlr.press/v28/agrawal13.pdf">Gaussian</a></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>LinUCB</p></td>
<td class="text-center"><p><a class="reference external" href="http://proceedings.mlr.press/v70/li17c/li17c.pdf">GLM</a>/<a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/1772690.1772758?casa_token=CJjeIziLmjEAAAAA:CkRvgHQNqpy10rzcUP5kx31NWJmgSldd6zx8wZxskZYCoCc8v7EDIw3t3Gk1_6mfurqQTqRZ7fVA">Guassian</a></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><strong>Meta Bandits</strong></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p>Meta-TS</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p>MTSS</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><strong>Structured Bandits</strong></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><em>Learning to Rank</em></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="http://proceedings.mlr.press/v89/cheung19a/cheung19a.pdf">TS-Cascade</a></p></td>
<td class="text-center"><p>Binary</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://arxiv.org/pdf/1603.05359.pdf">CascadeLinTS</a></p></td>
<td class="text-center"><p>Binary</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://arxiv.org/pdf/2202.13227.pdf">MTSS-Cascade</a></p></td>
<td class="text-center"><p>Binary</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>Scalable, Robust, accounts for inter-item heterogeneity</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><em>Combinatorial Optimization</em></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="http://proceedings.mlr.press/v80/wang18a/wang18a.pdf">CombTS</a></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="http://proceedings.mlr.press/v37/wen15.pdf">CombLinTS</a></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://arxiv.org/pdf/2202.13227.pdf">MTSS-Comb</a></p></td>
<td class="text-center"><p>Continuous</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>Scalable, Robust, accounts for inter-item heterogeneity</p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><em>Assortment Optimization</em></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="http://proceedings.mlr.press/v65/agrawal17a/agrawal17a.pdf">MNL-Thompson-Beta</a></p></td>
<td class="text-center"><p>Binary</p></td>
<td class="text-center"><p></p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-left"><p><a class="reference external" href="https://proceedings.neurips.cc/paper/2019/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf">TS-Contextual-MNL</a></p></td>
<td class="text-center"><p>Binary</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-left"><p><a class="reference external" href="https://arxiv.org/pdf/2202.13227.pdf">MTSS-MNL</a></p></td>
<td class="text-center"><p>Binary</p></td>
<td class="text-center"><p>✅</p></td>
<td class="text-center"><p>Scalable, Robust, accounts for inter-item heterogeneity</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this heading">#</a></h2>
<p>[1] Tsiatis, A. A., Davidian, M., Holloway, S. T., &amp; Laber, E. B. (2019). Dynamic treatment regimes: Statistical methods for precision medicine. Chapman and Hall/CRC.</p>
<p>[2] Dmitry Arkhangelsky, Susan Athey, David A Hirshberg, Guido W Imbens, and Stefan Wager. Synthetic
difference in differences. Technical report, National Bureau of Economic Research, 2019.</p>
</section>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
<div class="toctree-wrapper compound">
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

            </article>
            

            
            
            <footer class="bd-footer-article">
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
  <a class='right-next' id="next-link" href="0_Motivating_Examples/CSL.html" title="next page">
  <div class="prev-next-info">
      <p class="prev-next-subtitle">next</p>
      <p class="prev-next-title"><em>Causal Structure Learning (CSL)</em></p>
  </div>
  <i class="fa-solid fa-angle-right"></i>
  </a>
</div>
            </footer>
            
          </div>
          
          
          
            <div class="bd-sidebar-secondary bd-toc">
              
<div class="toc-item">
  
<div class="tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
</div>
<nav id="bd-toc-nav" class="page-toc">
    <ul class="visible nav section-nav flex-column">
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#">
   Overview
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#introduction">
     Introduction
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-to-expect">
     What to expect?
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-sl-a-causal-structure-learning-csl">
     <a name="SL">
     </a>
     Causal Structure Learning (CSL)
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-ml-a-causal-effect-learning-cel">
     <a name="ML">
     </a>
     Causal Effect Learning (CEL)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-1-i-i-d">
       <a name="Case1">
       </a>
       Paradigm 1: I.I.D
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-2-markovian-state-transition">
       <a name="Case1">
       </a>
       Paradigm 2: Markovian State Transition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-3-panel-data">
       <a name="Case1">
       </a>
       Paradigm 3: Panel Data
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-pl-a-causal-policy-learning-cpl">
     <a name="PL">
     </a>
     Causal Policy Learning (CPL)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case1-a-paradigm-1-fixed-policy-with-independent-states">
       <a name="Case1">
       </a>
       Paradigm 1: Fixed Policy with Independent States
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case2-a-paradigm-2-fixed-policy-with-markovian-state-transition">
       <a name="Case2">
       </a>
       Paradigm 2: Fixed Policy with Markovian State Transition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case3-a-paradigm-3-fixed-policy-with-non-markovian-state-transition">
       <a name="Case3">
       </a>
       Paradigm 3: Fixed Policy with Non-Markovian State Transition
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case4-a-paradigm-4-adaptive-policy-with-independent-states">
       <a name="Case4">
       </a>
       Paradigm 4: Adaptive Policy with Independent States
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-case5-a-paradigm-5-adaptive-policy-with-markovian-state-transition">
       <a name="Case5">
       </a>
       Paradigm 5: Adaptive Policy with Markovian State Transition
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="#building-upon-the-mdp-structure-when-an-adaptive-policy-is-applied-the-paradigm-5-clearly-depicts-the-data-generating-process-in-which-s-t-follows-the-markovian-state-transition-and-a-t-would-be-affected-by-all-previous-observations-h-t-1">
   Building upon the MDP structure, when an adaptive policy is applied, the Paradigm 5 clearly depicts the data-generating process, in which
   <span class="math notranslate nohighlight">
    \(S_t\)
   </span>
   follows the Markovian state transition and
   <span class="math notranslate nohighlight">
    \(A_t\)
   </span>
   would be affected by all previous observations
   <span class="math notranslate nohighlight">
    \(H_{t-1}\)
   </span>
   .
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case3-a-scenario-3-fixed-policy-with-non-markovian-state-transition">
     <a name="Case3">
     </a>
     Scenario 3: Fixed Policy with Non-Markovian State Transition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case4-a-scenario-4-adaptive-policy-with-independent-states">
     <a name="Case4">
     </a>
     Scenario 4: Adaptive Policy with Independent States
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case5-a-scenario-5-adaptive-policy-with-markovian-state-transition">
     <a name="Case5">
     </a>
     Scenario 5: Adaptive Policy with Markovian State Transition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-name-case6-a-paradigm-6-adaptive-policy-with-non-markovian-state-transition">
     <a name="Case6">
     </a>
     Paradigm 6: Adaptive Policy with Non-Markovian State Transition
    </a>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#appendix">
     Appendix
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-singeldtr-a-a-paradigm-1">
       <a name="SingelDTR">
       </a>
       A. Paradigm 1
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-mdp-a-b-paradigm-2">
       <a name="MDP">
       </a>
       B. Paradigm 2
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-multidtr-a-c-paradigm-3">
       <a name="MultiDTR">
       </a>
       C. Paradigm 3
      </a>
     </li>
     <li class="toc-h3 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-name-bandits-a-d-paradigm-4">
       <a name="Bandits">
       </a>
       D. Paradigm 4
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="#reference">
     Reference
    </a>
   </li>
  </ul>
 </li>
</ul>

</nav>
</div>

            </div>
          
          
        </div>
        <footer class="bd-footer-content">
          <div class="bd-footer-content__inner">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Causal Decision Making Team
</p>

  </div>
  
  <div class="footer-item">
    
<p class="copyright">

    &copy; Copyright 2022.<br>

</p>

  </div>
  
  <div class="footer-item">
    <p class="last-updated">
Last updated on None.<br>
</p>
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </div>
        </footer>
        

      </main>
    </div>
  </div>

  
    
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=796348d33e8b1d947c94"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=796348d33e8b1d947c94"></script>

  </body>
</html>