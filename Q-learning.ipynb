{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "\n",
    "Q-learning is a classic method of Reinforcement Learning. Early in 2000, it was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. In the following, we would start from a simple case having only one decision point, and then introduce the multistage case with multiple decision points. Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n",
    "\n",
    "### Single Decision Point\n",
    "\n",
    "- **Application Situation**: Suppose we have a dataset contains observations from $N$ individuals. For each individual $i$, we have $\\{\\mathbf{X}_{i},A_{i},Y_{i}\\}$, $i=1,\\cdots,N$. $\\mathbf{X}_{i}$ includes the feature information, $A_{i}$ is the action taken, and $Y_{i}$ is the observed reward received. The target of Q-learing is to find an optimal policy $\\pi$ that can maximize the expected reward received. In other words, training a model with the observed dataset, we want to find an optimal policy that can help us determine the optimal action for each individual to optimize the reward.\n",
    "\n",
    "- **Basic Logic**: Q-learning with single decision point is mainly a regression modeling problem, as the major component is to find the relationship between $Y$ and $\\{X,A\\}$. Let's first define a Q-function, such that\n",
    "\\begin{align}\n",
    "    Q(x,a) = E(Y|X=x, A=a).\n",
    "\\end{align} Then, to find the optimal policy is equivalent to solve\n",
    "\\begin{align}\n",
    "    \\text{arg max}_{\\pi}Q(x_{i},\\pi(x_{i})).\n",
    "\\end{align} \n",
    "\n",
    "- **Key Steps**:\n",
    "    1. Fitted a model $\\hat{Q}(x,a,\\hat{\\beta})$, which can be solved directly by existing approaches (i.e., OLS, .etc),\n",
    "    2. For each individual find the optimal action $a_{i}$ such that $a_{i} = \\text{arg max}_{a}\\hat{Q}(x_{i},a,\\hat{\\beta})$.\n",
    "\n",
    "### Multiple Decision Points\n",
    "\n",
    "- **Application Situation**: Suppose we have a dataset contains observations from $N$ individuals. For each individual $i$, the observed data is structured as follows\n",
    "\\begin{align}\n",
    "(X_{1i},A_{1i},\\cdots,X_{Ti},A_{Ti},Y), i=1,\\cdots, N.\n",
    "\\end{align} Let $h_{ti}=\\{X_{1i},A_{1i},\\cdots,X_{ti},A_{ti}\\})$ includes all the information observed till step t. The target of Q-learing is to find an optimal policy $\\pi$ that can maximize the expected reward received at the end of the final decision point $T$. In other words, training a model with the observed dataset, we want to find an optimal policy that can help us determine the optimal sequence of actions for each individual to optimize the reward.\n",
    "\n",
    "- **Basic Logic**: For the multistage case, we utilize a backward iterative framework, which means that we start from the final decision point T and then gioing backwards till the initial decision point. At the final step $T$, it is a standard regression modeling problem that is exactly the same as what we did for the single decision point case. Particularly, we posit a model $Q_{T}(h_{T},a_{T})$ for the observed outcome $Y$, and then the optimal policy at step $T$ is derived as $\\text{arg max}_{\\pi_{T}}Q_{T}(h_{T},\\pi_{T}(h_{T}))$. For the decision point $T-1$ till the decision point $1$, a new term is introduced, which is the pseudo-outcome $\\tilde{Y}_{t}$.\n",
    "\\begin{align}\n",
    "\\tilde{Y}_{t} = \\text{max}_{\\pi_{t}}\\hat{Q}_{t}(h_{t},\\pi_{t}(h_{t}),\\hat{\\beta}_{t})\n",
    "\\end{align} By doing so, the pseudo-outcome taking the **delayed effect** into account to help explore the optimal policy. Then, for each decision point $t<T$, with the $\\tilde{Y}_{t+1}$ calculated, we repeat the regression modeling step for $\\tilde{Y}_{t+1}$. After obtainning the fitted model $\\hat{Q}_{t}(h_{t},a_{t},\\hat{\\beta}_{t})$, the optimal policy is obtained as $\\text{arg max}_{\\pi_{t}}Q_{t}(h_{t},\\pi_{t}(h_{t}))$.\n",
    "\n",
    "- **Key Steps**: \n",
    "    1. At the final decision point $t=T$, fitted a model $\\hat{Q}_{T}(h_{T},a_{T},\\hat{\\beta}_{T})$ for the observed outcome $Y$;\n",
    "    2. For each individual $i$, calculated the pesudo-outcome $\\tilde{Y}_{Ti}=\\text{max}_{\\pi}\\hat{Q}_{T}(h_{Ti},\\pi(h_{Ti}),\\hat{\\beta}_{T})$, and the optimal action $a_{Ti}=\\text{arg max}_{a}\\hat{Q}_{T}(h_{Ti},a,\\hat{\\beta}_{T})$;\n",
    "    3. For decision point $t = T-1,\\cdots, 1$,\n",
    "        1. fitted a model $\\hat{Q}_{t}(h_{t},a_{t},\\hat{\\beta}_{t})$ for the pseudo-outcome $\\tilde{Y}_{t+1}$\n",
    "        2. For each individual $i$, calculated the pesudo-outcome $\\tilde{Y}_{ti}=\\text{max}_{\\pi}\\hat{Q}_{t}(h_{ti},\\pi(h_{ti}),\\hat{\\beta}_{t})$, and the optimal action $a_{ti}=\\text{arg max}_{a}\\hat{Q}_{t}(h_{ti},a,\\hat{\\beta}_{t})$;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import QLearning\n",
    "from causaldm.test import shared_simulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset (dataset from the DTR book)\n",
    "import pandas as pd\n",
    "file = pd.read_csv(\"hyper.txt\", sep=',')\n",
    "file['Y'] = file['SBP0']-file['SBP6']\n",
    "hyper = file\n",
    "Y = hyper['Y']\n",
    "X = hyper[['W','K','Cr','Ch']]\n",
    "A = hyper['A']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "# specify the model you would like to use\n",
    "# If want to include all the variable in X and A with no specific model structure, then use either \"Y~.\" or \"Y~X+A\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~Ch+K+A+Ch*A+K*A\",\n",
    "              'A': [0,1],\n",
    "             'feature':X,\n",
    "             'outcome':Y}]\n",
    "\n",
    "# train the policy\n",
    "QLearn.train(X, A, Y, model_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    1\n",
       "3    1\n",
       "4    1\n",
       "5    1\n",
       "6    1\n",
       "7    0\n",
       "8    1\n",
       "9    1\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend action\n",
    "QLearn.recommend_action(X)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.215622865312115"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the estimated value of the optimal regime\n",
    "QLearn.estimate_value(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept   -15.604845\n",
       "Ch           -0.203472\n",
       "K            12.284852\n",
       "A           -61.097909\n",
       "Ch:A          0.504816\n",
       "K:A          -6.609876\n",
       "dtype: float64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitted Model\n",
    "QLearn.fitted_model.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample data\n",
    "from causaldm.test import OWL_simu\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "instance = OWL_simu.generate_test_case(setup = 'case1', N = 1000, seed = 0, p = 5, sigma = 1)\n",
    "X, A, Y = instance['XAY']\n",
    "X = pd.DataFrame(X);A = pd.DataFrame(A);Y = pd.DataFrame(Y)\n",
    "\n",
    "# initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "# specify the model you would like to use\n",
    "model_info = [{\"model\": \"Y~X+A\", \n",
    "              'A': [0,1],\n",
    "             'feature':X,\n",
    "             'outcome':Y}]\n",
    "\n",
    "# train the policy\n",
    "QLearn.train(X, A, Y, model_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
