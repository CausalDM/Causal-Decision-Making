{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d00c0e8c",
   "metadata": {},
   "source": [
    "# Q-Learning (Multiple Stages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "70424f24",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ: '../CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_21412\\2982377520.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../CausalDM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑÊñá‰ª∂„ÄÇ: '../CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3502a69",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "\n",
    "Q-learning is a classic method of Reinforcement Learning. Early in 2000, it was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. In the following, we would start from a simple case having only one decision point and then introduce the multistage case with multiple decision points. Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd831758",
   "metadata": {},
   "source": [
    "## Multiple Decision Points\n",
    "\n",
    "- **Application Situation**: Suppose we have a dataset containning observations from $N$ individuals. For each individual $i$, the observed data is structured as follows\n",
    "    \\begin{align}\n",
    "    (X_{1i},A_{1i},\\cdots,X_{Ti},A_{Ti},Y), i=1,\\cdots, N.\n",
    "    \\end{align} \n",
    "    Let $h_{ti}=\\{X_{1i},A_{1i},\\cdots,X_{ti}\\})$ includes all the information observed till step t. The target of Q-learning is to find an optimal policy $\\pi$ that can maximize the expected reward received at the end of the final decision point $T$. In other words, by training a model with the observed dataset, we want to find an optimal policy that can help us determine the optimal sequence of actions for each individual to optimize the reward.\n",
    "\n",
    "- **Basic Logic**: For multistage cases, we apply a backward iterative approach, which means that we start from the final decision point T and work our way backward to the initial decision point. At the final step $T$, it is again a standard regression modeling problem that is the same as what we did for the single decision point case. Particularly, we posit a model $Q_{T}(h_{T},a_{T})$ for the observed outcome $Y$, and then the optimal policy at step $T$ is derived as $\\text{arg max}_{\\pi_{T}}Q_{T}(h_{T},\\pi_{T}(h_{T}))$. For the decision point $T-1$ till the decision point $1$, a new term is introduced, which is the pseudo-outcome $\\tilde{Y}_{t}$.\n",
    "    \\begin{align}\n",
    "    \\tilde{Y}_{t} = \\text{max}_{\\pi_{t}}\\hat{Q}_{t}(h_{t},\\pi_{t}(h_{t}),\\hat{\\beta}_{t})\n",
    "    \\end{align}\n",
    "    By doing so, the pseudo-outcome taking the **delayed effect** into account to help explore the optimal policy. Then, for each decision point $t<T$, with the $\\tilde{Y}_{t+1}$ calculated, we repeat the regression modeling step for $\\tilde{Y}_{t+1}$. After obtaining the fitted model $\\hat{Q}_{t}(h_{t},a_{t},\\hat{\\beta}_{t})$, the optimal policy is obtained as $\\text{arg max}_{\\pi_{t}}Q_{t}(h_{t},\\pi_{t}(h_{t}))$.\n",
    "\n",
    "- **Key Steps**: \n",
    "    1. At the final decision point $t=T$, fitted a model $\\hat{Q}_{T}(h_{T},a_{T},\\hat{\\beta}_{T})$ for the observed outcome $Y$;\n",
    "    2. For each individual $i$, calculated the pseudo-outcome $\\tilde{Y}_{Ti}=\\text{max}_{\\pi}\\hat{Q}_{T}(h_{Ti},\\pi(h_{Ti}),\\hat{\\beta}_{T})$, and the optimal action $a_{Ti}=\\text{arg max}_{a}\\hat{Q}_{T}(h_{Ti},a,\\hat{\\beta}_{T})$;\n",
    "    3. For decision point $t = T-1,\\cdots, 1$,\n",
    "        1. fitted a model $\\hat{Q}_{t}(h_{t},a_{t},\\hat{\\beta}_{t})$ for the pseudo-outcome $\\tilde{Y}_{t+1}$\n",
    "        2. For each individual $i$, calculated the pseudo-outcome $\\tilde{Y}_{ti}=\\text{max}_{\\pi}\\hat{Q}_{t}(h_{ti},\\pi(h_{ti}),\\hat{\\beta}_{t})$, and the optimal action $a_{ti}=\\text{arg max}_{a}\\hat{Q}_{t}(h_{ti},a,\\hat{\\beta}_{t})$;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b6e77f9",
   "metadata": {},
   "source": [
    "### 1. Optimal Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5284b17",
   "metadata": {},
   "source": [
    "#### import package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "07b66823",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: feasible set\n",
    "from causaldm.learners import QLearning\n",
    "from causaldm.test import shared_simulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34710520",
   "metadata": {},
   "source": [
    "#### prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb213d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset (dataset from the DTR book)\n",
    "import pandas as pd\n",
    "#Important!! reset the index is required\n",
    "dataMDP = pd.read_csv(\"dataMDP_feasible.txt\", sep=',')#.reset_index(drop=True) \n",
    "Y = dataMDP['Y']\n",
    "X = dataMDP[['CD4_0','CD4_6','CD4_12']]\n",
    "A = dataMDP[['A1','A2','A3']]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494d7bd3",
   "metadata": {},
   "source": [
    "#### train policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bd161775",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "# specify the model you would like to use\n",
    "# If want to include all the variable in X and A with no specific model structure, then use \"Y~.\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~CD4_0+A1+CD4_0*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+A2+CD4_6*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+CD4_12+A3+CD4_12*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]\n",
    "\n",
    "# train the policy\n",
    "QLearn.train(X, A, Y, model_info, T=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ff38d6",
   "metadata": {},
   "source": [
    "#### get the recommend actions and optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c6b15209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt regime:    A3  A2  A1\n",
      "1   0   0   1\n",
      "2   0   0   1\n",
      "3   0   1   1\n",
      "4   0   0   1\n",
      "5   0   1   1\n",
      "opt value: 1113.3004201781748\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = QLearn.recommend().head()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = QLearn.estimate_value()\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b481af9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A3       0\n",
       "A2     550\n",
       "A1    1000\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLearn.recommend().sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bb346d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 1113.2403528901648 Value_std: 3.565141574198611\n"
     ]
    }
   ],
   "source": [
    "# Optional: we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~CD4_0+A1+CD4_0*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+A2+CD4_0*A2+CD4_6*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+CD4_12+A3+CD4_0*A3+CD4_6*A3+CD4_12*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]\n",
    "QLearn.train(X, A, Y, model_info, T=3, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.estimate_value_boots()\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6c6f7b",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c1b83",
   "metadata": {},
   "source": [
    "We use the backward iteration as before. However, here for each round, the pseudo outcome is not the maximum of Q values. Instead, the pseudo outcome at decision point t is defined as below:\n",
    "\n",
    "\\begin{align}\n",
    "\\tilde{Y}_{t} = \\hat{Q}_{t}(h_{t},d_{t}(h_{t}),\\hat{\\beta}_{t})\n",
    "\\end{align}, where $d$ is the fixed regime that we want to evaluate.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3f692a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the fixed regime to be tested\n",
    "# For example, regime d = 1 for all subjects at all decision points\\\n",
    "N=len(X)\n",
    "# !! IMPORTANT: INDEX SHOULD BE THE SAME AS THAT OF THE X,Y,A\n",
    "regime = pd.DataFrame({'A1':[1]*N,\n",
    "                      'A2':[1]*N,\n",
    "                      'A3':[1]*N}).set_index(X.index)\n",
    "#evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~CD4_0+A1+CD4_0*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+A2+CD4_6*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+CD4_12+A3+CD4_12*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]\n",
    "QLearn.train(X, A, Y, model_info, T=3, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.estimate_value_boots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c0262030",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 979.0973431290403 Value_std: 3.8123652853003898\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ed238bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "979.4518636939481"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otional: just estimate the value\n",
    "QLearn.train(X, A, Y, model_info, T=3, regime = regime, evaluate = True)\n",
    "QLearn.estimate_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef3b0e",
   "metadata": {},
   "source": [
    "üí• Placeholder for C.I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef943586",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec18d0b0",
   "metadata": {},
   "source": [
    "!! Already tested for accuracy using the data provided in DTR book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "785f3e7f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}