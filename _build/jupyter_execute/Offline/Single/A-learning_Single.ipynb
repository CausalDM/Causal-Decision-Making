{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e9c48c2",
   "metadata": {},
   "source": [
    "# A-Learning (Single Stage)\n",
    "\n",
    "## Main Idea\n",
    "A-Learning, also known as Advantage Learning, is one of the main approaches to learning the optimal regime and works similarly to Q-learning. However, while Q-learning requires positing regression models to fit the expected outcome, A-learning models the contrasts between treatments and control, directly informing the optimal decision. For example, in the case of **Personalized Incentives**, A-learning aims to find the optimal incentive ($A$) for each user by modeling the difference in expected return-on-investment ($Y$) between treatments. A detailed comparison between Q-learning and A-learning can be found in [1]. While [1] mainly focus on the case with binary treatment options, a complete review of A-learning with multiple treatment options can be found in [2]. Here, following the algorithm in [1], we consider contrast-based A-learning. However, there is an alternative regret-based A-learning introduced in [3]. Some recent extensions to conventional A-learning, such as deep A-learning [4] and high-dimensional A-Learning [5], will be added soon. Overall, A-learning is doubly-robust. In other words, it is less sensitive and more robust to model misspecification. \n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., 0,1,2,3,4, where 0 stands for the control group by convention), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n",
    "\n",
    "## Algorithm Details\n",
    "Suppose there are $m$ number of options, and the action space $\\mathcal{A}=\\{0,1,\\dots,m-1\\}$. Contrast-based A-learning, as the name suggested, aims to learn and estimate the constrast function, $C_{j}(X)$ for each treatment $j=1,2,\\cdots, m-1$. Furthermore, we also need to posit a model for the conditional expected outcome for the control option (treatment $0$), $Q(X,0)$, and the propensity function $\\omega(X,A)$, if the true values are not specified. Detailed definitions are provided in the following.\n",
    "*   Q-function:\n",
    "    \\begin{align}\n",
    "    Q(x,a)=E[Y|X=x, A=a],\n",
    "    \\end{align}\n",
    "    Alternatively, with the contrast function $C_j(X)$ which will be defined later,\n",
    "    \\begin{align}\n",
    "    Q(x,j) = Q(x,0) + C_{j}(x),\\quad j=0,\\dots,m-1.\n",
    "    \\end{align}\n",
    "*   Contrast functions (optimal blip to zero functions)\n",
    "    \\begin{align}\n",
    "    C_{j}(x)=Q(x,j)-Q(x,0),\\quad j=0,\\dots,m-1,\n",
    "    \\end{align}\n",
    "    where $C_{0}(x) = 0$.\n",
    "*   Propensity score\n",
    "    \\begin{align}\n",
    "    \\omega(x,a)=P(A=a|X=x)\n",
    "    \\end{align}\n",
    "*   Optimal regime\n",
    "    \\begin{align}\n",
    "    d^{opt}(x)=\\arg\\max_{j\\in\\mathcal{A}}C_{j}(X)\n",
    "    \\end{align}\n",
    "Positting models, $C_{j}(x,\\psi_{j})$,$Q(x,0,\\phi)$,and $\\omega(x,a,\\gamma)$, A-learning aims to estimate $\\psi_{j}$, $\\phi$, and $\\gamma$ by g-estimation. With the $\\hat{\\psi}_{j}$ in hand, the optimal decision $d^{opt}(x)$ can be directly derived.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. Fitted a model $\\omega(x,a,\\gamma)$, which can be solved directly by existing approaches (i.e., logistic regression, .etc),\n",
    "2. Substituting the $\\hat{\\gamma}$, we estimate the $\\hat{\\psi}_{j}$ and $\\gamma$ by solving the euqations in Appendix A.1 jointly.      \n",
    "3. For each individual find the optimal action $d^{opt}(x_{i})$ such that $d^{opt}(x_{i}) = \\arg\\max_{j\\in\\mathcal{A}}C_{j}(h,\\hat{\\psi_{j}})$.\n",
    "    \n",
    "**Policy Evaluation:**    \n",
    "1. Fitted the functions $\\omega(x,a,\\gamma)$ï¼Œ $\\hat{Q}(x,0,\\hat{\\beta})$, and $\\hat{C}_{j}(x,\\hat{\\psi}_{j})$, based on the sampled dataset\n",
    "2. Estimated the value of a given regime $d$ using the estimated functions, such that, $\\hat{Y}_{i} = \\hat{Q}(x_{i},0,\\hat{\\beta})+I\\{d(x_{i}=j)\\}\\hat{C}_{j}(x,\\hat{\\psi}_{j})$\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilze the boostrap resampling to get the estimated value of the regime and the corresponding estimated standard error. Basically, for each round of bootstrap, we resample a dataset of the same size as the original dataset with replacement, fitted the $Q(x,0)$ function and contrast functions based on the sampled dataset, and estimated the value of a given regime using the estimated $Q(x,0)$ function and contrast functions function. \n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f738ea78",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb33993",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'causaldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_173992\\1249134311.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_util_causaldm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearners\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mALearning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'causaldm'"
     ]
    }
   ],
   "source": [
    "# import learner\n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners import ALearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c4dc7561",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "X,A,Y = get_data(target_col = 'spend', binary_trt = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b7fd03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data into 2d numpy array\n",
    "Y = np.array(Y)\n",
    "X = np.hstack([np.ones((len(X),1)),np.array(X)])# add an intercept column\n",
    "A = np.array(A)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d10eed73",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. specify the model you would like to use\n",
    "model_info = [{'X_prop': [0,1,2], #[0,1,2] here stands for the intercept, recency and history\n",
    "              'X_q0': [0,1,2],\n",
    "               'X_C':{1:[0,1,2],2:[0,1,2]},\n",
    "              'action_space': {'A':[0,1,2]}}] #A in [0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39662b4",
   "metadata": {},
   "source": [
    "By specifing the model_info, we assume  the following models:\n",
    "\\begin{align}\n",
    "Q(x,0,\\phi) &= \\phi_{00}+\\phi_{01}*recency+\\phi_{02}*history,\\\\\n",
    "C_{1}(x,\\psi_{0}) &= 0\\\\\n",
    "C_{1}(x,\\psi_{1}) &= \\psi_{10}+\\psi_{11}*recency+\\psi_{12}*history,\\\\\n",
    "C_{2}(x,\\psi_{2}) &= \\psi_{20}+\\psi_{21}*recency+\\psi_{22}*history,\\\\\n",
    "\\omega(x,a=j,\\gamma) &= \\frac{exp(\\gamma_{j0}+\\gamma_{j1}*recency+\\gamma_{j2}*history)}{\\sum_{j=0}^{2}exp(\\gamma_{j0}+\\gamma_{j1}*recency+\\gamma_{j2}*history)},\\\\\n",
    "\\end{align}\n",
    "where $\\gamma_{00}=\\gamma_{01}=\\gamma_{02}=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "461d6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. initialize the learner\n",
    "ALearn = ALearning.ALearning()\n",
    "#3. train the policy\n",
    "ALearn.train(X, A, Y, model_info, T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8e6c7d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted contrast model: {0: {1: array([ 2.3389e+01, -4.0295e+00,  5.3272e-03]), 2: array([ 2.1025e+01, -4.7135e+00, -2.7582e-03])}}\n",
      "opt regime: A\n",
      "1    376\n",
      "0    202\n",
      "dtype: int64\n",
      "opt value: 126.18615811062617\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = ALearn.recommend_action(X).value_counts()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = ALearn.predict_value(X)\n",
    "print(\"fitted contrast model:\",ALearn.fitted_model['contrast'])\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3176ca24",
   "metadata": {},
   "source": [
    "**Interpretation:** the fitted contrast models are \n",
    "\\begin{align}\n",
    "C_{0}(x,\\psi_{0}) &= 0\\\\\n",
    "C_{1}(x,\\psi_{1}) &= 23.39-4.03*recency+.005*history,\\\\\n",
    "C_{2}(x,\\psi_{2}) &= 21.03-4.71*recency-.003*history,\\\\\n",
    "\\end{align}\n",
    "Therefore, the estimated optimal regime is:\n",
    "1. We would recommend $A=0$ (No E-mail) if $23.39-4.03*recency+.005*history<0$ and $21.03-4.71*recency-.003*history<0$\n",
    "2. Else, we would recommend $A=1$ (Womens E-mail) if $23.39-4.03*recency+.005*history>21.03-4.71*recency-.003*history$\n",
    "3. Else, we would recommend $A=2$ (Mens E-Mail).\n",
    "\n",
    "The estimated value for the estimated optimal regime is 126.19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "818894c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 132.3843662825924 Value_std: 6.989242109807423\n"
     ]
    }
   ],
   "source": [
    "# Optional: \n",
    "#we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "ALearn = ALearning.ALearning()\n",
    "model_info = [{'X_prop': [0,1,2], #[0,1,2] here stands for the intercept, recency and history\n",
    "              'X_q0': [0,1,2],\n",
    "               'X_C':{1:[0,1,2],2:[0,1,2]},\n",
    "              'action_space': {'A':[0,1,2]}}] #A in [0,1,2]\n",
    "ALearn.train(X, A, Y, model_info, T=1, bootstrap = True, n_bs = 100)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.predict_value_boots(X)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a746b554",
   "metadata": {},
   "source": [
    "**Interpretation:** Based on the boostrap with 200 replicates, the estimated optimal value is 132.38 with a standard error of 6.99."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae04a0b1",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e345b3dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.37488160184647"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. specify the fixed regime to be tested\n",
    "# For example, regime d = 0 for all subjects\n",
    "N=len(X)\n",
    "regime = pd.DataFrame({'A':[0]*N})\n",
    "#2. evaluate the regime\n",
    "ALearn = ALearning.ALearning()\n",
    "model_info = [{'X_prop': [0,1,2], #[0,1,2] here stands for the intercept, recency and history\n",
    "              'X_q0': [0,1,2],\n",
    "               'X_C':{1:[0,1,2],2:[0,1,2]},\n",
    "              'action_space': {'A':[0,1,2]}}] #A in [0,1,2]\n",
    "ALearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True)\n",
    "ALearn.predict_value(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a092dff",
   "metadata": {},
   "source": [
    "**Interpretation:** the estimated value of the regime that always sends no emails ($A=0$) is 116.37, under the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f07dc504",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 116.37127842161804 Value_std: 10.2024261616976\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "ALearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.predict_value_boots(X)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb137b",
   "metadata": {},
   "source": [
    "**Interpretation:** the bootstrapped estimated value of the regime that always sends no emails is 116.37 with a bootstrapped standard error 10.20, under the specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b94b2",
   "metadata": {},
   "source": [
    "ðŸ’¥ Placeholder for C.I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e62c685",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Schulte, P. J., Tsiatis, A. A., Laber, E. B., & Davidian, M. (2014). Q-and A-learning methods for estimating optimal dynamic treatment regimes. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4), 640.\n",
    "2. Robins, J. M. (2004). Optimal structural nested models for optimal sequential decisions. In Proceedings of the second seattle Symposium in Biostatistics (pp. 189-326). Springer, New York, NY.\n",
    "3. Murphy, S. A. (2003). Optimal dynamic treatment regimes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(2), 331-355.\n",
    "4. Liang, S., Lu, W., & Song, R. (2018). Deep advantage learning for optimal dynamic treatment regime. Statistical theory and related fields, 2(1), 80-88.\n",
    "5. Shi, C., Fan, A., Song, R., & Lu, W. (2018). High-dimensional A-learning for optimal dynamic treatment regimes. Annals of statistics, 46(3), 925."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63727cbe",
   "metadata": {},
   "source": [
    "## A.1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\frac{\\partial C_{j}(X_{i};\\psi_{j})}{\\partial \\psi_{j}}\\{\\mathbb{I}\\{A_{i}=j\\}-\\omega(X_{i},j;\\hat{\\gamma})\\}\\times \\Big\\{Y_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(X_{i;\\psi_{j'}})-Q(X_{i},0;\\phi)\\Big\\}=0\\\\\n",
    "&\\sum_{i=1}^n \\frac{\\partial Q(X_{i},0;\\phi)}{\\partial \\phi}\\Big\\{Y_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(X_{i};\\psi_{j'}) Q(X_{i},0;\\phi)\\Big\\}=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}