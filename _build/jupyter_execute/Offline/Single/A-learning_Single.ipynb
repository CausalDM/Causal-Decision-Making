{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7d66570b",
   "metadata": {},
   "source": [
    "# A-Learning (Single Stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "78444eb7",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] 系统找不到指定的文件。: '../CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20916\\2982377520.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'..'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'../CausalDM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] 系统找不到指定的文件。: '../CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c537c3",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "A-Learning, also known as Advantage Learning, is one of the main approaches to learning the optimal regime and works similarly to Q-learning. However, while Q-learning requires positing regression models to fit the expected outcome, A-learning models the contrasts between treatments and control, which can directly inform the optimal decision. A detailed comparison between Q-learning and A-learning can be found in [1]. While [1] mainly focus on the case with binary treatment options, a complete review of A-learning with multiple treatment options can be found in [2]. Here, following the algorithm in [1], we consider contrast-based A-learning. However, there is an alternative regret-based A-learning introduced in [3]. Some recent extensions to conventional A-learning, such as deep A-learning [4] and high-dimensional A-Learning [5], will be added soon.\n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., 0,1,2,3,4, where 0 stands for the control group by convention), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n",
    "\n",
    "contrast-based A-learning, as the name suggested, aims to learn and estimate the constrast function, $C_{tj}(h_{t})$. Here, $h_{t}=\\{X_{1i},A_{1i},\\cdots,X_{ti}\\})$ includes all the information observed till step t. Furthermore, we also need to posit a model for the conditional expected outcome for the control option (treatment $0$), $Q_t(h_t,0)$, and the propensity function $\\omega(h_{t},a_{t})$. Detailed definitions are provided in the following. Suppose there are $m_t$ number of options, and the action space $\\mathcal{A}_t=\\{0,1,\\dots,m_t-1\\}$ for each step t. With decision point $t$, we define thoes key functions as follows:\n",
    "*   Q-function:\n",
    "    For the final step $T$, \n",
    "    \\begin{align}\n",
    "    Q_T(h_T,a_{T})=E[Y|H_{T}=h_{T}, A_{T}=a_{T}],\n",
    "    \\end{align}\n",
    "    \n",
    "    If there is a multi-stage case with total step $T>1$, for the step $t=1,\\cdots,T-1$,\n",
    "    \\begin{align}\n",
    "    Q_t(h_t,a_{t})=E[V_{t+1}|H_{t}=h_{t}, A_{t}=a_{t}],\n",
    "    \\end{align}\n",
    "    where \n",
    "    \\begin{align}\n",
    "    V_{t}(h_{t}) = \\max_{j\\in\\mathcal{A}_t}Q_{t}(h_t,j)\n",
    "    \\end{align}\n",
    "    Alternatively, with the contrast function defined in the follwing,\n",
    "    \\begin{align}\n",
    "    Q_t(h_t,j) = Q_t(h_t,0) + C_{tj}(h_t),\\quad j=0,\\dots,m_k-1,\\quad t=1,\\dots,T.\n",
    "    \\end{align}\n",
    "*   Contrast functions (optimal blip to zero functions)\n",
    "    \\begin{align}\n",
    "    C_{tj}(h_t)=Q_t(h_t,j)-Q_t(h_t,0),\\quad j=0,\\dots,m_k-1,\\quad t=1,\\dots,T.\n",
    "    \\end{align}\n",
    "*   Propensity score\n",
    "    \\begin{align}\n",
    "    \\omega_{t}(h_t,a_t)=P(A_t=a_t|H_t=h_t)\n",
    "    \\end{align}\n",
    "*   Optimal regime\n",
    "    \\begin{align}\n",
    "    d_t^{opt}(h_t)=\\arg\\max_{j\\in\\mathcal{A}_t}C_{tj}(h_t)\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "In the following, we would start from a simple case having only one decision point and then introduce the multistage case with multiple decision points. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a55270",
   "metadata": {},
   "source": [
    "## 1. Single Decision Point\n",
    "\n",
    "- **Basic Logic**: Positting models, $C_{j}(h,\\psi_{j})$,$Q(h,0,\\phi)$,and $\\omega(h,a,\\gamma)$, A-learning aims to estimate $\\psi_{j}$, $\\phi$, and $\\gamma$ by g-estimation. With the $\\hat{\\psi}_{j}$ in hand, the optimal decision is directly derived.\n",
    "\n",
    "- **Key Steps**:\n",
    "    1. Fitted a model $\\omega_{1}(h_1,a_1,\\gamma)$, which can be solved directly by existing approaches (i.e., logistic regression, .etc),\n",
    "    2. Substituting the $\\hat{\\gamma}$, we estimate the $\\hat{\\psi}_{j}$ and $\\gamma$ by solving the euqations in Appendix A.1 jointly.      \n",
    "    2. For each individual find the optimal action $a_{i}$ such that $a_{i} = \\arg\\max_{j\\in\\mathcal{A}}C_{j}(h,\\hat{\\psi_{j}})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a61a733c",
   "metadata": {},
   "source": [
    "### 1.1 Optimal Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "caf697f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import ALearning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklift.datasets import fetch_hillstrom"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab42f15",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f6ab9251",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous Y\n",
    "data, target, treatment = fetch_hillstrom(target_col='spend', return_X_y_t=True)\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "data = pd.concat([data,pd.get_dummies(data['zip_code'], prefix='zip_code')],axis=1)\n",
    "data = pd.concat([data,pd.get_dummies(data['channel'], prefix='channel')],axis=1)\n",
    "# now drop the original 'country' column (you don't need it anymore)\n",
    "data.drop(['zip_code'],axis=1, inplace=True)\n",
    "data.drop(['channel'],axis=1, inplace=True)\n",
    "data.drop(['history_segment'],axis=1, inplace=True)\n",
    "data.drop(['zip_code_Rural'],axis=1, inplace=True) # Rural as the reference group\n",
    "data.drop(['channel_Multichannel'],axis=1, inplace=True) # Multichannel as the reference group \n",
    "\n",
    "Y = np.array(target)\n",
    "X = np.hstack([np.ones((len(data),1)),np.array(data)])# add an intercept column\n",
    "# convert the categorical variable into integers with treatment 0 = No emails\n",
    "treatment.replace(['Womens E-Mail', 'No E-Mail', 'Mens E-Mail'],[1, 0, 2], inplace=True) \n",
    "treatment = np.array(treatment)\n",
    "#get the subset which has Y>0 == n=578\n",
    "X = X[Y>0]\n",
    "A = {}\n",
    "A[0] = treatment[Y>0]\n",
    "Y = Y[Y>0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3742d623",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1.  ,  10.  ,  88.37, ...,   1.  ,   1.  ,   0.  ],\n",
       "       [  1.  ,   4.  , 297.8 , ...,   1.  ,   1.  ,   0.  ],\n",
       "       [  1.  ,  10.  ,  29.99, ...,   0.  ,   0.  ,   1.  ],\n",
       "       ...,\n",
       "       [  1.  ,   5.  , 210.12, ...,   0.  ,   1.  ,   0.  ],\n",
       "       [  1.  ,   2.  , 215.61, ...,   0.  ,   0.  ,   1.  ],\n",
       "       [  1.  ,   1.  , 239.7 , ...,   1.  ,   0.  ,   1.  ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ea9755",
   "metadata": {},
   "source": [
    "#### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72c2c21e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prop': {0: <statsmodels.discrete.discrete_model.MultinomialResultsWrapper at 0x7f4426ce92d0>},\n",
       " 'Q0': {0: array([ 7.0107e+00,  3.6316e+00, -1.0577e-02,  5.8950e+01,  2.9656e+01,\n",
       "         -2.5558e+01,  5.5264e+01,  2.5197e+01,  2.0701e+01,  2.4073e+01])},\n",
       " 'contrast': {0: {1: array([ 1.6399e+02, -3.6746e+00,  3.2799e-02, -9.1699e+01, -1.0692e+02,\n",
       "           2.6025e+01, -4.4841e+01,  1.0508e+01, -2.6366e+01, -3.3564e+01]),\n",
       "   2: array([ 7.8924e+01, -4.0368e+00,  6.0152e-03, -3.3001e+01, -2.3821e+01,\n",
       "           3.3728e+01, -5.1146e+01, -3.3706e+01, -1.2922e+01, -9.3451e+00])}}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the learner\n",
    "ALearn = ALearning.ALearning()\n",
    "p = X.shape[1]\n",
    "model_info = [{'X_prop': list(range(p)),\n",
    "              'X_q0': list(range(p)),\n",
    "               'X_C':{1:list(range(p)),2:list(range(p))},\n",
    "              'action_space': [0,1,2]}] #A in [0,1,2]\n",
    "# train the policy\n",
    "ALearn.train(X, A, Y, model_info, T=1)\n",
    "# Fitted Model\n",
    "ALearn.fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3216698b",
   "metadata": {},
   "source": [
    "#### Recommend Optimal Decisions and Get the Estimated Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ad04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt regime:    A0\n",
      "0   1\n",
      "1   0\n",
      "2   0\n",
      "3   1\n",
      "4   0\n",
      "opt value: 140.72832928963362\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = ALearn.recommend().head()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = ALearn.estimate_value()\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5c1b8803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 149.05980769890385 Value_std: 8.118207745378117\n",
      "estimated_contrast: {1:          Mean        std\n",
      "0  169.894627  62.026384\n",
      "1   -3.836917   4.239486\n",
      "2    0.025260   0.058109\n",
      "3  -91.730308  48.293958\n",
      "4 -105.905625  51.008528\n",
      "5   25.534935  24.834953\n",
      "6  -51.969912  27.520615\n",
      "7    5.588929  25.360780\n",
      "8  -23.852997  37.373339\n",
      "9  -33.165417  37.473663, 2:         Mean        std\n",
      "0  83.157094  61.783377\n",
      "1  -4.611978   4.247750\n",
      "2   0.005548   0.047884\n",
      "3 -31.733156  45.761740\n",
      "4 -21.923397  47.521361\n",
      "5  33.663992  26.512190\n",
      "6 -56.240389  32.394128\n",
      "7 -39.440176  30.410919\n",
      "8 -10.603243  37.554011\n",
      "9  -7.366659  31.932494}\n"
     ]
    }
   ],
   "source": [
    "# Optional: we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "ALearn = ALearning.ALearning()\n",
    "p = X.shape[1]\n",
    "model_info = [{'X_prop': list(range(p)),\n",
    "              'X_q0': list(range(p)),\n",
    "               'X_C':{1:list(range(p)),2:list(range(p))},\n",
    "              'action_space': [0,1,2]}] #A in [0,1,2]\n",
    "ALearn.train(X, A, Y, model_info, T=1, bootstrap = True, n_bs = 100)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.estimate_value_boots()\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)\n",
    "##estimated contrast model\n",
    "print('estimated_contrast:',params[0]['contrast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4763330c",
   "metadata": {},
   "source": [
    "### 1.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "727a1d32",
   "metadata": {},
   "source": [
    "For a given policy, we utilze the boostrap resampling to get the estimated value of the regime and the corresponding estimated standard error. Basically, for each round of bootstrap, we resample a dataset of the same size as the original dataset with replacement, fitted the Q0 function and contrast functions based on the sampled dataset, and estimated the value of a given regime using the estimated Q0 function and contrast functions function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6add96a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the fixed regime to be tested\n",
    "# For example, regime d = 0 for all subjects\n",
    "N, p = X.shape\n",
    "ALearn = ALearning.ALearning()\n",
    "# regime should be in the same format as A, which is a dict\n",
    "regime = {}\n",
    "regime[0] = np.array([0]*N)\n",
    "model_info = [{'X_prop': list(range(p)),\n",
    "              'X_q0': list(range(p)),\n",
    "               'X_C':{1:list(range(p)),2:list(range(p))},\n",
    "              'action_space': [0,1,2]}] #A in [0,1,2]\n",
    "ALearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.estimate_value_boots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "914be48f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 113.8592661549983 Value_std: 10.670525568174071\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e7a48b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.1744826376249"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otional: just estimate the value\n",
    "ALearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True)\n",
    "ALearn.estimate_value()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72993cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_test_case(setup, N, seed = 0):\n",
    "    if setup == 'random_binary':\n",
    "        np.random.seed(seed)\n",
    "        s1 = np.random.normal(0,1, N)\n",
    "        A1 = np.random.binomial(1,np.exp(-2*s1)/(1+np.exp(-2*s1)),size = N)\n",
    "        Y = np.random.normal(1+s1+A1*(1+.5*s1),3)\n",
    "        opt_true = (1+.5*s1 >0).astype(int)\n",
    "        X = np.hstack([np.ones(N)[:, np.newaxis], s1[:, np.newaxis]])\n",
    "        instance = {\n",
    "            'X' : X, \n",
    "            'A' : A1, \n",
    "            'Y' : Y, \n",
    "            'optimal_A' : opt_true, \n",
    "            'XAY' : [X, A1, Y]\n",
    "        }\n",
    "        return instance\n",
    "    \n",
    "instance = generate_test_case('random_binary', 10000, seed = 0)\n",
    "X,A1,Y = instance['XAY']\n",
    "A = {}\n",
    "A[0] = A1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8de7582",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.016 ],\n",
       "       [-1.9631]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the learner\n",
    "ALearn = ALearning.ALearning()\n",
    "p = X.shape[1]\n",
    "model_info = [{'X_prop': list(range(p)),\n",
    "              'X_q0': list(range(p)),\n",
    "               'X_C':{1:list(range(p))},\n",
    "              'action_space': [0,1]}] #A in [0,1,2]\n",
    "# train the policy\n",
    "ALearn.train(X, A, Y, model_info, T=1)\n",
    "# Fitted Model\n",
    "ALearn.fitted_model['prop'][0].params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c388f7",
   "metadata": {},
   "source": [
    "💥 Placeholder for C.I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efbb6e8e",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Schulte, P. J., Tsiatis, A. A., Laber, E. B., & Davidian, M. (2014). Q-and A-learning methods for estimating optimal dynamic treatment regimes. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4), 640.\n",
    "2. Robins, J. M. (2004). Optimal structural nested models for optimal sequential decisions. In Proceedings of the second seattle Symposium in Biostatistics (pp. 189-326). Springer, New York, NY.\n",
    "3. Murphy, S. A. (2003). Optimal dynamic treatment regimes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(2), 331-355.\n",
    "4. Liang, S., Lu, W., & Song, R. (2018). Deep advantage learning for optimal dynamic treatment regime. Statistical theory and related fields, 2(1), 80-88.\n",
    "5. Shi, C., Fan, A., Song, R., & Lu, W. (2018). High-dimensional A-learning for optimal dynamic treatment regimes. Annals of statistics, 46(3), 925."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fa3ad46",
   "metadata": {},
   "source": [
    "## A.1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\frac{\\partial C_{j}(H_{i};\\psi_{j})}{\\partial \\psi_{j}}\\{\\mathbb{I}\\{A_{i}=j\\}-\\omega(H_{i},j;\\hat{\\gamma})\\}\\times \\Big\\{Y_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(H_{i;\\psi_{j'}})-Q(H_{i},0;\\phi)\\Big\\}=0\\\\\n",
    "&\\sum_{i=1}^n \\frac{\\partial Q(H_{i},0;\\phi)}{\\partial \\phi}\\Big\\{Y_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(H_{i};\\psi_{j'}) Q(H_{i}0;\\phi)\\Big\\}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## A.2\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial C_{Tj}(H_{Ti};\\psi_{Tj})}{\\partial \\psi_{Tj}}\\{\\mathbb{I}\\{A_{Ti}=j\\}-\\omega_T(H_{Ti},j;\\gamma_T)\\}\\times \\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_{T})\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial Q_T(H_{Ti},0;\\phi_T)}{\\partial \\phi_T}\\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_T)\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial \\omega_T(H_{Ti},j;\\gamma_T)}{\\partial \\gamma_T}\\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_T)\\Big\\}\\right]=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## A.3\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial C_{tj}(H_{ti};\\psi_{tj})}{\\partial \\psi_{tj}}\\{\\mathbb{I}\\{A_{ti}=j\\}-\\omega_T(H_{ti},j;\\gamma_t)\\}\\times \\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_{t})\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial Q_t(H_{ti},0;\\phi_t)}{\\partial \\phi_t}\\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_t)\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial \\omega_t(H_{ti},j;\\gamma_t)}{\\partial \\gamma_t}\\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_t)\\Big\\}\\right]=0\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd8408e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}