{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dea3543a",
   "metadata": {},
   "source": [
    "# Q-Learning (Single Stage)\n",
    "\n",
    "## Main Idea\n",
    "Early in 2000, as a classic method of Reinforcement Learning, Q-learning was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. Q-learning with finite decision points is mainly a regression modeling problem based on positing regression models for outcome at each decision point. The target of Q-learning is to find an optimal policy $\\pi$ that can maximize the expected reward received. In other words, by training a model with the observed data, we hope to find an optimal policy to predict the optimal action for each individual to maximize rewards. For example, considering the motivating example **Personalized Incentives**, Q-learning aims to find the best policy to assign different incentives ($A$) to different users to optimize the return-on-investment ($Y$). Overall, Q-learning is practical and easy to understand, as it allows straightforward implementation of diverse established regression methods. \n",
    "\n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better.\n",
    "\n",
    "\n",
    "\n",
    "## Algorithm Details\n",
    "Q-learning with a single decision point is mainly a regression modeling problem, as the major component is to find the relationship between $Y$ and $\\{X,A\\}$. Let's first define a Q-function, such that\n",
    "\\begin{align}\n",
    "    Q(x,a) = E(Y|X=x, A=a).\n",
    "\\end{align} Then, to find the optimal policy is equivalent to solve\n",
    "\\begin{align}\n",
    "    \\text{arg max}_{\\pi}Q(x_{i},\\pi(x_{i})).\n",
    "\\end{align} \n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. Fitted a model $\\hat{Q}(x,a,\\hat{\\beta})$, which can be solved directly by existing approaches (i.e., OLS, .etc),\n",
    "2. For each individual find the optimal action $d^{opt}(x_{i})$ such that $d^{opt}(x_{i}) = \\text{arg max}_{a}\\hat{Q}(x_{i},a,\\hat{\\beta})$.\n",
    "\n",
    "**Policy Evaluation:**    \n",
    "1. Fitted the Q function $\\hat{Q}(x,a,\\hat{\\beta})$, based on the sampled dataset\n",
    "2. Estimated the value of a given regime $d$ using the estimated Q function, such that, $\\hat{Y}_{i} = \\hat{Q}(x_{i},d(x_{i}),\\hat{\\beta})$\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilize bootstrap resampling to get the estimated value of the regime and the corresponding estimated standard error. For each round of bootstrapping, we first resample a dataset of the same size as the original dataset, then fit the Q function based on the sampled dataset, and finally estimate the value of a given regime based on the estimated Q function. \n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5df34ad6",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d335ed3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'causaldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_175148\\3479176628.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# import learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_util_causaldm\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearners\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mQLearning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'causaldm'"
     ]
    }
   ],
   "source": [
    "# import learner\n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners import QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7366b702",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "X,A,Y = get_data(target_col = 'spend', binary_trt = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b54e346c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. specify the model you would like to use\n",
    "# If want to include all the variable in X and A with no specific model structure, then use \"Y~.\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbbea72",
   "metadata": {},
   "source": [
    "By specifing the model_info, we assume a regression model that:\n",
    "\\begin{align}\n",
    "Q(x,a,\\beta) &= \\beta_{00}+\\beta_{01}*recency+\\beta_{02}*history\\\\\n",
    "&+I(a=1)*\\{\\beta_{10}+\\beta_{11}*recency+\\beta_{12}*history\\} \\\\\n",
    "&+I(a=2)*\\{\\beta_{20}+\\beta_{21}*recency+\\beta_{22}*history\\} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3a4e1df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "#3. train the policy\n",
    "QLearn.train(X, A, Y, model_info, T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9af876a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted model: Intercept            94.202956\n",
      "C(A)[T.1]            23.239801\n",
      "C(A)[T.2]            20.611375\n",
      "recency               4.526133\n",
      "C(A)[T.1]:recency    -4.152892\n",
      "C(A)[T.2]:recency    -4.843148\n",
      "history               0.000549\n",
      "C(A)[T.1]:history     0.007584\n",
      "C(A)[T.2]:history     0.000416\n",
      "dtype: float64\n",
      "opt regime: A\n",
      "1    371\n",
      "0    207\n",
      "dtype: int64\n",
      "opt value: 126.48792828230047\n"
     ]
    }
   ],
   "source": [
    "#4. recommend action\n",
    "opt_d = QLearn.recommend_action(X).value_counts()\n",
    "#5. get the estimated value of the optimal regime\n",
    "V_hat = QLearn.predict_value(X)\n",
    "print(\"fitted model:\",QLearn.fitted_model[0].params)\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21cb8ac3",
   "metadata": {},
   "source": [
    "**Interpretation:** the fitted model is \n",
    "\\begin{align}\n",
    "Q(x,a,\\beta) &= 94.20+4.53*recency+0.0005*history\\\\\n",
    "&+I(a=1)*\\{23.24-4.15*recency+0.0076*history\\} \\\\\n",
    "&+I(a=2)*\\{20.61-4.84*recency+0.0004history\\}. \n",
    "\\end{align}\n",
    "Therefore, the estimated optimal regime is:\n",
    "1. We would recommend $A=0$ (No E-mail) if $23.24-4.15*recency+0.0076*history<0$ and $20.61-4.84*recency+0.0004history<0$\n",
    "2. Else, we would recommend $A=1$ (Womens E-mail) if $23.24-4.15*recency+0.0076*history>20.61-4.84*recency+0.0004history$\n",
    "3. Else, we would recommend $A=2$ (Mens E-Mail).\n",
    "\n",
    "The estimated value for the estimated optimal regime is 126.49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49de5af4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 132.31352423343282 Value_std: 7.37323834017361\n"
     ]
    }
   ],
   "source": [
    "# Optional: \n",
    "#we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]\n",
    "QLearn.train(X, A, Y, model_info, T=1, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(X)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a14654d",
   "metadata": {},
   "source": [
    "**Interpretation:** Based on the boostrap with 200 replicates, the estimated optimal value is 132.31 with a standard error of 7.37."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f94f1233",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "80be46ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.40675465960642"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. specify the fixed regime to be tested (For example, regime d = 'No E-Mail' for all subjects)\n",
    "# !! IMPORTANTï¼š index shold be the same as that of the X\n",
    "N=len(X)\n",
    "regime = pd.DataFrame({'A':[0]*N}).set_index(X.index)\n",
    "#2. evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]\n",
    "QLearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True)\n",
    "QLearn.predict_value(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3939b7e2",
   "metadata": {},
   "source": [
    "**Interpretation:** the estimated value of the regime that always sends no emails ($A=0$) is 116.41, under the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "49b914e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 115.95548975939548 Value_std: 10.502988081748748\n"
     ]
    }
   ],
   "source": [
    "# Optional: Boostrap\n",
    "QLearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(X)\n",
    "# bootstrap average and the std of estimate value\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "819613e2",
   "metadata": {},
   "source": [
    "**Interpretation:** the bootstrapped estimated value of the regime that always sends no emails is 115.96 with a bootstrapped standard error 10.50, under the specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b504b84",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8230d1df",
   "metadata": {},
   "source": [
    "!!Functions are already tested with the data and results provided in the DTR book\n",
    "\n",
    "TODO: \n",
    "    1. estimate the standard error for the binary case with sandwich formula;\n",
    "    2. inference for the estimated optimal regime: projected confidence interval? m-out-of-n CI?...."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}