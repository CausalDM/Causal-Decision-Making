{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e8882b9",
   "metadata": {},
   "source": [
    "# Fitted-Q Iteration\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a6e4a9",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "\n",
    "**Q-function.**\n",
    "The Q-function-based approach aims to direct learn the state-action value function (referred to as the Q-function) \n",
    "\\begin{eqnarray}\n",
    "Q^\\pi(a,s)&= \\mathbb{E}^{\\pi} (\\sum_{t=0}^{+\\infty} \\gamma^t R_{t}|A_{0}=a,S_{0}=s)   \n",
    "\\end{eqnarray}\n",
    "of either the policy $\\pi$ that we aim to evaluate or the optimal policy $\\pi = \\pi^*$. \n",
    "\n",
    "**Bellman optimality equations.**\n",
    "The Q-learning-type policy learning is commonly based on the Bellman optimality equation, which characterizes the optimal policy $\\pi^*$ and is commonly used in policy optimization. \n",
    "Specifically, $Q^*$ is the unique solution of \n",
    "\\begin{equation}\n",
    "    Q(a, s) = \\mathbb{E} \\Big(R_t + \\gamma \\arg \\max_{a'} Q(a, S_{t+1})  | A_t = a, S_t = s \\Big).  \\;\\;\\;\\;\\; \\text{(2)} \n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "532367c0",
   "metadata": {},
   "source": [
    "**FQI.**\n",
    "Similar to FQE **[CROSS REFERENCE]**, the fitted-Q iteration (FQI) [1] algorithm is also popular due to its simple form and good numerical performance. \n",
    "It is mainly motivated by the fact that, the optimal value function $Q^*$ is the unique solution to the Bellman optimality equation (2). \n",
    "Besides, the right-hand side of (2) is a contraction mapping. \n",
    "Therefore, we can consider a fixed-point method: \n",
    "with an initial estimate $\\widehat{Q}^{0}$, \n",
    "FQI iteratively solves the following optimization problem, \n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\\widehat{Q}^{{\\ell}}=\\arg \\min_{Q} \n",
    "\t\\sum_{\\substack{i \\le n}}\\sum_{t<T}\n",
    "\t\\Big\\{\n",
    "\t\\gamma \\max_{a'} \\widehat{Q}^{\\ell-1}(a',S_{i, t+1}) \n",
    "\t+R_{i,t}- Q(A_{i, t}, S_{i, t})  \n",
    "\\Big\\}^2,\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "for $\\ell=1,2,\\cdots$, until convergence. \n",
    "The final estimate is denoted as $\\widehat{Q}_{FQI}$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fc51f4c",
   "metadata": {},
   "source": [
    "## Demo [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6cf78f0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/c5dcq1_d1gn8w9yg5728l9dm0000gn/T/ipykernel_16321/2982377520.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../CausalDM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4959c6d",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Ernst, D., Geurts, P., and Wehenkel, L. Tree-based batch mode reinforcement learning. Journal of Machine Learn- ing Research, 6(Apr):503â€“556, 2005."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}