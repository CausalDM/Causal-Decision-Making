{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9bbf0c3",
   "metadata": {},
   "source": [
    "# UCB\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: There is no need to specify any hyper-parameters. It carefully quantifies the uncertainty of the estimation to better combine exploration and exploitation. \n",
    "- **Disadvantage**: Inefficient if there is a large number of action items. \n",
    "- **Application Situation**: discrete action space, binary/Gaussian reward space\n",
    "\n",
    "## Main Idea\n",
    "As the name suggested, the UCB algorithm estimates the upper confidence bound $U_{a}^{t}$ of the mean of the potential reward of arm $a$, $R_t(a)$, based on the observations and then choose the action has the highest estimates. The class of UCB-based algorithms is firstly introduced by Auer et al. [1]. Generally, at each round $t$, $U_{a}^{t}$ is calculated as the sum of the estimated reward (exploitation) and the estimated confidence radius (exploration) of item $i$ based on previous observations. Then, $A_{t}$ is selected as \n",
    "$$\n",
    "\\begin{equation}\n",
    "    A_t = \\arg \\max_{a \\in \\mathcal{A}} U_a^t.\n",
    "\\end{equation} \n",
    "$$\n",
    "As an example, **UCB** [1] estimates the confidence radius as $\\sqrt{\\frac{2log(t)}{\\text{\\# item $i$ played so far}}}$. Doing so, either the item with a large average reward or the item with limited exploration will be selected. Note that this algorithm support cases with either binary reward or continuous reward.\n",
    "\n",
    "## Algorithms Details\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. The UCB1 algorithm start with initializing the estimated upper confidence bound $U_a^{0}$ and the count of being pulled $C_a^{0}$ for each action $a$ as 0. At each round $t$, we greedily select an action $A_t$ as \n",
    "\\begin{align}\n",
    "A_t = \\arg \\max_{a\\in \\mathcal{A}} U_{a}^{t}.\n",
    "\\end{align}\n",
    "\n",
    "After observing the rewards corresponding to the selected action $A_t$, we first update the total number of being pulled for $A_t$ accordingly. Then, we estimate the upper confidence bound for each action $a$ as\n",
    "\\begin{align}\n",
    "U_{a}^{t+1} = \\frac{1}{C_a^{t+1}}\\sum_{t'=0}^{t}R_{t'}I(A_{t'}=a) + \\sqrt{\\frac{2*log(t+1)}{C_a^{t+1}}} ,\n",
    "\\end{align}where $R_{t'}$ is the reward received at round $t'$. Intuitively, $U_{a}^{t}$ is the sum of the sample average reward of action $a$ for expolitation and a confidence radius for exploration.\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Initializing the $\\boldsymbol{U}^0$ and $\\boldsymbol{C}^0$ for $K$ items as 0\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "    - select action $A_t$ as the arm with the maximum $U_a^t$\n",
    "    - Received the reward R, and update $C$ and $U$ with\n",
    "        \\begin{align}\n",
    "        C_{A_{t}}^{t+1} &= C_{A_{t}}^{t} + 1 \\\\\n",
    "        U_{A_{t}}^{t+1} &= \\frac{1}{C_a^{t+1}}\\sum_{t'=0}^{t}R_{t'}I(A_{t'}=a) + \\sqrt{\\frac{2*log(t+1)}{C_a^{t+1}}} \n",
    "        \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9f0330f",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "497c1949",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\GitHub\\CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c1ca0dc-a12f-487c-863c-691ebf2af9fd",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "94066040",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mOnline\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMAB\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UCB\n",
      "File \u001b[1;32mD:\\GitHub\\CausalDM\\causaldm\\learners\\Online\\MAB\\UCB.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_base_online_learner\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BaseOnlineLearner\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util_online\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcopy\u001b[39;00m\n",
      "File \u001b[1;32mD:\\GitHub\\CausalDM\\causaldm\\learners\\Online\\_base_online_learner.py:1\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util_online\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mBaseOnlineLearner\u001b[39;00m():\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;129m@autoargs\u001b[39m()\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n",
      "File \u001b[1;32mD:\\GitHub\\CausalDM\\causaldm\\learners\\Online\\_util_online.py:55\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcollections\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Counter\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mensemble\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m GradientBoostingRegressor \u001b[38;5;28;01mas\u001b[39;00m GBT\n\u001b[0;32m     56\u001b[0m \u001b[38;5;66;03m####################################\u001b[39;00m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\__init__.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stacking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingClassifier\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stacking\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m StackingRegressor\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_hist_gradient_boosting\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgradient_boosting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     23\u001b[0m     HistGradientBoostingRegressor,\n\u001b[0;32m     24\u001b[0m     HistGradientBoostingClassifier,\n\u001b[0;32m     25\u001b[0m )\n\u001b[0;32m     27\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBaseEnsemble\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRandomForestClassifier\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistGradientBoostingRegressor\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     47\u001b[0m ]\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\sklearn\\ensemble\\_hist_gradient_boosting\\gradient_boosting.py:22\u001b[0m, in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[1;32m---> 22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_gradient_boosting\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _update_raw_predictions\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Y_DTYPE, X_DTYPE, X_BINNED_DTYPE\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbinning\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _BinMapper\n",
      "File \u001b[1;32msklearn\\ensemble\\_hist_gradient_boosting\\_gradient_boosting.pyx:8\u001b[0m, in \u001b[0;36minit sklearn.ensemble._hist_gradient_boosting._gradient_boosting\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:398\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.Online.MAB import UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3eacd3-3684-4a74-920f-902451993f34",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the MovieLens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22b6b447-ace9-4e13-baa5-c68388f01896",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.MAB import _env_realMAB as _env\n",
    "env = _env.Single_Gaussian_Env(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab64a1c2-894f-4cda-bc82-ddf25136834e",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "\n",
    "- K: # of arms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3cb9bd5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "UCB_agent = UCB.UCB1(env.K)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54877ee5-aace-4ca2-8aee-e6bd031a28ed",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "\n",
    "Starting from t = 0, for each step t, there are three steps:\n",
    "1. Recommend an action \n",
    "<code> A = UCB_agent.take_action() </code>\n",
    "2. Get the reward from the environment \n",
    "<code> R = env.get_reward(t,A) </code>\n",
    "3. Update the posterior distribution\n",
    "<code> UCB_agent.receive_reward(t,A,R) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ba8b09ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 2)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "A = UCB_agent.take_action()\n",
    "R = env.get_reward(A)\n",
    "UCB_agent.receive_reward(t,A,R)\n",
    "t, A, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea24204c",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the UCB agent recommend a Comedy (arm 0), and received a rate of 2 from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b90c0eb-b1dd-4d37-a324-5b18de24fe38",
   "metadata": {},
   "source": [
    "### Demo Code for Bernoulli Bandit\n",
    "The steps are similar to those previously performed with a Gaussian Bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5ba2f16f-473d-484d-9776-1b8f0098185a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = _env.Single_Bernoulli_Env(seed=42)\n",
    "\n",
    "UCB_agent = UCB.UCB1(env.K)\n",
    "\n",
    "t = 0\n",
    "A = UCB_agent.take_action()\n",
    "R = env.get_reward(A)\n",
    "UCB_agent.receive_reward(t,A,R)\n",
    "t, A, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07dc1f28-f953-4e75-ab19-13da18fca5a6",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the UCB agent recommend a Comedy (arm 0), and received a reward of 0 from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beeaf399",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235–256."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}