{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "14761259",
   "metadata": {},
   "source": [
    "# TS\n",
    "\n",
    "## Main Idea\n",
    "\n",
    "\n",
    "## Algorithms Details\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. The TS algorithm starts with specifying a prior distribution of the reward, based on the domian knowledge. At each round $t$, the agent will samples a vector of $\\theta^{t}$ from the posterior distribution of the rewards. The action $a$ with the greatest $\\theta_{a}^{t}$ is then selected. Finally, the posterior distribution would be updated after receiving the feedback at the end of each round. Note that the posterior updating step differs for different pairs of prior distribution of the mean reward and reward distribution. Here, we consider two classical examples of the TS algorithm, including Gaussian reward with Gaussian prior and Bernoulli with Breward with Beta prior. The posterior updating is straightforward for both cases, since the nice conjugate property. In both cases, the variance of reward is assumed to be known, and need to be specified manually. Note that code can be easily modified to different specifications of the prior/reward distribution.\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Specifying a prior distirbution of $\\theta$, and the variance of the reward distribution.\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "    - sample a $\\tilde{\\theta}^{t}$ from the posterior distribution of $\\theta$ or prior distribution if in round $0$\n",
    "    - select action $A_t$ which has the greatest $\\tilde{\\theta}_{a}$, i.e. $A_t = argmax_{a \\in \\mathcal{A}} \\tilde{\\theta}_{a}^{t}$\n",
    "    - receive the rewad $R$, and update the posterior distirbution accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4be3cc",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4cb448b5",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_6704\\3636065689.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/nas/longleaf/home/lge/CausalDM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# code used to import the learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e94e9140",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Single import TS\n",
    "from causaldm.learners.Online.Single import Env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a2cda4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 5\n",
    "with_intercept = True\n",
    "p=3\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_theta = sigma_gamma = np.identity(p)\n",
    "mu_theta = np.zeros(p)\n",
    "seed = 0\n",
    "sigma = 1\n",
    "\n",
    "env = Env.Single_Gaussian_Env(T, K, p, sigma\n",
    "                         , mu_theta, Sigma_theta\n",
    "                        , seed = 42, with_intercept = True\n",
    "                         , X_mu = X_mu, X_Sigma = X_sigma)\n",
    "TS_Gaussian_agent = TS.TS(Reward_Type = \"Gaussian\", sigma = sigma, u_prior_mean = np.ones(K), u_prior_cov = np.identity(K), prior_phi_beta = None)\n",
    "A = TS_Gaussian_agent.take_action()\n",
    "t = 0\n",
    "R = env.get_reward(t,A)\n",
    "TS_Gaussian_agent.receive_reward(t,A,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "defb6220",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4375, 0.    , 0.    , 0.    , 0.    ])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TS_Gaussian_agent.posterior_u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d92149dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 5\n",
    "\n",
    "phi_beta = 1/4\n",
    "with_intercept = True\n",
    "p=3\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_theta = sigma_gamma = np.identity(p)\n",
    "mu_theta = np.zeros(p)\n",
    "seed = 0\n",
    "\n",
    "env = Env.Single_Bernoulli_Env(T, K, p, phi_beta\n",
    "                         , mu_theta, Sigma_theta\n",
    "                        , seed = 42, with_intercept = True\n",
    "                         , X_mu = X_mu, X_Sigma = X_sigma)\n",
    "TS_Bernoulli_agent = TS.TS(Reward_Type = \"Bernoulli\", sigma = 1, u_prior_mean = .5*np.ones(K), u_prior_cov = None, prior_phi_beta = phi_beta)\n",
    "A = TS_Bernoulli_agent.take_action()\n",
    "t = 0\n",
    "R = env.get_reward(t,A)\n",
    "TS_Bernoulli_agent.receive_reward(t,A,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f7fd99df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 3.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TS_Bernoulli_agent.posterior_alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "48115e9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 2., 2., 2., 2.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TS_Bernoulli_agent.posterior_beta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22511d65",
   "metadata": {},
   "source": [
    "**Interpretation:** A sentence to include the analysis result: the estimated optimal regime is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77a985dd",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018). A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1), 1-96."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a81ef9ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}