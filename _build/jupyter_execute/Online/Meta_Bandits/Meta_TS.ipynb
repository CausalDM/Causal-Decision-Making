{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b115ac-4e61-4c5d-a5a8-a849fd9b46b6",
   "metadata": {},
   "source": [
    "# Meta Thompson Sampling\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: When task instances are sampled from the same unknown instance prior (i.e., the tasks are similar), it efficiently learns the prior distribution of the mean potential rewards to achieve a regret bound that is comparable to that of the TS algorithm with known priors. \n",
    "- **Disadvantage**: When there is a large number of different tasks, the algorithm is not scalable and inefficient.\n",
    "- **Application Situation**: Useful when there are multiple **similar** multi-armed bandit tasks, each with the same action space. The reward space can be either binary or continuous.\n",
    "\n",
    "## Main Idea\n",
    "The **Meta-TS**[1] assumes that the mean potential rewards, $\\mu_{j,a} = E(R_{j,t}(a))$, for each task $j$ are i.i.d sampled from some distribution parameterized by $\\boldsymbol{\\gamma}$. Specifically, it assumes that\n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(meta-Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Prior)} \\quad\n",
    "\\; \\quad\\quad\\quad\\quad   \\boldsymbol{\\mu}_j | \\boldsymbol{\\gamma} &&\\sim g(\\boldsymbol{\\mu}_j | \\boldsymbol{\\gamma})\\\\\n",
    "&\\text{(Reward)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&\\sim f(Y_{j,t}(a)|\\mu_{j,a}).\n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "To learn the prior distribution of $\\boldsymbol{\\mu}_{j}$, it introduces a meta-parameter $\\gamma$ with a meta-prior distribution $Q(\\gamma)$. The **Meta-TS** efficiently leverages the knowledge received from different tasks to learn the prior distribution and to guide the exploration of each task by maintaining the meta-posterior distribution of $\\gamma$. Theoretically, it is demonstrated to have a regret bound comparable to that of the Thompson sampling method with known prior distribution of $\\mu_{j,a}$. Both the \n",
    "\n",
    "Considering a Gaussian bandits, we assume that\n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(meta-Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Prior)} \\quad\n",
    "\\;  \\quad\\quad\\quad\\quad \\boldsymbol{\\mu}_j |\\boldsymbol{\\gamma} &&\\sim g(\\boldsymbol{\\mu}_j |\\boldsymbol{\\gamma})=\\boldsymbol{\\gamma}+ \\boldsymbol{\\delta}_{j}, \\\\\n",
    "&\\text{(Reward)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&= \\mu_{j,a} + \\epsilon_{j,t}, \n",
    "      \\end{alignedat}\n",
    "\\end{equation} where $\\boldsymbol{\\delta}_{j} \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$, and $\\epsilon_{j,t} \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\boldsymbol{0}, \\sigma^{2})$. The $\\boldsymbol{\\Sigma}$ and $\\sigma$ are both supposed to be known. A Gaussian meta-prior is employed by default with explicit forms of posterior distributions for simplicity. However, d ifferent meta-priors are welcome, with only minor modifications needed, such as using the **Pymc3** to accomplish posterior updating instead if there is no explicit form.\n",
    "\n",
    "Similarly, considering the Bernoulli bandits, we assume that \n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(meta-Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Prior)} \\quad\n",
    "\\;  \\quad\\quad\\quad\\quad \\boldsymbol{\\mu}_j |\\boldsymbol{\\gamma} &&\\sim Beta(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Reward)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&= Bernoulli(\\mu_{j,a}). \n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "While various meta-priors can be used, by default, we consider a finite space of $\\boldsymbol{\\gamma}$,\n",
    "$$\\mathcal{P} = \\{(\\alpha_{i,j})_{i=1}^{K}, (\\beta_{i,j})_{i=1}^{K}\\}_{j=1}^{L},$$ \n",
    "which contains **L** potential instance priors and assume a categorical distribution over the $\\mathcal{P}$ as the meta-prior. See [1] for more information about the corresponding meta-posterior updating.\n",
    "\n",
    "**Remark.** While the original system only supported a sequential schedule of interactions (i.e., a new task will not be interacted with until the preceding task is completed), we adjusted the algorithm to accommodate different recommending schedules.\n",
    "\n",
    "## Key Steps\n",
    "For $(j,t) = (1,1),(1,2),\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H})$ either by implementing **Pymc3** or by calculating the explicit form of the posterior distribution;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H})$;\n",
    "3. Update $P(\\boldsymbol{\\mu}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H})$ and sample $\\tilde{\\boldsymbol{\\mu}} \\sim P(\\boldsymbol{\\mu}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H})$;\n",
    "4. Take the action $A_{j,t}$ such that $A_{j,t} = argmax_{a \\in \\mathcal{A}} \\tilde\\mu_{j,a}$;\n",
    "6. Receive reward $R_{j,t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a00b1-fb6e-48e3-8833-016e73f4b844",
   "metadata": {},
   "source": [
    "## Demo Code\n",
    "\n",
    "TODO： Bernoulli: specify the prior space"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d2146-d82c-4c02-a2a5-a1c3ad605cad",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Kveton, B., Konobeev, M., Zaheer, M., Hsu, C. W., Mladenov, M., Boutilier, C., & Szepesvari, C. (2021, July). Meta-thompson sampling. In International Conference on Machine Learning (pp. 5884-5893). PMLR.\n",
    "\n",
    "[2] Basu, S., Kveton, B., Zaheer, M., & Szepesvári, C. (2021). No regrets for learning the prior in bandits. Advances in Neural Information Processing Systems, 34, 28029-28041.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}