#!/usr/bin/env python
# coding: utf-8

# # Slate Recommendation
# 
# This chapter focuses on slate recommendation problems, where the agent needs to recommend a slate of items at each round. Here, an item can be a product, a web page, a movie, etc., depending on the application. 
# 
# ## Problem Setting
# Let $T$ be the total number of rounds, and $N$ be the total number of items that are available to recommend. The agent would choose a slate of items at each round $t = 1, \dots, T$. In other words, an action $a$ is defined as a subset of items, and then the action space $\mathcal{A}$ consists of all subsets satisfied with some problem-specific constraints. Then we will receive the corresponding stochastic observations $\boldsymbol{Y}_t$ from the environment, which further determines the reward $R_t$. We assume that the observation $\boldsymbol{Y}_t$ is generated by a domain model $f$, and the reward $R_t$ is determined through a deterministic function $f_r$. Mathematically, we consider the following popular and general class of bandit problems[1]: 
# \begin{equation}
#     \begin{split}
#     \boldsymbol{Y}_t &\sim f(\boldsymbol{Y}_t |A_t, \boldsymbol{\theta}),\\
#     R_t &= f_r(\boldsymbol{Y}_t; \boldsymbol{\eta}).
#     \end{split}
#     \label{eqn:main_raw_model}
# \end{equation} Here, $f$ is parameterized by some **unknown** parameters $\boldsymbol{\theta}$, which can be factorized over these items as $\boldsymbol{\theta} = (\theta_1, \dots, \theta_N)^T$,  where $\theta_i$ is the parameter related to the $i$th item. $f_r$ is parameterized by some **known** parameters $\boldsymbol{\eta}$. Still, the objective is to find a bandit algorithm to maximize the cumulative Reward $\sum_{t=1}^{T}R_{t}$.
# 
# This problem setting subsumes many popular bandit problems, such as 
# - **Dynamic Assortment Optimization**: Multinomial Logit Bandits [2] aims to solve the most profitable subset of items to offer, especially when there exist substitution effects;
# - **Online Learning to Rank**: Cascading Bandits [3] is used to characterize how a user interacts with an ordered list of $K$ items;
# - **Online Combinatorial Optimization**: Combinatorial Semi-Bandits [4] which have numerous applications, including maximum weighted matching, ad allocation, and news page optimization;
# - ...... 
# 
# Note that, for any positive integer $N$, we denote the set $\{1, \dots, N\}$ by $[N]$. Further, we denote the cardinality of set $A$ as $|A|$. $\mathcal{H}_{t}$ denotes a sequence of observations containing all the tuples of action and corresponding feedback received in previous rounds, excluding round $t$. Specifically, $\mathcal{H}_{t}=\{(A_{i},\boldsymbol{Y}_{i},R_{i})\}_{i=1}^{t-1}$. If feature information is avialable, we denote $\boldsymbol{x}_i$ as a $d$-dimensional vector of item-specific features.
# 
# ## Real Data
# 
# **MovieLens**
# TODO: description of the dataset(how to cooked for three cases), and the code to get the data.
# 

# ## Reference
# [1] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. (2017). A tutorial on thompson sampling. arXiv preprint arXiv:1707.02038.
# 
# [2] Luce, R. D. (2012). Individual choice behavior: A theoretical analysis. Courier Corporation.
# 
# [3] Kveton, B., Szepesvari, C., Wen, Z., and Ashkan, A. (2015a). Cascading bandits: Learning to rank in the cascade model. In International Conference on Machine Learning, pages 767–776. PMLR.
# 
# [4] Chen, W., Wang, Y., and Yuan, Y. (2013). Combinatorial multi-armed bandit: General framework and applications. In International conference on machine learning, pages 151–159. PMLR.

# In[ ]:




