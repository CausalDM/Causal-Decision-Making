{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TS_Contextual_MNL\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is scalable when the features are used. It outperforms algorithms based on other frameworks, such as UCB, in practice.\n",
    "- **Disadvantage**: It is susceptible to model misspecification.\n",
    "- **Application Situation**: Useful when a list of items is presented, each with a matching price or income, and only one is chosen for each interaction. Binary responses from users include click/don't-click and buy/don't-buy.\n",
    "\n",
    "\n",
    "## Main Idea\n",
    "Feature-determined approaches have been developed recently to provide a more feasible approach for large-scale problems, by adapting either the UCB framwork or the TS framework. While all of them [1,2,3] are under the standard offering structure, here we modify the TS-type algorithm in [3] by adapting to the epoch-type offering framework and assuming a linear realtionship between the utility and the item features as \n",
    "\\begin{equation}\n",
    "\\theta_i = \\frac{logistic(\\boldsymbol{x}_{i,t}^T \\boldsymbol{\\gamma})+ 1}{2},\n",
    "\\end{equation} to tackle the challenge of a large item space. We named the proposed algorithm as **TS_Contextual_MNL**. At each decision round $t$, **TS_Contextual_MNL** samples $\\tilde{\\boldsymbol{\\gamma}}_{t}$ from the posterior distribution, which is updated by **Pymc3**, and get the $\\tilde{\\theta}_{i}^{t}$ as $\\frac{logistic(\\boldsymbol{x}_{i,t}^T \\text{ }\\tilde{\\boldsymbol{\\gamma}})+ 1}{2}$ and $\\tilde{v}_{i}^{l}$ as $1/\\tilde{\\theta}_{i}^{l}-1$. Finally, linear programming is employed to determine the optimal assortment $A^{l}$, such that\n",
    "\\begin{equation}\n",
    "    A^{l} = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid\\tilde{\\boldsymbol{v}})=argmax_{a \\in \\mathcal{A}} \\frac{\\sum_{i\\in a}\\eta_{i}\\tilde{v}_{i}}{1+\\sum_{j\\in a} \\tilde{v}_{j}},\n",
    "\\end{equation} where $t$ is the first round of epoch $l$.  \n",
    "\n",
    "It should be noted that the posterior updating step differs for different pairs of the prior distribution of $\\boldsymbol{\\gamma}$ and the reward distribution, and the code can be easily modified to different prior/reward distribution specifications if necessary.\n",
    "\n",
    "## Key Steps\n",
    "For epoch $l = 1,2,\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H}^{l})$ by **Pymc3**;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}^{l})$;\n",
    "3. Update $\\tilde{\\boldsymbol{\\theta}} = \\frac{logistic(\\boldsymbol{x}_{i,t}^T \\text{ }\\tilde{\\boldsymbol{\\gamma}})+ 1}{2}$\n",
    "4. Compute the utility $\\tilde{v}_{i} = \\frac{1}{\\tilde{\\theta}_{i}}-1$;\n",
    "5. Take the action $A^{l}$ w.r.t $\\{\\tilde{v}_{i}\\}_{i=1}^{N}$ such that $A^{l} = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid\\tilde{\\boldsymbol{v}})=argmax_{a \\in \\mathcal{A}} \\frac{\\sum_{i\\in a}\\eta_{i}\\tilde{v}_{i}}{1+\\sum_{j\\in a} \\tilde{v}_{j}}$;\n",
    "6. Offer $A^{l}$ until no purchase appears;\n",
    "7. Receive reward $R^{l}$.\n",
    "\n",
    "*Notations can be found in either the inroduction of the chapter \"Structured Bandits\" or the introduction of the Multinomial Logit Bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/nas/longleaf/home/lge/CausalDM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Slate.MNL import MTSS_MNL\n",
    "from causaldm.learners.Online.Slate.MNL import _env_MNL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20000\n",
    "L = 1000\n",
    "update_freq = 500\n",
    "update_freq_linear = 500\n",
    "\n",
    "phi_beta = 1/4\n",
    "n_init = 500\n",
    "with_intercept = True\n",
    "same_reward = True\n",
    "p=3\n",
    "K=5\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_gamma = sigma_gamma = np.identity(p)\n",
    "mu_gamma = np.zeros(p)\n",
    "seed = 0\n",
    "\n",
    "env = _env_MNL.MNL_env(L, K, T, mu_gamma, sigma_gamma, X_mu, X_sigma,                                       \n",
    "                        phi_beta, same_reward = same_reward, \n",
    "                        seed = seed, p = p, with_intercept = with_intercept)\n",
    "MTSS_agent = MTSS_MNL.MTSS_MNL(L, env.r, K, env.Phi, phi_beta = phi_beta,n_init = n_init,\n",
    "                                    gamma_prior_mean = mu_gamma, gamma_prior_cov = Sigma_gamma,\n",
    "                                    update_freq=update_freq, seed = seed, pm_core = 1, same_reward = same_reward, clip = True)\n",
    "S = MTSS_agent.take_action(env.Phi)\n",
    "t = 1\n",
    "c, exp_R, R = env.get_reward(S)\n",
    "MTSS_agent.receive_reward(S, c, R, exp_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([426, 394, 715, 213, 285])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Ou, M., Li, N., Zhu, S., & Jin, R. (2018). Multinomial logit bandit with linear utility functions. arXiv preprint arXiv:1805.02971.\n",
    "\n",
    "[2] Agrawal, P., Avadhanula, V., & Tulabandhula, T. (2020). A tractable online learning algorithm for the multinomial logit contextual bandit. arXiv preprint arXiv:2011.14033.\n",
    "\n",
    "[3] Oh, M. H., & Iyengar, G. (2019). Thompson sampling for multinomial logit contextual bandits. Advances in Neural Information Processing Systems, 32.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}