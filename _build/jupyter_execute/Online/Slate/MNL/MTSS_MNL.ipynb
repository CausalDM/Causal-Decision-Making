{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTSS_MNL\n",
    "\n",
    "## Main Idea\n",
    "MTSS_MNL is an example of the general Thompson Sampling(TS)-based framework, MTSS [1], to deal with dynamic assortment optimization problems.\n",
    "\n",
    "**Review of MTSS:** MTSS[1] is a meta-learning framework designed for large-scale structured bandit problems [2]. Mainly, it is a TS-based algorithm that learns the information-sharing structure while minimizing the cumulative regrets. Adapting the TS framework to a problem-specific Bayesian hierarchical model, MTSS simultaneously enables information sharing among items via their features and models the inter-item heterogeneity. Specifically, it assumes that the item-specific parameter $\\theta_i$ is sampled from a distribution $g(\\theta_i|\\boldsymbol{x}_i, \\boldsymbol{\\gamma})$ instead of being entirely determined by $\\boldsymbol{x}_i$ via a deterministic function. Here, $g$ is a model parameterized by an **unknown** vector $\\boldsymbol{\\gamma}$. The following is the general feature-based hierarchical model MTSS considered. \n",
    "\\begin{equation}\\label{eqn:general_hierachical}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(Prior)} \\quad\n",
    "\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}),\\\\\n",
    "&\\text{(Generalization function)} \\;\n",
    "\\;    \\theta_i| \\boldsymbol{x}_i, \\boldsymbol{\\gamma}  &&\\sim g(\\theta_i|\\boldsymbol{x}_i, \\boldsymbol{\\gamma}), \\forall i \\in [N],\\\\ \n",
    "&\\text{(Observations)} \\quad\\quad\\quad\\quad\\quad\\quad\\;\n",
    "\\;    \\boldsymbol{Y}_t &&\\sim f(\\boldsymbol{Y}_t|A_t, \\boldsymbol{\\theta}),\\\\\n",
    "&\\text{(Reward)} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\n",
    "\\;   R_t &&= f_r(\\boldsymbol{Y}_t ; \\boldsymbol{\\eta}), \n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "where $Q(\\boldsymbol{\\gamma})$ is the prior distribution for $\\boldsymbol{\\gamma}$. \n",
    "Overall, MTTS is a **general** framework that subsumes a wide class of practical problems, **scalable** to large systems, and **robust** to the specification of the generalization model.\n",
    "\n",
    "**Review of MTSS_MNL:** In this tutorial, as a concrete example, we focus on the epoch-type offering schedule and consider modeling the relationship between $\\theta_i$ and $\\boldsymbol{x}_i$ with the following Beta-Geometric logistic model:\n",
    "\\begin{equation}\\label{eqn1}\n",
    "    \\begin{split}\n",
    "     \\theta_i &\\sim Beta \\big(\\frac{logistic(\\boldsymbol{x}_i^T \\boldsymbol{\\gamma})+ 1}{2}, \\psi \\big) , \\forall i \\in [N],\\\\\n",
    "    Y_{i}^l &\\sim Geometric(\\theta_i), \\forall i \\in A^l,\\\\\n",
    "    R_l &= \\sum_{i\\in A^{l}}Y_{i}^l\\eta_{i},\n",
    "    \\end{split}\n",
    "\\end{equation}where we adopt the mean-precision parameterization of the Beta distribution and $\\psi$ and $\\boldsymbol{\\eta}$ are known. We choose this specific form as it is widely observed that $v_i < 1$ [2,3], i.e., no item is more popular than the no-purchase option. This is equal to $\\theta_i \\in (1/2, 1)$. Other generalization models are also possible with minor modifications to the posterior sampling code. \n",
    "The prior $Q(\\boldsymbol{\\gamma})$ can be chosen as many appropriate distributions. For instance, we choose the prior $\\boldsymbol{\\gamma} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}}, {\\boldsymbol{\\Sigma}}_{\\boldsymbol{\\gamma}})$ with parameters as known. To update the posterior of $\\boldsymbol{\\gamma}$, we utilize the **Pymc3** package [4]. With a given $\\boldsymbol{\\gamma}$, the posterior of $\\boldsymbol{\\theta}$ enjoys the Beta-Geometric conjugate relationship and hence can be updated explicitly and efficiently. Finally, for each epoch $l$, $A^{l}$ is selected by linear programming as\n",
    "\\begin{equation}\n",
    "    A^{l} = argmax_{a \\in \\mathcal{A}} \\frac{\\sum_{i\\in a}\\eta_{i}v_{i}}{1+\\sum_{j\\in a} v_{j}}.\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "üí• Application Situation?\n",
    "\n",
    "## Algorithm Details\n",
    "At each epoch $l$, given the feedback $\\mathcal{H}_{l}$ received from previous rounds, there are two major steps, including posterior sampling and combinatorial optimization. Specifically, the posterior sampling step is decomposed into four steps: 1. approximating a posterior distribution of $\\boldsymbol{\\gamma}$, $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{l})$, by **Pymc3**; 2. sampling a $\\tilde{\\boldsymbol{\\gamma}}$ from $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{l})$; 3. updating the posterior distribution of $\\boldsymbol{\\theta}$ conditional on $\\tilde{\\boldsymbol{\\gamma}}$, $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{l})$, which has an explicit form under the assumption of a Beta-Geometric logistic model; 4. sampling $\\tilde{\\boldsymbol{\\theta}}$ from $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{l})$. Then $\\tilde{v}_{i}$ is calculated as $\\frac{1}{\\tilde{\\theta}_{i}}-1$. Then, the action $A^{l}$ is selected greedily via linear programming. Note that $\\tilde{\\boldsymbol{\\gamma}}$ can be sampled in a batch mode to further facilitate computationally efficient online deployment.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "For epoch $l = 1,2,\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{l})$ by **Pymc3**;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}_{l})$;\n",
    "3. Update $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{l})$\n",
    "4. Sample $\\tilde{\\boldsymbol{\\theta}} \\sim P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{l})$;\n",
    "5. Compute the utility $\\tilde{v}_{i} = \\frac{1}{\\tilde{\\theta}_{i}}-1$\n",
    "6. Take the action $A^{l}$ w.r.t $\\{\\tilde{v}_{i}\\}_{i=1}^{N}$ such that $A^{l} = argmax_{a \\in \\mathcal{A}} \\frac{\\sum_{i\\in a}\\eta_{i}\\tilde{v}_{i}}{1+\\sum_{j\\in a} \\tilde{v}_{j}}$;\n",
    "7. Offer $A^{l}$ until no purchase appears;\n",
    "8. Receive reward $R^{l}$.\n",
    "\n",
    "\n",
    "## Demo Code\n",
    "üí• In the following, we exhibit how to apply the learner on real data.\n",
    "\n",
    "*Notations can be found in the introduction of the combinatorial Semi-Bandit problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2448\\3636065689.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/nas/longleaf/home/lge/CausalDM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# code used to import the learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Slate.MNL import MTSS_MNL\n",
    "from causaldm.learners.Online.Slate.MNL import _env_MNL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20000\n",
    "L = 1000\n",
    "update_freq = 500\n",
    "update_freq_linear = 500\n",
    "\n",
    "phi_beta = 1/4\n",
    "n_init = 500\n",
    "with_intercept = True\n",
    "same_reward = True\n",
    "p=3\n",
    "K=5\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_gamma = sigma_gamma = np.identity(p)\n",
    "mu_gamma = np.zeros(p)\n",
    "seed = 0\n",
    "\n",
    "env = _env_MNL.MNL_env(L, K, T, mu_gamma, sigma_gamma, X_mu, X_sigma,                                       \n",
    "                        phi_beta, same_reward = same_reward, \n",
    "                        seed = seed, p = p, with_intercept = with_intercept)\n",
    "MTSS_agent = MTSS_MNL.MTSS_MNL(L, env.r, K, env.Phi, phi_beta = phi_beta,n_init = n_init,\n",
    "                                    gamma_prior_mean = mu_gamma, gamma_prior_cov = Sigma_gamma,\n",
    "                                    update_freq=update_freq, seed = seed, pm_core = 1, same_reward = same_reward, clip = True)\n",
    "S = MTSS_agent.take_action(env.Phi)\n",
    "t = 1\n",
    "c, exp_R, R = env.get_reward(S)\n",
    "MTSS_agent.receive_reward(S, c, R, exp_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([426, 394, 715, 213, 285])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Wan, R., Ge, L., & Song, R. (2022). Towards Scalable and Robust Structured Bandits: A Meta-Learning Framework. arXiv preprint arXiv:2202.13227.\n",
    "\n",
    "[2] Agrawal, S., Avadhanula, V., Goyal, V., & Zeevi, A. (2017, June). Thompson sampling for the mnl-bandit. In Conference on Learning Theory (pp. 76-78). PMLR.\n",
    "\n",
    "[3] Oh, M. H., & Iyengar, G. (2019). Thompson sampling for multinomial logit contextual bandits. Advances in Neural Information Processing Systems, 32.\n",
    "\n",
    "[4] Salvatier J., Wiecki T.V., Fonnesbeck C. (2016) Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 DOI: 10.7717/peerj-cs.55."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}