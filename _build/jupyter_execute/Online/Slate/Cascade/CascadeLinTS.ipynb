{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CascadeLinTS\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is scalable when the features are used. It outperforms algorithms based on other frameworks, such as UCB, in practice.\n",
    "- **Disadvantage**: It is susceptible to model misspecification.\n",
    "- **Application Situation**: Useful when presenting a ranked list of items, with only one selected at each interaction. The outcome is binary.\n",
    "\n",
    "## Main Idea\n",
    "\n",
    "Motivated by observations in most real-world applications, which have a large number of candidate items, Zong et al. (2016) proposed using feature information that is widely available to improve learning efficiency. Utilizing the feature information of each item $i$, **CascadeLinTS** [1] characterize $\\theta_{i}=E[W_t(i)]$ by assuming that\n",
    "\\begin{equation}\n",
    "\\theta_{i} = logistic(\\boldsymbol{x}_{i,t}^T \\boldsymbol{\\gamma}),\n",
    "\\end{equation}where $logistic(x) \\equiv 1 / (1 + exp^{-1}(x))$. \n",
    "\n",
    "Similar to the Thompson Sampling algorithm with generalized linear bandits [2], we approximate the posterior distribution of $\\boldsymbol{\\gamma}$ by its Laplace approximation. Specifically, we approximate the posterior of $\\boldsymbol{\\gamma}$ as:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\tilde{\\boldsymbol{\\gamma}}^{t} &\\sim \\mathcal{N}\\Big(\\hat{\\boldsymbol{\\gamma}}_{t}, \\alpha^2 \\boldsymbol{H}_{t}^{-1}\\Big),\\\\\n",
    "    \\boldsymbol{H}_{t} &= \\sum_{t}\\mu^{'}(\\boldsymbol{X}_{t}^{T}\\hat{\\boldsymbol{\\gamma}}^{t})\\boldsymbol{X}_{t}\\boldsymbol{X}_{t}^{T},\n",
    "    \\end{split}\n",
    "\\end{equation} where $\\alpha$ is a pre-specified constant to control the degree of exploration, and $\\mu^{'}(\\cdot)$ is the derivative of the mean function. It should be noted that the posterior updating step differs for different pairs of the prior distribution of $\\boldsymbol{\\gamma}$ and the reward distribution, and the code can be easily modified to different prior/reward distribution specifications if necessary.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "For round $t = 1,2,\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$ by the Laplace approximation;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$;\n",
    "3. Update $\\tilde{\\boldsymbol{\\theta}}$ as $logistic(\\boldsymbol{x}_{i,t}^T \\tilde{\\boldsymbol{\\gamma}})$;\n",
    "5. Take the action $A_{t}$ w.r.t $\\tilde{\\boldsymbol{\\theta}}$ such that $A_t = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid \\tilde{\\boldsymbol{\\theta}})$;\n",
    "6. Receive reward $R_{t}$.\n",
    "\n",
    "*Notations can be found in either the inroduction of the chapter \"Structured Bandits\" or the introduction of the cascading Bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 6>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m----> 6\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/nas/longleaf/home/lge/CausalDM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Slate.Cascade import CascadeLinTS\n",
    "from causaldm.learners.Online.Slate.Cascade import _env_Cascade\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, T, K, p = 250, 10000, 3, 5\n",
    "update_freq = 500\n",
    "update_freq_linear = 500\n",
    "\n",
    "phi_beta = 1/4\n",
    "n_init = 500\n",
    "with_intercept = True\n",
    "same_reward = True\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_gamma = sigma_gamma = np.identity(p)\n",
    "mu_gamma = np.zeros(p)\n",
    "seed = 0\n",
    "\n",
    "env = _env_Cascade.Cascading_env(L, K, T, mu_gamma, sigma_gamma,                                   \n",
    "                                    X_mu, X_sigma,                                       \n",
    "                                    phi_beta, same_reward = same_reward, \n",
    "                                    seed = seed, p = p, with_intercept = with_intercept)\n",
    "LinTS_agent = CascadeLinTS.CascadeLinTS(L = L, K = K, p = p)\n",
    "S = LinTS_agent.take_action(env.Phi)\n",
    "t = 1\n",
    "W, E, exp_R, R = env.get_reward(S)\n",
    "LinTS_agent.receive_reward(S, W, E, exp_R, R, t, env.Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([123,  27,  49])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Zong, S., Ni, H., Sung, K., Ke, N. R., Wen, Z., & Kveton, B. (2016). Cascading bandits for large-scale recommendation problems. arXiv preprint arXiv:1603.05359.\n",
    "\n",
    "[2] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., & Boutilier, C. (2020, June). Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics (pp. 2066-2076). PMLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}