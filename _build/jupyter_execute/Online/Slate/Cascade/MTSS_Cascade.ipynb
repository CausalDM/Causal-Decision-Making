{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MTSS_Cascade\n",
    "\n",
    "## Main Idea\n",
    "MTSS_Cascade is an example of the general Thompson Sampling(TS)-based framework, MTSS [1], to deal with online learning to rank problems.\n",
    "\n",
    "**Review of MTSS:** MTSS[1] is a meta-learning framework designed for large-scale structured bandit problems [2]. Mainly, it is a TS-based algorithm that learns the information-sharing structure while minimizing the cumulative regrets. Adapting the TS framework to a problem-specific Bayesian hierarchical model, MTSS simultaneously enables information sharing among items via their features and models the inter-item heterogeneity. Specifically, it assumes that the item-specific parameter $\\theta_i$ is sampled from a distribution $g(\\theta_i|\\boldsymbol{x}_i, \\boldsymbol{\\gamma})$ instead of being entirely determined by $\\boldsymbol{x}_i$ via a deterministic function. Here, $g$ is a model parameterized by an **unknown** vector $\\boldsymbol{\\gamma}$. The following is the general feature-based hierarchical model MTSS considered. \n",
    "\\begin{equation}\\label{eqn:general_hierachical}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(Prior)} \\quad\n",
    "\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}),\\\\\n",
    "&\\text{(Generalization function)} \\;\n",
    "\\;    \\theta_i| \\boldsymbol{x}_i, \\boldsymbol{\\gamma}  &&\\sim g(\\theta_i|\\boldsymbol{x}_i, \\boldsymbol{\\gamma}), \\forall i \\in [N],\\\\ \n",
    "&\\text{(Observations)} \\quad\\quad\\quad\\quad\\quad\\quad\\;\n",
    "\\;    \\boldsymbol{Y}_t &&\\sim f(\\boldsymbol{Y}_t|A_t, \\boldsymbol{\\theta}),\\\\\n",
    "&\\text{(Reward)} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\n",
    "\\;   R_t &&= f_r(\\boldsymbol{Y}_t ; \\boldsymbol{\\eta}), \n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "where $Q(\\boldsymbol{\\gamma})$ is the prior distribution for $\\boldsymbol{\\gamma}$. \n",
    "Overall, MTTS is a **general** framework that subsumes a wide class of practical problems, **scalable** to large systems, and **robust** to the specification of the generalization model.\n",
    "\n",
    "**Review of MTSS_Cascade:** To characterize the relationship between items using their features, one example choice of $g$ is the popular Beta-Bernoulli logistic model [1,2], where $\\theta_i {\\sim} Beta(logistic(\\boldsymbol{x}_i^T \\boldsymbol{\\gamma}), \\psi)$ for some known parameter $\\psi$. We adopt the mean-precision parameterization of the Beta distribution, with $logistic(\\boldsymbol{x}_i^T \\boldsymbol{\\gamma})$ being the mean and $\\psi$ being the precision parameter. Specifically, the full model is as follows: \n",
    "\\begin{equation}\\label{eqn:model_cascading}\n",
    "    \\begin{split}\n",
    "    \\theta_i & \\sim Beta(logistic(\\boldsymbol{x}_i^T \\boldsymbol{\\gamma}), \\psi), \\forall i \\in [N]\\\\\n",
    "    W_{k, t} &\\sim Bernoulli(\\theta_{a^k_t}), \\forall k \\in [K], \\\\\n",
    "    Y_{k,t} &= W_{k,t} E_{k,t}, \\forall k \\in [K],\\\\\n",
    "    E_{k,t} &= (1-Y_{k-1}) E_{k-1,t}, \\forall k \\in [K],\\\\\n",
    "    R_t &= \\sum_{k \\in [K]} Y_{k,t}, \n",
    "    \\end{split}\n",
    "\\end{equation} with $E_{1,t} \\equiv 1$.   \n",
    "The prior $Q(\\boldsymbol{\\gamma})$ can be chosen as many appropriate distributions. For instance, we choose the prior $\\boldsymbol{\\gamma} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}}, {\\boldsymbol{\\Sigma}}_{\\boldsymbol{\\gamma}})$ with parameters as known. To update the posterior of $\\boldsymbol{\\gamma}$, we utilize the **Pymc3** package [3]. With a given $\\boldsymbol{\\gamma}$, the posterior of $\\boldsymbol{\\theta}$ enjoys the Beta-Geometric conjugate relationship and hence can be updated explicitly and efficiently. Finally, for each round, we select the top $K$ items with the highest estimated attractiveness factors.\n",
    "\n",
    "üí• Application Situation?\n",
    "\n",
    "\n",
    "## Algorithm Details\n",
    "At each round $t$, given the feedback $\\mathcal{H}_{t}$ received from previous rounds, there are two major steps including posterior sampling and combinatorial optimization. Specifically, the posterior sampling step is decomposed into four steps: 1. approximating a posterior distribution of $\\boldsymbol{\\gamma}$, $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$, by **Pymc3**; 2. sampling a $\\tilde{\\boldsymbol{\\gamma}}$ from $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$; 3. updating the posterior distribution of $\\boldsymbol{\\theta}$ conditional on $\\tilde{\\boldsymbol{\\gamma}}$, $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$, which has an explicit form under the assumption of a Beta-Bernoulli logistic model; 4. sampling $\\tilde{\\boldsymbol{\\theta}}$ from $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$. Then, the action $A_{t}$ is selected greedily as $A_t = arg max_{a \\in \\mathcal{A}} E(R_t \\mid a, \\tilde{\\boldsymbol{\\theta}})$. Specifically, the top $K$ items with the highest $\\tilde{\\theta}_{i}$ will be displayed. Note that $\\tilde{\\boldsymbol{\\gamma}}$ can be sampled in a batch mode to further facilitate computationally efficient online deployment.\n",
    "\n",
    "## Key Steps\n",
    "For round $t = 1,2,\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$ by **Pymc3**;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$;\n",
    "3. Update $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$;\n",
    "4. Sample $\\tilde{\\boldsymbol{\\theta}} \\sim P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$;\n",
    "5. Take the action $A_{t}$ w.r.t $\\tilde{\\boldsymbol{\\theta}}$ such that $A_t = arg max_{a \\in \\mathcal{A}} E(R_t \\mid a, \\tilde{\\boldsymbol{\\theta}})$;\n",
    "6. Receive reward $R_{t}$.\n",
    "\n",
    "\n",
    "## Demo Code\n",
    "üí• In the following, we exhibit how to apply the learner on real data.\n",
    "\n",
    "*Notations can be found in the introduction of the combinatorial Semi-Bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_427184\\3636065689.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/nas/longleaf/home/lge/CausalDM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;31m# code used to import the learner\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] Á≥ªÁªüÊâæ‰∏çÂà∞ÊåáÂÆöÁöÑË∑ØÂæÑ„ÄÇ: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Slate.Cascade import MTSS_Cascade\n",
    "from causaldm.learners.Online.Slate.Cascade import _env_Cascade\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "L, T, K, p = 250, 10000, 3, 5\n",
    "update_freq = 500\n",
    "update_freq_linear = 500\n",
    "\n",
    "phi_beta = 1/4\n",
    "n_init = 500\n",
    "with_intercept = True\n",
    "same_reward = True\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_gamma = sigma_gamma = np.identity(p)\n",
    "mu_gamma = np.zeros(p)\n",
    "seed = 0\n",
    "\n",
    "env = _env_Cascade.Cascading_env(L, K, T, mu_gamma, sigma_gamma,                                   \n",
    "                                    X_mu, X_sigma,                                       \n",
    "                                    phi_beta, same_reward = same_reward, \n",
    "                                    seed = seed, p = p, with_intercept = with_intercept)\n",
    "MTSS_agent = MTSS_Cascade.MTSS_Cascade(phi_beta = phi_beta, K = K\n",
    "                                              , gamma_prior_mean = mu_gamma, gamma_prior_cov = Sigma_gamma\n",
    "                                              , Xs = env.Phi\n",
    "                                              , update_freq = update_freq, n_init = n_init)\n",
    "S = MTSS_agent.take_action(env.Phi)\n",
    "t = 1\n",
    "W, E, exp_R, R = env.get_reward(S)\n",
    "MTSS_agent.receive_reward(S, W, E, exp_R, R, t, env.Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 95, 152, 126])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Wan, R., Ge, L., & Song, R. (2022). Towards Scalable and Robust Structured Bandits: A Meta-Learning Framework. arXiv preprint arXiv:2202.13227.\n",
    "\n",
    "[2] Forcina, A. and Franconi, L. Regression analysis with the beta-binomial distribution. Rivista di Statistica Applicata, 21(1), 1988.\n",
    "\n",
    "[3] Salvatier J., Wiecki T.V., Fonnesbeck C. (2016) Probabilistic programming in Python using PyMC3. PeerJ Computer Science 2:e55 DOI: 10.7717/peerj-cs.55."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}