{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ca32e0",
   "metadata": {},
   "source": [
    "# LinUCB\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is more scalable and efficient than **UCB** by utilizing features.\n",
    "- **Disadvantage**:  \n",
    "- **Application Situation**: discrete action space, Gaussian reward space\n",
    "\n",
    "## Main Idea\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. **LinUCB** uses feature information to guide exploration by assuming a linear model between the expected potential reward and the features. Specifcially, for the Gaussain bandits, we assume that \n",
    "\\begin{align}\n",
    "E(R_{t}(a)) = \\theta_a = \\boldsymbol{s}_a^T \\boldsymbol{\\gamma}.\n",
    "\\end{align} Solving a linear gression model, at each round $t$, the corresponding estimated upper confidence interval of the mean potential reward is then updated as\n",
    "\\begin{align}\n",
    "U_a^t = \\boldsymbol{s}_a^T \\hat{\\boldsymbol{\\gamma}} + \\alpha\\sqrt{\\boldsymbol{s}_a^T V^{-1}  \\boldsymbol{s}_a},\n",
    "\\end{align} where $\\alpha$ is a tuning parameter that controls the rate of exploration, $V^{-1} = \\sum_{j=0}^{t-1}\\boldsymbol{s}_{a_j}\\boldsymbol{s}_{a_j}^T$, and $\\hat{\\boldsymbol{\\gamma}} = V^{-1}\\sum_{j=0}^{t-1}\\boldsymbol{s}_{a_j}R_j$.\n",
    "\n",
    "As for the Bernoulli bandits, we assume that \n",
    "\\begin{align}\n",
    "\\theta_{a} = logistic(\\boldsymbol{s}_a^T \\boldsymbol{\\gamma}),\n",
    "\\end{align}where $logistic(s) \\equiv 1 / (1 + exp^{-1}(s))$. At each round $t$, by fitting a generalized linear model to all historical observations, we obtain the maximum likelihood estimator of $\\boldsymbol{\\gamma}$. The corresponding estimated confidence upper bound is then calculated in the same way as for Gaussian bandits, such that\n",
    "\\begin{align}\n",
    "U_a^t = \\boldsymbol{s}_a^T \\hat{\\boldsymbol{\\gamma}} + \\alpha\\sqrt{\\boldsymbol{s}_a^T V^{-1}  \\boldsymbol{s}_a},\n",
    "\\end{align}where $\\alpha$ and $V$ are defined in the same way as before. \n",
    "\n",
    "Finally, using the estimated upper confidence bounds, $A_t = \\arg \\max_{a \\in \\mathcal{A}} U_a^t$ would be selected.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Initializing $\\hat{\\boldsymbol{\\gamma}}=\\boldsymbol{0}$ and $V = I$, and specifying $\\alpha$;\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "    - Calculate the upper confidence bound $U_a^t$;\n",
    "    - Select action $A_t$ as the arm with the maximum $U_a^t$;\n",
    "    - Receive the reward R, and update $\\hat{\\boldsymbol{\\gamma}}$, $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1be59",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b02f48",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/c5dcq1_d1gn8w9yg5728l9dm0000gn/T/ipykernel_16376/3636065689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/nas/longleaf/home/lge/CausalDM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# code used to import the learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "291f88a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Single import LinUCB\n",
    "from causaldm.learners.Online.Single import Env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c9c17056",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 5\n",
    "with_intercept = True\n",
    "p=3\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_theta = sigma_gamma = np.identity(p)\n",
    "mu_theta = np.zeros(p)\n",
    "seed = 0\n",
    "sigma = 1\n",
    "\n",
    "env = Env.Single_Gaussian_Env(T, K, p, sigma\n",
    "                         , mu_theta, Sigma_theta\n",
    "                        , seed = 42, with_intercept = True\n",
    "                         , X_mu = X_mu, X_Sigma = X_sigma)\n",
    "LinUCB_Gaussian_agent = LinUCB.LinUCB_Gaussian(alpha = .5, K = K, p = p)\n",
    "A = LinUCB_Gaussian_agent.take_action(env.Phi)\n",
    "t = 0\n",
    "R = env.get_reward(t,A)\n",
    "LinUCB_Gaussian_agent.receive_reward(t,A,R, env.Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc79fede",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 1., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinUCB_Gaussian_agent.cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee7a2a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Chu, W., Li, L., Reyzin, L., & Schapire, R. (2011, June). Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (pp. 208-214). JMLR Workshop and Conference Proceedings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec41cc-5469-4c5b-95dc-f44e1ed5b14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}