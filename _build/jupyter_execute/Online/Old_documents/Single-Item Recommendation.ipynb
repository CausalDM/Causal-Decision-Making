{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Single-Item Recommendation\n",
    "\n",
    "The bandit problems have received increasing attention recently and have been widely applied to areas such as clinical trials [1], finance [2], and recommendation systems [3], among others. The most classical version of it is the multi-armed bandit (MAB) [4], where an agent will sequentially select an item (arm) from a few and then receive a random reward for the item selected. Since the reward distributions are unknown in most real applications, the central task of a MAB algorithm is to learn the distributions from feedback received and find the optimal item to maximize the cumulative rewards or, equivalently, to minimize the cumulative regret. This chapter focuses on the MAB problems by illustrating a group of classical algorithms to tackle the well-known exploration-exploitation trade-off.\n",
    "\n",
    "## Problem Setting\n",
    "Let $T$ be the total number of rounds, and $K$ be the number of arms (actions to be selected). The agent would choose one arm at each round $t = 1, \\dots, T$. Then the agent will receive the corresponding stochastic reward $R_t$ from the environment. Denote the expected reward for each arm $i$ as $r_{i}$. Since, in most real applications, such a reward distribution is always unknown, the agent needs to learn the reward distribution from feedback received. Overall, the objective is to find a bandit algorithm to maximize the cumulative Reward $\\sum_{t=1}^{T}R_{t}$.\n",
    "\n",
    "MAB has been extensively studied and widely applied to different areas, including healthcare, recommender system, and finance, to name a few. See [4] for a detailed review of MAB and [5] for a survey of practical applications. Among them, the ultimate goal of a learning algorithm is always to strike a good balance between exploration (try an unfamiliar action to learn more information) and exploitation (take the action that has the highest estimated reward so far) so as to maximize the cumulative reward. In the following, we will briefly illustrate three popular and classical categories of algorithms to handle the exploration-exploitation trade-off: i) $\\epsilon$-greedy, ii) Upper Confidence Bound (UCB), and iii) Thompson Sampling (TS). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Claasical Methods\n",
    "### $\\epsilon$-Greedy\n",
    "An intuitive algorithm to incorporate the exploration and exploitation is $\\epsilon$-Greedy, which is simple and widely used [6]. Specifically, at each round $t$, we will select a random action with probability $\\epsilon$, and select an action with the highest estimated mean reward based on the history so far with probability $1-\\epsilon$. Here the parameter $\\epsilon$ is pre-specified. A more adaptive variant is $\\epsilon_{t}$-greedy, where the probability of taking a random action is defined as a decreasing function of $t$. Auer et al. [7] showed that $\\epsilon_{t}$-greedy performs well in practice with $\\epsilon_{t}$ decreases to 0 at a rate of $\\frac{1}{t}$.\n",
    "\n",
    "#### Supported Algorithms\n",
    "\n",
    "| algorithm | Reward | with features? | Advantage |\n",
    "|:-|:-:|:-:|:-:|\n",
    "| [$\\epsilon$-greedy]() | Binary/Gaussian | |Simple| \n",
    "\n",
    "\n",
    "### Thompson Sampling\n",
    "Thompson Sampling, also known as posterior sampling, solves the exploration-exploitation dilemma by selecting an action according to its posterior distribution [8].  At each round $t$, the agent sample the rewards from the corresponding posterior distributions and then select the action with the highest sampled reward greedily. It has been shown that, when the true reward distribution is known, a TS algorithm with the true reward distribution as the prior is nearly optimal [9]. However, such a distribution is always unknown in practice. Therefore, one of the major objectives of TS-based algorithms is to find an informative prior to guide the exploration.\n",
    "\n",
    "#### Supported Algorithms\n",
    "\n",
    "| algorithm | Reward | with features? | Advantage |\n",
    "|:-|:-:|:-:|:-:|\n",
    "| [TS [8]](https://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/sampling/TS_Tutorial.pdf) | Binary/Guaasian | | | \n",
    "| [LinTS [13]](http://proceedings.mlr.press/v28/agrawal13.pdf) | Gaussian | ✅ | | \n",
    "| [GLMTS [12]](http://proceedings.mlr.press/v108/kveton20a/kveton20a.pdf) | GLM | ✅ | | \n",
    "\n",
    "### Upper Confidence Bounds \n",
    "As the name suggested, the UCB algorithm estimates the upper confidence bound $U_{i}^{t}$ of the mean rewards based on the observations and then choose the action has the highest estimates. The class of UCB-based algorithms is firstly introduced by Auer et al. [7]. Generally, at each round $t$, $U_{i}^{t}$ is calculated as the sum of the estimated reward (exploitation) and the estimated confidence radius (exploration) of item $i$ based on $\\mathcal{H}_{t}$. Then, $A_{t}$ is selected as \n",
    "\\begin{equation}\n",
    "    A_t = argmax_{a \\in \\mathcal{A}} E(R_t \\mid a,\\{ U_{i}^{t}\\}_{i=1}^{N}).\n",
    "\\end{equation} As an example, **UCB1** [7] estimates the confidence radius as $\\sqrt{\\frac{2log(t)}{\\text{\\# item $i$ played so far}}}$. Doing so, either the item with a large average reward or the item with limited exploration will be selected.\n",
    "\n",
    "#### Supported Algorithms\n",
    "\n",
    "| algorithm | Reward | with features? | Advantage |\n",
    "|:-|:-:|:-:|:-:|\n",
    "| [UCB1 [7]](https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf) | Binary/Gaussian | | | \n",
    "| [LinUCB [10]](https://dl.acm.org/doi/pdf/10.1145/1772690.1772758?casa_token=CJjeIziLmjEAAAAA:CkRvgHQNqpy10rzcUP5kx31NWJmgSldd6zx8wZxskZYCoCc8v7EDIw3t3Gk1_6mfurqQTqRZ7fVA) | Guassian | ✅ | | \n",
    "| [UCB-GLM [11]](http://proceedings.mlr.press/v70/li17c/li17c.pdf) | GLM | ✅ | | \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Real Data\n",
    "**1. MovieLens**\n",
    "\n",
    "Movie Lens is a website that helps users find the movies they like and where they will rate the recommended movies. [MovieLens 1M dataset](https://grouplens.org/datasets/movielens/1m/) is a dataset including the observations collected in an online movie recommendation experiment and is widely used to generate data for online bandit simulation studies. The goal of the simulation studies below is to learn the reward distribution of different movie genres and hence to recommend the optimal movie genres to the users to optimize the cumulative user satisfaction. In other words, every time a user visits the website, the agent will recommend a movie genre ($A_t$) to the user, and then the user will give a rating ($R_t$) to the genre recommended. We assume that users' satisfaction is fully reflected through the ratings. Therefore, the ultimate goal of the bandit algorithms is to optimize the cumulative ratings received by finding and recommending the optimal movie genre that will receive the highest rating. In this chapter, we mainly focus on the top 5 Genres, including \n",
    "\n",
    "- **Comedy**: $a=0$,\n",
    "- **Drama**: $a=1$,\n",
    "- **Action**: $a=2$,\n",
    "- **Thriller**: $a=3$,\n",
    "- **Sci-Fi**: $a=4$.\n",
    "\n",
    "Therefore, $K=5$. For each user, feature information, including age, gender and occupation, are available:\n",
    "\n",
    "- **age**: numerical, from 18 to 56,\n",
    "- **gender**: binary, =1 if male,\n",
    "- **college/grad student**: binary, =1 if a college/grad student,\n",
    "- **executive/managerial**: binary, =1 if a executive/managerial,\n",
    "- **academic/educator**: binary, =1 if an academic/educator,\n",
    "- **technician/engineer**: binary, =1 if a technician/engineer,\n",
    "- **writer**: if a writer, then all the previous occupation-related variables = 0 (baseline).\n",
    "\n",
    "Furthermore, there are two different types of the reward $R_t$:\n",
    "\n",
    "- **Gaussian Bandit**: $R_t$ is a numerical variable, taking the value of $\\{1,2,3,4,5\\}$, where 1 is the least satisfied and 5 is the most satisfied.\n",
    "- **Bernoulli Bandit**: $R_t$ is a binary variable, =1 if the rating is higher than 3.\n",
    "\n",
    "In the following, we evaluated the empirical performance of the supported algorithms on the MovieLens dataset under either the Gaussian bandit or Bernoulli bandit settings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation 1: Gaussian Bandit\n",
    "We repeat the experiment over **100** random seeds, with $T = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation 1: Bernoulli Bandit\n",
    "We repeat the experiment over **100** random seeds, with $T = 100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference\n",
    "[1] Durand, A., Achilleos, C., Iacovides, D., Strati, K., Mitsis, G. D., and Pineau, J. (2018). Contextual bandits for adapting treatment in a mouse model of de novo carcinogenesis. In Machine learning for healthcare conference, pages 67–82. PMLR.\n",
    "\n",
    "[2] Shen, W., Wang, J., Jiang, Y.-G., and Zha, H. (2015). Portfolio choices with orthogonal bandit learning. In Twenty-fourth international joint conference on artificial intelligence.\n",
    "\n",
    "[3] Zhou, Q., Zhang, X., Xu, J., and Liang, B. (2017). Large-scale bandit approaches for recommender systems. In International Conference on Neural Information Processing, pages 811–821. Springer.\n",
    "\n",
    "[4] Slivkins, A. (2019). Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272.\n",
    "\n",
    "[5] Bouneffouf, D. and Rish, I. (2019). A survey on practical applications of multi-armed and contextual bandits. arXiv preprint arXiv:1904.10040.\n",
    "\n",
    "[6] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press\n",
    "\n",
    "[7] Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235–256.\n",
    "\n",
    "[8] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. (2017). A tutorial on thompson sampling. arXiv preprint arXiv:1707.0203\n",
    "\n",
    "[9] Lattimore, T. and Szepesv´ari, C. (2020). Bandit algorithms. Cambridge University Press.\n",
    "\n",
    "[10] Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670\n",
    "\n",
    "[11] Li, L., Lu, Y., and Zhou, D. (2017). Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071–2080. PMLR.\n",
    "\n",
    "[12] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., and Boutilier, C. (2020). Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 2066–2076. PMLR.\n",
    "\n",
    "[13] Agrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127–135. PMLR.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}