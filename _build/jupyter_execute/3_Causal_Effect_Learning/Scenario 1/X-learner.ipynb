{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8lwheJQ8RxAw",
   "metadata": {
    "id": "8lwheJQ8RxAw"
   },
   "source": [
    "### **3. X-learner**\n",
    "Next, let's introduce the X-learner. As a combination of S-learner and T-learner, the X-learner can use information from the control(treatment) group to derive better estimators for the treatment(control) group, which is provably more efficient than the above two.\n",
    "\n",
    "The basic\n",
    "\n",
    "\n",
    "**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods (same as T-learner);\n",
    "\n",
    "\n",
    "**Step 2:**  Obtain the imputed treatment effects for individuals\n",
    "\\begin{equation*}\n",
    "\\tilde{\\Delta}_i^1:=R_i^1-\\hat\\mu_0(S_i^1), \\quad \\tilde{\\Delta}_i^0:=\\hat\\mu_1(S_i^0)-R_i^0.\n",
    "\\end{equation*}\n",
    "\n",
    "**Step 3:**  Fit the imputed treatment effects to obtain $\\hat\\tau_1(s):=\\mathbb{E}[\\tilde{\\Delta}_i^1|S=s]$ and $\\hat\\tau_0(s):=\\mathbb{E}[\\tilde{\\Delta}_i^0|S=s]$;\n",
    "\n",
    "**Step 4:**  The final HTE estimator is given by\n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{X-learner}}(s)=g(s)\\hat\\tau_0(s)+(1-g(s))\\hat\\tau_1(s),\n",
    "\\end{equation*}\n",
    "\n",
    "where $g(s)$ is a weight function between $[0,1]$. A possible way is to use the propensity score model as an estimate of $g(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "epBAzoVzVqTK",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5036,
     "status": "ok",
     "timestamp": 1676133719672,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "epBAzoVzVqTK",
    "outputId": "2f3f929c-fbba-40a3-d9bf-b74ea77f3f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-uplift in /opt/anaconda3/lib/python3.8/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (0.23.1)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (4.47.0)\r\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (1.4.2)\r\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (3.2.2)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy>=1.16 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (1.22.4)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (2.24.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (0.16.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.5.0)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (2.1.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->scikit-uplift) (2020.1)\r\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->scikit-uplift) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (0.10.0)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (1.2.0)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (3.0.4)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (1.25.9)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (2020.6.20)\r\n",
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (2.10)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas->scikit-uplift) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "kDtKXxNtVqTM",
   "metadata": {
    "id": "kDtKXxNtVqTM"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d2906cddc1f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import related packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util_causaldm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# import related packages\n",
    "from matplotlib import pyplot as plt;\n",
    "from lightgbm import LGBMRegressor;\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from causaldm._util_causaldm import *;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dMktSxiPVqTN",
   "metadata": {
    "id": "dMktSxiPVqTN"
   },
   "outputs": [],
   "source": [
    "n = 10**3  # sample size in observed data\n",
    "n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n",
    "seed=223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4rhP8UeSVqTN",
   "metadata": {
    "id": "4rhP8UeSVqTN"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n",
    "#data_target = get_data_simulation(n0, seed, policy=\"target\")\n",
    "\n",
    "# The true expected heterogeneous treatment effect\n",
    "HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sfb-mplOP9HJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 449,
     "status": "ok",
     "timestamp": 1676133738275,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "sfb-mplOP9HJ",
    "outputId": "8b2be802-5918-414b-de0f-8d26dd0930cf"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=3)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Fit two models under treatment and control separately, same as T-learner\n",
    "\n",
    "import numpy as np\n",
    "mu0 = LGBMRegressor(max_depth=3)\n",
    "mu1 = LGBMRegressor(max_depth=3)\n",
    "\n",
    "S_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2]\n",
    "S_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2]\n",
    "R_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] \n",
    "R_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] \n",
    "\n",
    "mu0.fit(S_T0, R_T0)\n",
    "mu1.fit(S_T1, R_T1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zb42ZMw3pkqm",
   "metadata": {
    "id": "zb42ZMw3pkqm"
   },
   "outputs": [],
   "source": [
    "# Step 2: impute the potential outcomes that are unobserved in original data\n",
    "\n",
    "n_T0 = len(R_T0)\n",
    "n_T1 = len(R_T1)\n",
    "\n",
    "Delta0 = mu1.predict(S_T0) - R_T0\n",
    "Delta1 = R_T1 - mu0.predict(S_T1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxYLjE0Ar2_5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 194,
     "status": "ok",
     "timestamp": 1676133740928,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "pxYLjE0Ar2_5",
    "outputId": "539a9b7e-21ad-4c03-8a48-da65ec5d3a2e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=2)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit tau_1(s) and tau_0(s)\n",
    "\n",
    "tau0 = LGBMRegressor(max_depth=2)\n",
    "tau1 = LGBMRegressor(max_depth=2)\n",
    "\n",
    "tau0.fit(S_T0, Delta0)\n",
    "tau1.fit(S_T1, Delta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LRvEZ4uluT-U",
   "metadata": {
    "id": "LRvEZ4uluT-U"
   },
   "outputs": [],
   "source": [
    "# Step 4: fit the propensity score model $\\hat{g}(s)$ and obtain the final HTE estimator by taking weighted average of tau0 and tau1\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "g = LogisticRegression()\n",
    "g.fit(data_behavior.iloc[:,0:2],data_behavior['A'])\n",
    "\n",
    "HTE_X_learner = g.predict_proba(data_behavior.iloc[:,0:2])[:,0]*tau0.predict(data_behavior.iloc[:,0:2]) + g.predict_proba(data_behavior.iloc[:,0:2])[:,1]*tau1.predict(data_behavior.iloc[:,0:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mz8I_J0h4N6B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 172,
     "status": "ok",
     "timestamp": 1676133743287,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "Mz8I_J0h4N6B",
    "outputId": "a45b9739-0f4e-4934-dacd-e17420d86055"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-learner:   [ 1.9341  1.9235  0.2944  0.2013 -0.4147 -0.5626 -2.214  -1.5443]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"X-learner:  \",HTE_X_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XyxjzDBy4N6k",
   "metadata": {
    "id": "XyxjzDBy4N6k"
   },
   "source": [
    "From the result above we can see that X-learner can roughly catch the trend of treatment effect w.r.t. the change of baseline information $S$. In this synthetic example, X-learner also performs slightly better than T-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlbacETd4N6l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12,
     "status": "ok",
     "timestamp": 1676133743982,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "rlbacETd4N6l",
    "outputId": "b99bc160-95ce-4eee-8718-376a69ce0164"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of X-learner is :      0.2827518068171628 , \n",
      " The overall estimation variance of X-learner is : 1.7686646616779012 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_X_learner = np.sum(HTE_X_learner-HTE_true)/n\n",
    "Variance_X_learner = np.sum((HTE_X_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of X-learner is :     \", Bias_X_learner, \", \\n\", \"The overall estimation variance of X-learner is :\",Variance_X_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EnxQ_Tkg4o_q",
   "metadata": {
    "id": "EnxQ_Tkg4o_q"
   },
   "source": [
    "**Conclusion:** In this toy example, the overall estimation variance of X-learner is the smallest, followed by T-learner, and the worst is given by S-learner.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "zxpRscObJmbX",
   "metadata": {
    "id": "zxpRscObJmbX"
   },
   "source": [
    "**Note**: For more details about the meta learners, please refer to [1]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nyirbjS5JdGh",
   "metadata": {
    "id": "nyirbjS5JdGh"
   },
   "source": [
    "## References\n",
    "1. Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 4156–4165.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}