{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Z_pBCOWOruCh",
   "metadata": {
    "id": "Z_pBCOWOruCh"
   },
   "source": [
    "## **R-Learner, DR-Learner, and Lp-R-Learner**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32szzPY4RyWO",
   "metadata": {
    "id": "32szzPY4RyWO"
   },
   "source": [
    "### **4. R learner**\n",
    "The idea of classical R-learner came from Robinson 1988 [3] and was formalized by Nie and Wager in 2020 [2]. The main idea of R learner starts from the partially linear model setup, in which we assume that\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    R&=A\\tau(S)+g_0(S)+U,\\\\\n",
    "    A&=m_0(S)+V,\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "where $U$ and $V$ satisfies $\\mathbb{E}[U|D,X]=0$, $\\mathbb{E}[V|X]=0$.\n",
    "\n",
    "After several manipulations, it’s easy to get\n",
    "\\begin{equation}\n",
    "\tR-\\mathbb{E}[R|S]=\\tau(S)\\cdot(A-\\mathbb{E}[A|S])+\\epsilon.\n",
    "\\end{equation}\n",
    "Define $m_0(X)=\\mathbb{E}[A|S]$ and $l_0(X)=\\mathbb{E}[R|S]$. A natural way to estimate $\\tau(X)$ is given below, which is also the main idea of R-learner:\n",
    "\n",
    "**Step 1**: Regress $R$ on $S$ to obtain model $\\hat{\\eta}(S)=\\hat{\\mathbb{E}}[R|S]$; and regress $A$ on $S$ to obtain model $\\hat{m}(S)=\\hat{\\mathbb{E}}[A|S]$.\n",
    "\n",
    "**Step 2**: Regress outcome residual $R-\\hat{l}(S)$ on propensity score residual $A-\\hat{m}(S)$.\n",
    "\n",
    "That is,\n",
    "\\begin{equation}\n",
    "\t\\hat{\\tau}(S)=\\arg\\min_{\\tau}\\left\\{\\mathbb{E}_n\\left[\\left(\\{R_i-\\hat{\\eta}(S_i)\\}-\\{A_i-\\hat{m}(S_i)\\}\\cdot\\tau(S_i)\\right)^2\\right]\\right\\}\t\n",
    "\\end{equation}\n",
    "\n",
    "The easiest way to do so is to specify $\\hat{\\tau}(S)$ to the linear function class. In this case, $\\tau(S)=S\\beta$, and the problem becomes to estimate $\\beta$ by solving the following linear regression:\n",
    "\\begin{equation}\n",
    "\t\\hat{\\beta}=\\arg\\min_{\\beta}\\left\\{\\mathbb{E}_n\\left[\\left(\\{R_i-\\hat{\\eta}(S_i)\\}-\\{A_i-\\hat{m}(S_i)\\} S_i\\cdot \\beta\\right)^2\\right]\\right\\}.\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRpP5k9MBtzO",
   "metadata": {
    "id": "eRpP5k9MBtzO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
<<<<<<< Updated upstream
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import related packages\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression \n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
=======
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import related packages\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LogisticRegression \n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
>>>>>>> Stashed changes
     ]
    }
   ],
   "source": [
    "# import related packages\n",
    "from matplotlib import pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners.Causal_Effect_Learning.Single_Stage.Rlearner import Rlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovM_twTxuOj",
   "metadata": {
    "id": "lovM_twTxuOj"
   },
   "outputs": [],
   "source": [
    "n = 10**3  # sample size in observed data\n",
    "n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n",
    "seed=223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "AnRQO0viX3D1",
   "metadata": {
    "id": "AnRQO0viX3D1"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n",
    "#data_target = get_data_simulation(n0, seed, policy=\"target\")\n",
    "\n",
    "# The true expected heterogeneous treatment effect\n",
    "HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jYIe491FKQAs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 480,
     "status": "ok",
     "timestamp": 1675479873490,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "jYIe491FKQAs",
    "outputId": "f6d25b27-5303-41d8-fcde-ede3c1acbd85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate with R-learner\n",
      "fold 1,testing r2 y_learner: 0.942, ps_learner: 0.943\n",
      "fold 2,testing r2 y_learner: 0.958, ps_learner: 0.966\n",
      "fold 3,testing r2 y_learner: 0.951, ps_learner: 0.948\n",
      "fold 4,testing r2 y_learner: 0.957, ps_learner: 0.932\n",
      "fold 5,testing r2 y_learner: 0.950, ps_learner: 0.944\n",
      "fold 1, training r2 R-learner: 0.683, testing r2 R-learner: 0.584\n",
      "fold 2, training r2 R-learner: 0.659, testing r2 R-learner: 0.705\n",
      "fold 3, training r2 R-learner: 0.677, testing r2 R-learner: 0.536\n",
      "fold 4, training r2 R-learner: 0.667, testing r2 R-learner: 0.642\n",
      "fold 5, training r2 R-learner: 0.669, testing r2 R-learner: 0.551\n"
     ]
    }
   ],
   "source": [
    "# R-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model = LogisticRegression()\n",
    "Rlearner_model = LGBMRegressor(max_depth=2)\n",
    "\n",
    "HTE_R_learner = Rlearner(data_behavior, outcome, treatment, controls, n_folds, y_model, ps_model, Rlearner_model)\n",
    "HTE_R_learner = HTE_R_learner.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D_B2JzoeEVkM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1674887445154,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "D_B2JzoeEVkM",
    "outputId": "9550e64d-4d25-441f-f105-514a6846a9d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-learner:   [-0.4971  0.0231 -1.0514 -0.0037 -1.0943 -1.4128 -1.1436 -1.4714]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"R-learner:  \",HTE_R_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2FvnH_FtEVkj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1674887465909,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "2FvnH_FtEVkj",
    "outputId": "dd2383fd-395c-4204-9eb0-346c39882475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of R-learner is :      0.010664510462813687 , \n",
      " The overall estimation variance of R-learner is : 3.3201771635462656 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_R_learner = np.sum(HTE_R_learner-HTE_true)/n\n",
    "Variance_R_learner = np.sum((HTE_R_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of R-learner is :     \", Bias_R_learner, \", \\n\", \"The overall estimation variance of R-learner is :\",Variance_R_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EWhausRhExr5",
   "metadata": {
    "id": "EWhausRhExr5"
   },
   "source": [
    "**Conclusion:** It's amazing to see that the bias of R-learner is significantly smaller than all other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J2z2JRumRzdo",
   "metadata": {
    "id": "J2z2JRumRzdo"
   },
   "source": [
    "### **5. DR-learner**\n",
    "\n",
    "DR-learner is a two-stage doubly robust estimator for HTE estimation. Before Kennedy et al. 2020 [4], there are several related approaches trying to extend the doubly robust procedure to HTE estimation, such as [5, 6, 7]. Compared with the above three estimators, DR-learner is proved to be oracle efficient under some mild assumptions detailed in Theorem 2 of [4].\n",
    "\n",
    "The basic steps of DR-learner is given below:\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1}^n$ to construct estimates $\\hat{\\pi}$ for the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1}^n$ to construct estimates $\\hat\\mu_a(s)$ for $\\mu_a(s):=\\mathbb{E}[R|S=s,A=a]$;\n",
    "\n",
    "**Step 2**: Pseudo-outcome regression: \\\\\n",
    "Define $\\widehat{\\phi}(Z)$ as the pseudo-outcome where \n",
    "\\begin{equation}\n",
    "\\widehat{\\phi}(Z)=\\frac{A-\\hat{\\pi}(S)}{\\hat{\\pi}(S)\\{1-\\hat{\\pi}(S)\\}}\\Big\\{R-\\hat{\\mu}_A(S)\\Big\\}+\\hat{\\mu}_1(S)-\\hat{\\mu}_0(S),\n",
    "\\end{equation}\n",
    "and regress it on covariates $S$ in the test sample $I_2^n$, yielding \n",
    "\\begin{equation}\n",
    "\\widehat{\\tau}_{\\text{DR-learner}}(s)=\\widehat{\\mathbb{E}}_n[\\widehat{\\phi}(Z)|S=s].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hIv8926xN6-A",
   "metadata": {
    "id": "hIv8926xN6-A"
   },
   "outputs": [],
   "source": [
    "from causaldm.learners.Causal_Effect_Learning.Single_Stage.DRlearner import DRlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_F-3H7NFBHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1675479895278,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "i_F-3H7NFBHZ",
    "outputId": "3383f83c-5015-4dd5-a756-529361a17346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate with DR-learner\n",
      "fold 1, testing r2 baselearner: 0.980, pslearner: 0.943\n",
      "fold 2, testing r2 baselearner: 0.978, pslearner: 0.947\n",
      "fold 3, testing r2 baselearner: 0.975, pslearner: 0.942\n",
      "fold 4, testing r2 baselearner: 0.978, pslearner: 0.946\n",
      "fold 5, testing r2 baselearner: 0.978, pslearner: 0.940\n"
     ]
    }
   ],
   "source": [
    "# DR-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model = LogisticRegression()\n",
    "Rlearner_model = LGBMRegressor(max_depth=2)\n",
    "\n",
    "HTE_DR_learner = DRlearner(data_behavior, outcome, treatment, controls, y_model, ps_model)\n",
    "HTE_DR_learner = HTE_DR_learner.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0-u2xNvpFBHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1675479896943,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "0-u2xNvpFBHZ",
    "outputId": "2bda9075-e9e8-48e2-dd06-fbab8b7ff0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR-learner:   [-1.2566  0.0408 -0.8131 -0.0906 -0.5665 -0.7341 -0.6459 -1.272 ]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"DR-learner:  \",HTE_DR_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yvb360k8FBHa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1675479898442,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "Yvb360k8FBHa",
    "outputId": "926b5a81-e86d-4d5f-f9da-9f5f0b61f258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of DR-learner is :      0.29436318987432813 , \n",
      " The overall estimation variance of DR-learner is : 4.011818461500106 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_DR_learner = np.sum(HTE_DR_learner-HTE_true)/n\n",
    "Variance_DR_learner = np.sum((HTE_DR_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of DR-learner is :     \", Bias_DR_learner, \", \\n\", \"The overall estimation variance of DR-learner is :\",Variance_DR_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YgKc3F0cR0Y4",
   "metadata": {
    "id": "YgKc3F0cR0Y4"
   },
   "source": [
    "### **6. Lp-R-learner**\n",
    "\n",
    "As an extension of R-learner, Lp-R-learner combined the idea of residual regression with local polynomial adaptation, and leveraged the idea of cross fitting to further relax the conditions needed to obtain the oracle convergence rate. For brevity of content, we will just introduce their main algorithm. For more details about its theory and real data performance please see the paper written by Kennedy [4]. \n",
    "\t\n",
    "Let $(I_{1a}^n, I_{1b}^n,I_{2}^n)$ denote three independent samples of $n$ observations of $Z_i = (S_i, A_i, R_i)$. Let $b:\\mathbb{R}^d\\rightarrow \\mathbb{R}^p$ denote the vector of basis functions consisting of all powers of each covariate, up to order $\\gamma$, and all interactions up to degree $\\gamma$ polynomials. Let $K_{hs}(S)=\\frac{1}{h^d}K\\left(\\frac{S-s}{h}\\right)$ for $k:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ a bounded kernel function with support $[-1,1]^d$, and $h$ is a bandwidth parameter.\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1a}^n$ to construct estimates $\\hat{\\pi}_a$ of the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1b}^n$ to construct estimates $\\hat{\\eta}$ of the regression function $\\eta=\\pi\\mu_1+(1-\\pi)\\mu_0$, and estimtes $\\hat{\\pi}_b$ of the propensity scores $\\pi$.\n",
    "\n",
    "**Step 2**: Localized double-residual regression: \\\\\n",
    "Define $\\hat{\\tau}_r(s)$ as the fitted value from a kernel-weighted least squares regression (in the test sample $I_2^n$) of outcome residual $(R-\\hat{\\eta})$ on basis terms $b$ scaled by the treatment residual $A-\\hat{\\pi}_b$, with weights $\\Big(\\frac{A-\\hat{\\pi}_a}{A-\\hat{\\pi}_b}\\Big)\\cdot K_{hs}$. Thus $\\hat{\\tau}_r(s)=b(0)^T\\hat{\\theta}$ for\n",
    "\\begin{equation}\n",
    "\t\t\\hat{\\theta}=\\arg\\min_{\\theta\\in\\mathbb{R}^p}\\mathbb{P}_n\\left(K_{hs}(S)\\Big\\{ \\frac{A-\\hat{\\pi}_a(S)}{A-\\hat{\\pi}_b(S)}\\Big\\} \\left[  \\big\\{R-\\hat{\\eta}(S)\\big\\}-\\theta^Tb(S-s_0)\\big\\{A-\\hat{\\pi}_b(S)\\big\\} \\right] \\right).\n",
    "\\end{equation}\n",
    "**Step 3**: Cross-fitting(optional): \\\\\n",
    "Repeat Step 1–2 twice, first using $(I^n_{1b} , I_2^n)$ for nuisance training and $I_{1a}^n$ as the test samplem and then using $(I^n_{1a} , I_2^n)$ for training and $I_{1b}^n$ as the test sample. Use the average of the resulting three estimators of $\\tau$ as the final estimator $\\hat{\\tau}_r$.\n",
    "\n",
    "In the theory section, Kennedy proved that Lp-R-learner, compared with traditional DR learner, can achieve the oracle convergence rate under milder conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DPyK1J2AQvXU",
   "metadata": {
    "id": "DPyK1J2AQvXU"
   },
   "outputs": [],
   "source": [
    "from causaldm.learners.Causal_Effect_Learning.Single_Stage.LpRlearner import LpRlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r703l-q0GeyE",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 177694,
     "status": "ok",
     "timestamp": 1675480091797,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "r703l-q0GeyE",
    "outputId": "b04e7218-50f9-40d0-8105-eb5fc0193eab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate with Lp-R-learner\n"
     ]
    }
   ],
   "source": [
    "# Lp-R-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model_a = LogisticRegression()\n",
    "ps_model_b = LogisticRegression()\n",
    "s = 1\n",
    "LpRlearner_model = LinearRegression()\n",
    "\n",
    "HTE_Lp_R_learner = LpRlearner(data_behavior, outcome, treatment, controls, y_model, ps_model_a, ps_model_b, s, LpRlearner_model, degree = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oZEfhLMIGeyg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39,
     "status": "ok",
     "timestamp": 1675480091798,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "oZEfhLMIGeyg",
    "outputId": "67425f5c-8264-4b5c-a07c-e013583f40e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lp_R-learner:   [-1.0353  0.2368 -1.0444  0.0884 -0.6845 -0.6876 -2.6223 -1.85  ]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lp_R-learner:  \",HTE_Lp_R_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nSGfmmgEGeyg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1675480091800,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "nSGfmmgEGeyg",
    "outputId": "e1d050c9-49ae-4359-f673-c17f52f17100"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of Lp_R-learner is :      -0.2909913487561472 , \n",
      " The overall estimation variance of Lp_R-learner is : 1.1822936738050482 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_Lp_R_learner = np.sum(HTE_Lp_R_learner-HTE_true)/n\n",
    "Variance_Lp_R_learner = np.sum((HTE_Lp_R_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of Lp_R-learner is :     \", Bias_Lp_R_learner, \", \\n\", \"The overall estimation variance of Lp_R-learner is :\",Variance_Lp_R_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bla210-HHT-E",
   "metadata": {
    "id": "bla210-HHT-E"
   },
   "source": [
    "**Conclusion**: It will cost more time to use Lp-R-learner than other approaches. However, the overall estimation variance of Lp-R-learner is incredibly smaller than other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {
    "id": "1098b550"
   },
   "source": [
    "## References\n",
    "\n",
    "2. Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299–319, 2021.\n",
    "\n",
    "3. Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econometric Society, pages 931–954, 1988.\n",
    "\n",
    "4. Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020\n",
    "\n",
    "5. M. J. van der Laan. Statistical inference for variable importance. The International Journal of Biostatistics, 2(1), 2006.\n",
    "\n",
    "6. S. Lee, R. Okui, and Y.-J. Whang. Doubly robust uniform confidence band for the conditional average treatment effect function. Journal of Applied Econometrics, 32(7):1207–1225, 2017.\n",
    "\n",
    "7. D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036, 2019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1098b550"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.11.3"
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}