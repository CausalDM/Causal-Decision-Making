{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "Z_pBCOWOruCh",
   "metadata": {
    "id": "Z_pBCOWOruCh"
   },
   "source": [
    "## **Other Approaches**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SgWh47pKR1XR",
   "metadata": {
    "id": "SgWh47pKR1XR"
   },
   "source": [
    "### **7. Generalized Random Forest**\n",
    "\n",
    "Developed by Susan Athey, Julie Tibshirani and Stefan Wager, Generalized Random Forest [8] aims to give the solution to a set of local moment equations:\n",
    "\\begin{equation}\n",
    "  \\mathbb{E}\\big[\\psi_{\\tau(s),\\nu(s)}(O_i)\\big| S_i=s\\big]=0,\n",
    "\\end{equation}\n",
    "where $\\tau(s)$ is the parameter we care about and $\\nu(s)$ is an optional nuisance parameter. In the problem of Heterogeneous Treatment Effect Evaluation, our parameter of interest $\\tau(s)=\\xi\\cdot \\beta(s)$ is identified by \n",
    "\\begin{equation}\n",
    "  \\psi_{\\beta(s),\\nu(s)}(R_i,A_i)=(R_i-\\beta(s)\\cdot A_i-c(s))(1 \\quad A_i^T)^T.\n",
    "\\end{equation}\n",
    "The induced estimator $\\hat{\\tau}(s)$ for $\\tau(s)$ can thus be solved by\n",
    "\\begin{equation}\n",
    "  \\hat{\\tau}(s)=\\xi^T\\left(\\sum_{i=1}^n \\alpha_i(s)\\big(A_i-\\bar{A}_\\alpha\\big)^{\\otimes 2}\\right)^{-1}\\sum_{i=1}^n \\alpha_i(s)\\big(A_i-\\bar{A}_\\alpha\\big)\\big(R_i-\\bar{R}_\\alpha\\big),\n",
    "\\end{equation}\n",
    "where $\\bar{A}_\\alpha=\\sum \\alpha_i(s)A_i$ and $\\bar{R}_\\alpha=\\sum \\alpha_i(s)R_i$, and we write $v^{\\otimes 2}=vv^T$.\n",
    "\n",
    "Notice that this formula is just a weighted version of R-learner introduced above. However, instead of using ordinary kernel weighting functions that are prone to a strong curse of dimensionality, GRF uses an adaptive weighting function $\\alpha_i(s)$ derived from a forest designed to express heterogeneity in the specified quantity of interest. \n",
    "    \n",
    "To be more specific, in order to obtain $\\alpha_i(s)$, GRF first grows a set of $B$ trees indexed by $1,\\dots,B$. Then for each such tree, define $L_b(s)$ as the set of training samples falling in the same ``leaf\" as x. The weights $\\alpha_i(s)$ then capture the frequency with which the $i$-th training example falls into the same leaf as $s$:\n",
    "\\begin{equation}\n",
    "  \\alpha_{bi}(s)=\\frac{\\boldsymbol{1}\\big(\\{S_i\\in L_b(s)\\}\\big)}{\\big|L_b(s)\\big|},\\quad \\alpha_i(s)=\\frac{1}{B}\\sum_{b=1}^B \\alpha_{bi}(s).\n",
    "\\end{equation}\n",
    "\n",
    "To sum up, GRF aims to leverage the splitting result of a series of trees to decide the ``localized” weight for HTE estimation at each point $x_0$. Compared with kernel functions, we may expect tree-based weights to be more flexible and better performed in real settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRpP5k9MBtzO",
   "metadata": {
    "id": "eRpP5k9MBtzO"
   },
   "outputs": [],
   "source": [
    "# import related packages\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from causaldm._util_causaldm import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "lovM_twTxuOj",
   "metadata": {
    "id": "lovM_twTxuOj"
   },
   "outputs": [],
   "source": [
    "n = 10**3  # sample size in observed data\n",
    "n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n",
    "seed=223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "AnRQO0viX3D1",
   "metadata": {
    "id": "AnRQO0viX3D1"
   },
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'data' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_50841/2180662469.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# The true expected heterogeneous treatment effect\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mHTE_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_data_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mget_data_simulation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'R'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/_util_causaldm.py\u001b[0m in \u001b[0;36mget_data_simulation\u001b[0;34m(n0, seed, policy)\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'data' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# Get data\n",
    "data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n",
    "#data_target = get_data_simulation(n0, seed, policy=\"target\")\n",
    "\n",
    "# The true expected heterogeneous treatment effect\n",
    "HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cmjcDa7juPbB",
   "metadata": {
    "id": "cmjcDa7juPbB"
   },
   "source": [
    "The generalized random forest (GRF) approach has been implemented in package *grf* for R and C++, and *econml* in python. Here we implement the package of *econml* for a simple illustration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fZT7U8YnNLGo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4180,
     "status": "ok",
     "timestamp": 1675480054329,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "fZT7U8YnNLGo",
    "outputId": "b3d4a3ec-3680-4f86-906e-deeafd162109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting econml\n",
      "  Downloading econml-0.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m96.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scikit-learn<1.2,>0.22.0 in /usr/local/lib/python3.8/dist-packages (from econml) (1.0.2)\n",
      "Collecting sparse\n",
      "  Downloading sparse-0.13.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: statsmodels>=0.10 in /usr/local/lib/python3.8/dist-packages (from econml) (0.12.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (from econml) (2.2.3)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.8/dist-packages (from econml) (1.2.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from econml) (1.21.6)\n",
      "Collecting shap<0.41.0,>=0.38.1\n",
      "  Downloading shap-0.40.0-cp38-cp38-manylinux2010_x86_64.whl (571 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 KB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from econml) (1.3.5)\n",
      "Requirement already satisfied: scipy>1.4.0 in /usr/local/lib/python3.8/dist-packages (from econml) (1.7.3)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>0.22.0->econml) (3.1.0)\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (23.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (0.56.4)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (4.64.1)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (2.2.1)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.10->econml) (0.5.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->econml) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->econml) (2022.7.1)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->shap<0.41.0,>=0.38.1->econml) (6.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap<0.41.0,>=0.38.1->econml) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->shap<0.41.0,>=0.38.1->econml) (57.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels>=0.10->econml) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->shap<0.41.0,>=0.38.1->econml) (3.12.0)\n",
      "Installing collected packages: slicer, sparse, shap, econml\n",
      "Successfully installed econml-0.14.0 shap-0.40.0 slicer-0.0.7 sparse-0.13.0\n"
     ]
    }
   ],
   "source": [
    "# import the package for Causal Random Forest\n",
    "! pip install econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gk0nYH559XIL",
   "metadata": {
    "id": "gk0nYH559XIL"
   },
   "outputs": [],
   "source": [
    "# A demo code of Causal Random Forest\n",
    "from econml.grf import CausalForest, CausalIVForest, RegressionForest\n",
    "from econml.dml import CausalForestDML\n",
    "est = CausalForest(criterion='het', n_estimators=400, min_samples_leaf=5, max_depth=None,\n",
    "                    min_var_fraction_leaf=None, min_var_leaf_on_val=True,\n",
    "                    min_impurity_decrease = 0.0, max_samples=0.45, min_balancedness_tol=.45,\n",
    "                    warm_start=False, inference=True, fit_intercept=True, subforest_size=4,\n",
    "                    honest=True, verbose=0, n_jobs=-1, random_state=1235)\n",
    "\n",
    "\n",
    "est.fit(data_behavior.iloc[:,0:2], data_behavior['A'], data_behavior['R'])\n",
    "\n",
    "HTE_GRF = est.predict(data_behavior.iloc[:,0:2], interval=False, alpha=0.05)\n",
    "HTE_GRF = HTE_GRF.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cNcRW6yBOQJy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 27,
     "status": "ok",
     "timestamp": 1675480056535,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "cNcRW6yBOQJy",
    "outputId": "8ec90bcb-06a2-41f2-fda2-c9d5b0c110f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Random Forest:   [-1.2344  1.612  -0.7801  0.6886 -0.6297  0.2293  0.4417 -0.819 ]\n",
      "true value:                  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generalized Random Forest:  \",HTE_GRF[0:8])\n",
    "print(\"true value:                 \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_djs10pxOQJ1",
   "metadata": {
    "id": "_djs10pxOQJ1"
   },
   "source": [
    "Causal Forest performs just okay in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0p9XqW8DOQJ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22,
     "status": "ok",
     "timestamp": 1675480056538,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "0p9XqW8DOQJ2",
    "outputId": "f0409488-3aab-4d84-84d0-40587cab54c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of Generalized Random Forest is :      0.706857912147952 , \n",
      " The overall estimation variance of Generalized Random Forest is : 5.198946462195667 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_GRF = np.sum(HTE_GRF-HTE_true)/n\n",
    "Variance_GRF = np.sum((HTE_GRF-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of Generalized Random Forest is :     \", Bias_GRF, \", \\n\", \"The overall estimation variance of Generalized Random Forest is :\",Variance_GRF ,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XUdcqLkabYny",
   "metadata": {
    "id": "XUdcqLkabYny"
   },
   "source": [
    "### **8. Dragon Net**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LAtbTkgLbcZU",
   "metadata": {
    "id": "LAtbTkgLbcZU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {
    "id": "1098b550"
   },
   "source": [
    "## References\n",
    "\n",
    "8. Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. The Annals of Statistics, 47(2):1148–1178, 2019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1098b550"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}