{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112864aa",
   "metadata": {
    "id": "112864aa"
   },
   "source": [
    "# ATE Estimation \n",
    "Average treatment effect (ATE), is a measure used to compare treatments (or interventions) in randomized experiments, evaluation of policy interventions, and medical trials. As we've introduced in the preliminary section, it aims to estimate the difference of some reward function in between treatment and control. Under the potential outcome's framework, or the notation of do-operator in RL-based literature, our main purpose lies in estimating and inferring on \n",
    "\\begin{equation}\n",
    "\\text{ATE} = E[R^*(1) - R^*(0)] = E[ R|do(A=1)] -  E[ R|do(A=0)].\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Assumptions\n",
    "Under three common assumptions in causal inference, we can identify, or consistently estimate the potential outcome $\\{R^*(0),R^*(1)\\}$ from the observed data. These assumptions are 1) Consistency, 2) No unmeasured confounders (NUC), and 3) Positivity assumption.\n",
    "\n",
    "- **SUTVA (Stable Unit Treatment Value Assumption, or Consistency Assumption)** \n",
    "\\begin{align}\n",
    "R_i = R_i^*(1) A_i + R_i^*(0) (1-A_i), i = 1, \\cdots, n.\n",
    "\\end{align}\n",
    "That is, the actual response that is observed for the $i$-th individual in our sample, $R_i$, who received treatment $A_i$, is the same as the potential outcome for that assigned treatment regardless of the experimental conditions used to assign treatment. This assumption also indicates that there is no interference between subjects in the population, that is, the observed response for any individual is not affected by the responses of other individuals.\n",
    "\n",
    "- **NUC (No Unmeasured Confounders Assumption, or Strong Ignorability)**\n",
    "\\begin{align}\n",
    "\\{R^*(0), R^*(1)\\} \\perp\\!\\!\\!\\perp A|S\n",
    "\\end{align}\n",
    "We cannot use the data in an observational study to either confirm or refute this assumption. However, if we believe that $X$ contained all the relevant information about the treatment assignment, then it will be reasonable to make the above assumption. \n",
    "- **Positivity Assumption**\n",
    "\\begin{align}\n",
    "0 < P(A=1|S=s) < 1\n",
    "\\end{align}\n",
    "This assumption ensures that for any given individual in treatment group $1$, we are able to find a similar individual in treatment group $0$, and vice versa.\n",
    "\n",
    "We remark that these three assumptions are commonly imposed in causal inference and individualized treatment regimes literature [1]. Moreover, NUC and Positivity assumptions are automatically satisfied in randomized studies where the behavior policy is usually a strictly positive function independent of $s$. \n",
    "\n",
    "\n",
    "## Identification\n",
    "Based on the three assumptions above, we can re-write ATE as a function of the observed data. The details are shown below:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "E[Y^*(1)] &= E_x [E\\{Y^*(1)|X\\}] \\\\\n",
    "&= E_x[E\\{Y^*(1)|T=1, X\\}] \\quad \\text{(NUC)}\\\\\n",
    "&= E_x[E\\{Y|T=1, X\\}] \\quad\\quad \\text{(SUTVA)}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Similarly, we can show that $E[Y^*(0)] = E_x[E\\{Y|T=0, X\\}].$ Hence, the average causal treatment effect can be written as\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\text{ATE} = E_x[E\\{Y|T=1, X\\}] - E_x[E\\{Y|T=0, X\\}],\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "where the RHS we get rid of the potential outcome's notations and thus can be identified from purely the observed data. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sGYqEx8AuLYs",
   "metadata": {
    "id": "sGYqEx8AuLYs"
   },
   "source": [
    "## Estimation\n",
    "In this section, we will introduce three categories of estimators that have been widely used in ATE estimation: direct method (DM), importance sampling estimator (IS), and doubly robust estimator (DR).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3559b08",
   "metadata": {
    "id": "e3559b08"
   },
   "source": [
    "### 1. Direct Method \n",
    "\n",
    "The first approach to estimate the average treatment effect is through direct model fitting based on the identification result, which is also known as the outcome regression model. \n",
    "\n",
    "Specifically, we can posit a model for $E[R|A, S] = \\mu(S, A;\\gamma)$, and estimate the parameter $\\gamma$ via least square or any other machine learning-based tools for model fitting. Then, The DM estimator of ATE is given by\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\widehat{\\text{ATE}}_{\\text{DM}} = \\frac{1}{n}\\sum_{i=1}^n \\{\\mu(S_i, 1;\\hat{\\gamma}) - \\mu(S_i, 0; \\hat{\\gamma})\\}\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "This specific estimation procedure, as will be detailed in later sections, is the same as implementing S-learner and averaging over all heterogeneous treatment effects to obtain the ATE. If we fit the regression models separately for $A=1$ and $A=0$, the resulting HTE estimation method becomes T-learner. We will elaborate them in the next section.\n",
    "\n",
    "Next, we implement direct method on the MovieLens dataset to provide users with a detailed ATE estimation procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRpP5k9MBtzO",
   "metadata": {
    "id": "eRpP5k9MBtzO"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>movie_id</th>\n",
       "      <th>rating</th>\n",
       "      <th>age</th>\n",
       "      <th>Drama</th>\n",
       "      <th>gender_M</th>\n",
       "      <th>occupation_academic/educator</th>\n",
       "      <th>occupation_college/grad student</th>\n",
       "      <th>occupation_executive/managerial</th>\n",
       "      <th>occupation_other</th>\n",
       "      <th>occupation_technician/engineer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1193.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>48.0</td>\n",
       "      <td>919.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>48.0</td>\n",
       "      <td>527.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48.0</td>\n",
       "      <td>1721.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>48.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65637</th>\n",
       "      <td>5878.0</td>\n",
       "      <td>3300.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65638</th>\n",
       "      <td>5878.0</td>\n",
       "      <td>1391.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65639</th>\n",
       "      <td>5878.0</td>\n",
       "      <td>185.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65640</th>\n",
       "      <td>5878.0</td>\n",
       "      <td>2232.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65641</th>\n",
       "      <td>5878.0</td>\n",
       "      <td>426.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>65642 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id  movie_id  rating   age  Drama  gender_M  \\\n",
       "0         48.0    1193.0     4.0  25.0    1.0       1.0   \n",
       "1         48.0     919.0     4.0  25.0    1.0       1.0   \n",
       "2         48.0     527.0     5.0  25.0    1.0       1.0   \n",
       "3         48.0    1721.0     4.0  25.0    1.0       1.0   \n",
       "4         48.0     150.0     4.0  25.0    1.0       1.0   \n",
       "...        ...       ...     ...   ...    ...       ...   \n",
       "65637   5878.0    3300.0     2.0  25.0    0.0       0.0   \n",
       "65638   5878.0    1391.0     1.0  25.0    0.0       0.0   \n",
       "65639   5878.0     185.0     4.0  25.0    0.0       0.0   \n",
       "65640   5878.0    2232.0     1.0  25.0    0.0       0.0   \n",
       "65641   5878.0     426.0     3.0  25.0    0.0       0.0   \n",
       "\n",
       "       occupation_academic/educator  occupation_college/grad student  \\\n",
       "0                               0.0                              1.0   \n",
       "1                               0.0                              1.0   \n",
       "2                               0.0                              1.0   \n",
       "3                               0.0                              1.0   \n",
       "4                               0.0                              1.0   \n",
       "...                             ...                              ...   \n",
       "65637                           0.0                              0.0   \n",
       "65638                           0.0                              0.0   \n",
       "65639                           0.0                              0.0   \n",
       "65640                           0.0                              0.0   \n",
       "65641                           0.0                              0.0   \n",
       "\n",
       "       occupation_executive/managerial  occupation_other  \\\n",
       "0                                  0.0               0.0   \n",
       "1                                  0.0               0.0   \n",
       "2                                  0.0               0.0   \n",
       "3                                  0.0               0.0   \n",
       "4                                  0.0               0.0   \n",
       "...                                ...               ...   \n",
       "65637                              0.0               1.0   \n",
       "65638                              0.0               1.0   \n",
       "65639                              0.0               1.0   \n",
       "65640                              0.0               1.0   \n",
       "65641                              0.0               1.0   \n",
       "\n",
       "       occupation_technician/engineer  \n",
       "0                                 0.0  \n",
       "1                                 0.0  \n",
       "2                                 0.0  \n",
       "3                                 0.0  \n",
       "4                                 0.0  \n",
       "...                               ...  \n",
       "65637                             0.0  \n",
       "65638                             0.0  \n",
       "65639                             0.0  \n",
       "65640                             0.0  \n",
       "65641                             0.0  \n",
       "\n",
       "[65642 rows x 11 columns]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import related packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt;\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Get the MovieLens data\n",
    "MovieLens_CEL = pd.read_csv(\"/Users/alinaxu/Documents/CDM/CausalDM/causaldm/data/MovieLens_CEL.csv\")\n",
    "MovieLens_CEL.pop(MovieLens_CEL.columns[0])\n",
    "MovieLens_CEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "J__3Ozs7Uxxs",
   "metadata": {
    "id": "J__3Ozs7Uxxs"
   },
   "outputs": [],
   "source": [
    "n = len(MovieLens_CEL)\n",
    "userinfo_index = np.array([3,5,6,7,8,9,10])\n",
    "SandA = MovieLens_CEL.iloc[:, np.array([3,4,5,6,7,8,9,10])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "h5G8dAwM-PGO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 837,
     "status": "ok",
     "timestamp": 1676750134359,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "h5G8dAwM-PGO",
    "outputId": "affb7b39-83cd-4d7e-8572-02cbce6be447"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingRegressor(max_depth=5)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S-learner\n",
    "np.random.seed(0)\n",
    "S_learner = GradientBoostingRegressor(max_depth=5)\n",
    "S_learner.fit(SandA, MovieLens_CEL['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "Vqsb5wLTaR0q",
   "metadata": {
    "id": "Vqsb5wLTaR0q"
   },
   "outputs": [],
   "source": [
    "SandA_all1 = SandA.copy()\n",
    "SandA_all0 = SandA.copy()\n",
    "SandA_all1.iloc[:,4]=np.ones(n)\n",
    "SandA_all0.iloc[:,4]=np.zeros(n)\n",
    "\n",
    "ATE_DM = np.sum(S_learner.predict(SandA_all1) - S_learner.predict(SandA_all0))/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "X1VmlNjstdsN",
   "metadata": {
    "id": "X1VmlNjstdsN"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.14529621186817238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATE_DM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b3fc93",
   "metadata": {},
   "source": [
    " That is, people are more inclined to give higher ratings to drama than science fictions. The expected rating difference given by direct method is 0.145."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dh4NnOWvtDS",
   "metadata": {
    "id": "3dh4NnOWvtDS"
   },
   "source": [
    "### 2. Importance Sampling Estimator\n",
    "The second type of estimators is called importance sampling (IS) estimator, or inverse propensity score (IPW) and augmented inverse propensity score (AIPW) in causal inference literature.\n",
    "\n",
    "Before we proceed, let's define the propensity score as below:\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\pi(S) = P(A=1|S).\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "In the case where there are only two treatments, it refers to the propensity of getting one of the treatments as a function of the covariates. One of the attractive features of the propensity score is that given the `SUTVA` and `strong ignorability` assumptions, we have\n",
    "\\begin{align}\n",
    "\\{R^*(1), R^*(0)\\} \\perp\\!\\!\\!\\perp A|\\pi(S)\n",
    "\\end{align}\n",
    "In some cases when it is difficult to fit an outcome regression model for $E[R|A, S]$ and $S$ may be high dimensional, we can alternatively fit models for $E[R|A, \\pi(S)]$ based on the result above.\n",
    "\n",
    "The motivation of IPW is as follows. Since the propensity (probability) of receiving treatment $1$ is $\\pi(S)$ for an individual with baseline covariates $S$, then every individual with covariates $X_i$ that was observed to receive treatment 1 in our sample is weighted by $1/\\pi(S_i)$ so that their response not only represent themselves but also other individuals who did not receive treatment 1. More formally, we will show that $\\frac{AR}{\\pi(S)}$ is an unbiased estimator of $E[R^*(1)]$. This follows that, given the `positivity` assumption,\n",
    "\\begin{equation} \n",
    "\\begin{aligned}\n",
    "E\\left[\\frac{AR}{\\pi(S)}\\right] = E\\left[\\frac{AR^*(1)}{\\pi(S)}\\right] = E\\left[E\\bigg\\{\\frac{AR^*(1)}{\\pi(S)}\\Big|S\\bigg\\}\\right] = E\\left[E\\bigg\\{\\frac{A}{\\pi(S)}\\Big| S\\bigg\\}R^*(1)\\right] = E[R^*(1)].\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "\n",
    "Similarly, by flipping the role of treatment ($A=1$) and control ($A=0$), we have \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "E[R^*(0)] = E\\left[\\frac{(1-A)R}{1-\\pi(S)} \\right].\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "Consequently, the IS (or IPW) estimator for the estimation of ATE is given by\n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\widehat{\\text{ATE}}_{\\text{IS}} =\\frac{1}{n}\\sum_{i=1}^n \\left\\{\\frac{A_iR_i}{\\hat\\pi(S_i)}  - \\frac{(1-A_i)R_i}{1-\\hat\\pi(S_i)}  \\right\\}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "PKKkPwH6paU_",
   "metadata": {
    "id": "PKKkPwH6paU_"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# propensity score model fitting\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "ps_model = LogisticRegression()\n",
    "ps_model.fit(MovieLens_CEL.iloc[:, userinfo_index],  MovieLens_CEL['Drama'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5591b6e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pi_S = ps_model.predict_proba(MovieLens_CEL.iloc[:, userinfo_index])\n",
    "ATE_IS = np.sum((MovieLens_CEL['Drama']/pi_S[:,1] - (1-MovieLens_CEL['Drama'])/pi_S[:,0])*MovieLens_CEL['rating'])/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "233a5a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3556423779577757"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ATE_IS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099aa2f9",
   "metadata": {},
   "source": [
    "People watching `Drama` is expected to give ratings 0.356 point higher than watching `Sci-Fi`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Js_WHjzbvpfG",
   "metadata": {
    "id": "Js_WHjzbvpfG"
   },
   "source": [
    "### 3. Doubly Robust Estimator\n",
    "The third type of estimator is the doubly robust estimator. Basically, DR combines DM and IS estimators, which is more robust to model misspecifications. \n",
    "\\begin{equation}\n",
    "\\begin{aligned}\n",
    "\\widehat{\\text{ATE}}_{\\text{DR}} = \\frac{1}{n}\\sum_{i=1}^n \\left\\{\\mu(S_i,1;\\hat{\\gamma})- \\mu(S_i,0;\\hat{\\gamma})+\\frac{A_i(R_i - \\mu(S_i,1;\\hat{\\gamma}))}{\\hat{\\pi}(S_i)}  - \\frac{(1-A_i)(R_i-\\mu(S_i,0;\\hat{\\gamma}))}{1-\\hat{\\pi}(S_i)} \\right\\}.\n",
    "\\end{aligned}\n",
    "\\end{equation}\n",
    "To be more specific, the first two terms on the RHS forms a direct estimator and the last two terms serve as an augmentation to correct the bias rising from outcome regression models. When either the outcome regression models or the propensity scores are correctly specified, $\\widehat{\\text{ATE}}_{\\text{DR}}$ can be proved to be consistent.\n",
    "\n",
    "Under some mild entropy conditions or sample splitting, DR estimator is also a semi-parametrically efficient estimator when the convergence rate of both $\\hat{\\mu}$ and $\\hat{\\pi}$ are at least $o(n^{-1/4})$. Details can be found in Chernozhukov et al. 2018 [2]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b038a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.15559049518872164"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(MovieLens_CEL['Drama']*(MovieLens_CEL['rating']-S_learner.predict(SandA_all1))/pi_S[:,1] - (1-MovieLens_CEL['Drama'])*(MovieLens_CEL['rating']-S_learner.predict(SandA_all0))/pi_S[:,0])/n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1sAlhG0ppa0p",
   "metadata": {
    "id": "1sAlhG0ppa0p"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.010294283320549269"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# combine the DM estimator and IS estimator\n",
    "ATE_DR = ATE_DM + np.sum(MovieLens_CEL['Drama']*(MovieLens_CEL['rating']-S_learner.predict(SandA_all1))/pi_S[:,1] - (1-MovieLens_CEL['Drama'])*(MovieLens_CEL['rating']-S_learner.predict(SandA_all0))/pi_S[:,0])/n\n",
    "ATE_DR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e3873",
   "metadata": {},
   "source": [
    "After correcting the bias by doubly robust procedure, the expected rating of `Drama` is even slightly lower than that of `Sci-Fi`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {
    "id": "1098b550"
   },
   "source": [
    "## References\n",
    "1. Zhang, B., A. A. Tsiatis, E. B. Laber, and M. Davidian (2013). Robust estimation of optimal dynamic treatment regimes for sequential treatment decisions. Biometrika 100 (3), 681–694.\n",
    "2. Chernozhukov, V., D. Chetverikov, M. Demirer, E. Duflo, C. Hansen, W. Newey, and J. Robins (2018). Double/debiased machine learning for treatment and structural parameters."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}