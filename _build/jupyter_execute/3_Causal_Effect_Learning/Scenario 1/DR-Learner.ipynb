{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "J2z2JRumRzdo",
   "metadata": {
    "id": "J2z2JRumRzdo"
   },
   "source": [
    "### **5. DR-learner**\n",
    "\n",
    "DR-learner is a two-stage doubly robust estimator for HTE estimation. Before Kennedy et al. 2020 [4], there are several related approaches trying to extend the doubly robust procedure to HTE estimation, such as [5, 6, 7]. Compared with the above three estimators, DR-learner is proved to be oracle efficient under some mild assumptions detailed in Theorem 2 of [4].\n",
    "\n",
    "The basic steps of DR-learner is given below:\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1}^n$ to construct estimates $\\hat{\\pi}$ for the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1}^n$ to construct estimates $\\hat\\mu_a(s)$ for $\\mu_a(s):=\\mathbb{E}[R|S=s,A=a]$;\n",
    "\n",
    "**Step 2**: Pseudo-outcome regression: \\\\\n",
    "Define $\\widehat{\\phi}(Z)$ as the pseudo-outcome where \n",
    "\\begin{equation}\n",
    "\\widehat{\\phi}(Z)=\\frac{A-\\hat{\\pi}(S)}{\\hat{\\pi}(S)\\{1-\\hat{\\pi}(S)\\}}\\Big\\{R-\\hat{\\mu}_A(S)\\Big\\}+\\hat{\\mu}_1(S)-\\hat{\\mu}_0(S),\n",
    "\\end{equation}\n",
    "and regress it on covariates $S$ in the test sample $I_2^n$, yielding \n",
    "\\begin{equation}\n",
    "\\widehat{\\tau}_{\\text{DR-learner}}(s)=\\widehat{\\mathbb{E}}_n[\\widehat{\\phi}(Z)|S=s].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "VBreyjCSYdHy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4287,
     "status": "ok",
     "timestamp": 1676134075357,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "VBreyjCSYdHy",
    "outputId": "9e32343f-1b89-4ab6-eca3-8a02680d80af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-uplift in /opt/anaconda3/lib/python3.8/site-packages (0.5.1)\r\n",
      "Requirement already satisfied: matplotlib in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (3.2.2)\r\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (2.24.0)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (4.47.0)\r\n",
      "Requirement already satisfied: scikit-learn>=0.21.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (0.23.1)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (1.4.2)\r\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-uplift) (1.22.4)\r\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (1.2.0)\r\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (2.8.1)\r\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (2.4.7)\r\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.8/site-packages (from matplotlib->scikit-uplift) (0.10.0)\r\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (1.25.9)\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: idna<3,>=2.5 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (2.10)\r\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (3.0.4)\r\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.8/site-packages (from requests->scikit-uplift) (2020.6.20)\r\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (2.1.0)\r\n",
      "Requirement already satisfied: joblib>=0.11 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (0.16.0)\r\n",
      "Requirement already satisfied: scipy>=0.19.1 in /opt/anaconda3/lib/python3.8/site-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.5.0)\r\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.8/site-packages (from pandas->scikit-uplift) (2020.1)\r\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.8/site-packages (from python-dateutil>=2.1->matplotlib->scikit-uplift) (1.15.0)\r\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install scikit-uplift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7_g6QQ8wXcb2",
   "metadata": {
    "id": "7_g6QQ8wXcb2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-7dc64393fbb7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import related packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLogisticRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# import related packages\n",
    "from matplotlib import pyplot as plt\n",
    "from lightgbm import LGBMRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners.Causal_Effect_Learning.Single_Stage.DRlearner import DRlearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vq-ZNkGjXcb7",
   "metadata": {
    "id": "vq-ZNkGjXcb7"
   },
   "outputs": [],
   "source": [
    "n = 10**3  # sample size in observed data\n",
    "n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n",
    "seed=223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "E4tdbzu6Xcb8",
   "metadata": {
    "id": "E4tdbzu6Xcb8"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n",
    "#data_target = get_data_simulation(n0, seed, policy=\"target\")\n",
    "\n",
    "# The true expected heterogeneous treatment effect\n",
    "HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_F-3H7NFBHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 540,
     "status": "ok",
     "timestamp": 1675479895278,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "i_F-3H7NFBHZ",
    "outputId": "3383f83c-5015-4dd5-a756-529361a17346"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate with DR-learner\n",
      "fold 1, testing r2 baselearner: 0.980, pslearner: 0.943\n",
      "fold 2, testing r2 baselearner: 0.978, pslearner: 0.947\n",
      "fold 3, testing r2 baselearner: 0.975, pslearner: 0.942\n",
      "fold 4, testing r2 baselearner: 0.978, pslearner: 0.946\n",
      "fold 5, testing r2 baselearner: 0.978, pslearner: 0.940\n"
     ]
    }
   ],
   "source": [
    "# DR-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model = LogisticRegression()\n",
    "Rlearner_model = LGBMRegressor(max_depth=2)\n",
    "\n",
    "HTE_DR_learner = DRlearner(data_behavior, outcome, treatment, controls, y_model, ps_model)\n",
    "HTE_DR_learner = HTE_DR_learner.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0-u2xNvpFBHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1675479896943,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "0-u2xNvpFBHZ",
    "outputId": "2bda9075-e9e8-48e2-dd06-fbab8b7ff0f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR-learner:   [-1.2566  0.0408 -0.8131 -0.0906 -0.5665 -0.7341 -0.6459 -1.272 ]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"DR-learner:  \",HTE_DR_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yvb360k8FBHa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 163,
     "status": "ok",
     "timestamp": 1675479898442,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "Yvb360k8FBHa",
    "outputId": "926b5a81-e86d-4d5f-f9da-9f5f0b61f258"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of DR-learner is :      0.29436318987432813 , \n",
      " The overall estimation variance of DR-learner is : 4.011818461500106 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_DR_learner = np.sum(HTE_DR_learner-HTE_true)/n\n",
    "Variance_DR_learner = np.sum((HTE_DR_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of DR-learner is :     \", Bias_DR_learner, \", \\n\", \"The overall estimation variance of DR-learner is :\",Variance_DR_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {
    "id": "1098b550"
   },
   "source": [
    "## References\n",
    "\n",
    "2. Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299–319, 2021.\n",
    "\n",
    "3. Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econometric Society, pages 931–954, 1988.\n",
    "\n",
    "4. Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020\n",
    "\n",
    "5. M. J. van der Laan. Statistical inference for variable importance. The International Journal of Biostatistics, 2(1), 2006.\n",
    "\n",
    "6. S. Lee, R. Okui, and Y.-J. Whang. Doubly robust uniform confidence band for the conditional average treatment effect function. Journal of Applied Econometrics, 32(7):1207–1225, 2017.\n",
    "\n",
    "7. D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036, 2019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "1098b550"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}