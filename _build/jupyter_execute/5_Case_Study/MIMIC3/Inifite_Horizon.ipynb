{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f3f76509-3b83-49f3-9946-e24b92f6ee8c",
   "metadata": {},
   "source": [
    "# MIMIC III (Infinite Horizon)\n",
    "\n",
    "In this notebook, we conducted analysis on the MIMIC III data with infinite horizon. We first analyzed the mediation effect and then evaluate the policy of interest and calculated the optimal policy. As informed by the causal structure learning, here we consider Glucose and PaO2_FiO2 as confounders/states, IV_Input as the action, SOFA as the mediator. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdfd9ef-9bee-42bc-8649-f2cc20806ce3",
   "metadata": {},
   "source": [
    "## CEL: Mediation Analysis with Infinite Horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a161516-9900-4523-bf04-d6f6edd31c76",
   "metadata": {},
   "source": [
    "We processed the MIMIC III data similarly to literature on reinforcement learning by setting the reward of each stage prior to the final stage to 0, and the reward of the final stage to the observed value of Died within 48H. In this section, we analyze the average treatment effect (ATE) of a target policy that provides IV input all of the time compared to a control policy that provides no IV input at all. Using the multiply-robust estimator proposed in [1], we decomposed the ATE into four components, including immediate dierct effect (IDE), immediate mediator effect (IME), delayed direct effect (DDE), and delayed mediator effect (DME), and estimated each of the effect component. The estimation results are summarized in the table below.\n",
    "\n",
    "| IDE           | IME | DDE           | DME           | ATE           |\n",
    "|---------------|-----|---------------|---------------|---------------|\n",
    "| -.1137(.0273) | -.0187(.0067)   | -.0247(.0153) | .0182(.0119) | -.1390(.0297) |\n",
    "\n",
    "Specifically, the ATE of the target policy is significantly negative, with an effect size of .1390. Diving deep, we find that the DME and DDE are insignificant, whereas the IDE and IME are all statistically significant. Further, taking the effect size into account, we can conclude that the majority of the average treatment effect is directly due to the actions derived from the target treatment policy, while the part of the effect that can be attributed to the mediators is negligible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dda318fb-b71a-49e8-8935-f32d44204983",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] 系统找不到指定的路径。: '/nas/longleaf/home/lge/CausalDM/DTR/MRL'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[1;32m----> 3\u001b[0m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchdir\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/nas/longleaf/home/lge/CausalDM/DTR/MRL\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspecial\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m expit\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] 系统找不到指定的路径。: '/nas/longleaf/home/lge/CausalDM/DTR/MRL'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM/DTR/MRL')\n",
    "import numpy as np\n",
    "from scipy.special import expit\n",
    "from evaluator_Linear import evaluator\n",
    "from probLearner import PMLearner, RewardLearner, PALearner\n",
    "from ratioLearner import  RatioLinearLearner as RatioLearner\n",
    "from qLearner_Linear import Qlearner\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM/DTR/Mediation Analysis')\n",
    "MRL_df = pd.read_csv('mimic3_MRL_df_V2.csv')\n",
    "MRL_df[MRL_df.icustayid==1006]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e35283e0-6646-4f14-ab56-ab7afabc7740",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('mimic3_MRL_data_dict_V2.pickle', 'rb')\n",
    "mimic3_MRL = pickle.load(file)\n",
    "# Control Policy\n",
    "def control_policy(state = None, dim_state=None, action=None, get_a = False):\n",
    "    # fixed policy with fixed action 0\n",
    "    if get_a:\n",
    "        action_value = np.array([0])\n",
    "    else:\n",
    "        state = np.copy(state).reshape(-1,dim_state)\n",
    "        NT = state.shape[0]\n",
    "        if action is None:\n",
    "            action_value = np.array([0]*NT)\n",
    "        else:\n",
    "            action = np.copy(action).flatten()\n",
    "            if len(action) == 1 and NT>1:\n",
    "                action = action * np.ones(NT)\n",
    "            action_value = 1-action\n",
    "    return action_value\n",
    "def target_policy(state, dim_state = 1, action=None):\n",
    "    state = np.copy(state).reshape((-1, dim_state))\n",
    "    NT = state.shape[0]\n",
    "    pa = 1 * np.ones(NT)\n",
    "    if action is None:\n",
    "        if NT == 1:\n",
    "            pa = pa[0]\n",
    "            prob_arr = np.array([1-pa, pa])\n",
    "            action_value = np.random.choice([0, 1], 1, p=prob_arr)\n",
    "        else:\n",
    "            raise ValueError('No random for matrix input')\n",
    "    else:\n",
    "        action = np.copy(action).flatten()\n",
    "        action_value = pa * action + (1-pa) * (1-action)\n",
    "    return action_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4a88c374-41a3-4269-997f-c0e87ce586f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fixed hyper-parameter--no need to modify\n",
    "expectation_MCMC_iter = 50\n",
    "expectation_MCMC_iter_Q3 = expectation_MCMC_iter_Q_diff = 50\n",
    "truncate = 50\n",
    "problearner_parameters = {\"splitter\":[\"best\",\"random\"], \"max_depth\" : range(1,50)},\n",
    "dim_state=2; dim_mediator = 1\n",
    "ratio_ndim = 10\n",
    "d = 2\n",
    "L = 5\n",
    "t_depend_target = False\n",
    "t_dependent_Q = False\n",
    "scaler = 'Identity'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0daebafc-3006-46e5-905d-1f2a3f1fd3c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building 0-th basis spline (total 3 state-mediator dimemsion) which has 3 basis, in total 3 features \n",
      "Building 1-th basis spline (total 3 state-mediator dimemsion) which has 3 basis, in total 6 features \n",
      "Building 2-th basis spline (total 3 state-mediator dimemsion) which has 3 basis, in total 9 features \n"
     ]
    }
   ],
   "source": [
    "est_obj1 = evaluator(mimic3_MRL, Qlearner, PMLearner, RewardLearner, PALearner, RatioLearner,\n",
    "                     problearner_parameters = problearner_parameters,\n",
    "                     ratio_ndim = ratio_ndim, truncate = truncate, l2penalty = 10**(-4),\n",
    "                     t_depend_target = t_depend_target,\n",
    "                     target_policy=target_policy, control_policy = control_policy, \n",
    "                     dim_state = dim_state, dim_mediator = dim_mediator, \n",
    "                     Q_settings = {'scaler': scaler,'product_tensor': False, 'beta': 3/7, \n",
    "                                   'include_intercept': False, 'expectation_MCMC_iter_Q3': expectation_MCMC_iter_Q3, \n",
    "                                   'expectation_MCMC_iter_Q_diff':expectation_MCMC_iter_Q_diff, \n",
    "                                   'penalty': 10**(-4),'d': d, 'min_L': L, \"t_dependent_Q\": t_dependent_Q},\n",
    "                     expectation_MCMC_iter = expectation_MCMC_iter,\n",
    "                     seed = 10)\n",
    "\n",
    "est_obj1.estimate_DE_ME_SE()\n",
    "est_value1 = est_obj1.est_DEMESE\n",
    "se_value1 = est_obj1.se_DEMESE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46df0c8a-2039-4aa8-beb5-d42c9b630459",
   "metadata": {},
   "outputs": [],
   "source": [
    "#The following are the estimations of our interest\n",
    "\n",
    "#1. estimation used the proposed triply robust estimator\n",
    "IDE_MR, IME_MR, DDE_MR, DME_MR = est_value1[:4]\n",
    "\n",
    "ATE = est_value1[16]\n",
    "\n",
    "#6. SE of each estimator\n",
    "se_IDE_MR, se_IME_MR, se_DDE_MR, se_DME_MR = se_value1[:4]\n",
    "\n",
    "se_ATE = se_value1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cc082bf-21c7-4d79-9cc5-1679908f075e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.11369785727090427,\n",
       " -0.018734623459877756,\n",
       " -0.02474802766411448,\n",
       " 0.018187656614314426,\n",
       " -0.13899285178058207)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDE_MR, IME_MR, DDE_MR, DME_MR, ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7c1e4c36-4217-4292-af36-d7ec6d7d3a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.02730353979464337,\n",
       " 0.006745477470886179,\n",
       " 0.015252674867721994,\n",
       " 0.011899325617065859,\n",
       " 0.029707778327035382)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "se_IDE_MR, se_IME_MR, se_DDE_MR, se_DME_MR, se_ATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "42b773d7-4855-4b39-9543-0b590277882e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-4.164216732557529,\n",
       " -2.7773606154252737,\n",
       " -1.6225368913151579,\n",
       " 1.528461124572465,\n",
       " -4.6786686722410495)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "IDE_MR/se_IDE_MR, IME_MR/se_IME_MR, DDE_MR/se_DDE_MR, DME_MR/se_DME_MR, ATE/se_ATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcd03a-13db-4924-8340-3df258236487",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Ge, L., Wang, J., Shi, C., Wu, Z., & Song, R. (2023). A Reinforcement Learning Framework for Dynamic Mediation Analysis. arXiv preprint arXiv:2301.13348."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d544c0-900b-457a-bd9d-5517128a9fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}