{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99b219d1-2f7b-4ca3-8a20-1adeec8fab31",
   "metadata": {},
   "source": [
    "## MIMIC III (3-Stages)\n",
    "\n",
    "In this notebook, we conducted analysis on the MIMIC III data with 3 stages. We first analyzed the mediation effect and then evaluate the policy of interest and calculated the optimal policy. As informed by the causal structure learning, here we consider Glucose and PaO2_FiO2 as confounders/states, IV_Input as the action, SOFA as the mediator. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51a5e576-f6f2-47c0-a96d-3935d0ee6ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>icustayid_1</th>\n",
       "      <th>Glucose_1</th>\n",
       "      <th>PaO2_FiO2_1</th>\n",
       "      <th>IV_Input_1</th>\n",
       "      <th>SOFA_1</th>\n",
       "      <th>Glucose_2</th>\n",
       "      <th>PaO2_FiO2_2</th>\n",
       "      <th>IV_Input_2</th>\n",
       "      <th>SOFA_2</th>\n",
       "      <th>Glucose_3</th>\n",
       "      <th>PaO2_FiO2_3</th>\n",
       "      <th>IV_Input_3</th>\n",
       "      <th>SOFA_3</th>\n",
       "      <th>Died_within_48H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>31005.0</td>\n",
       "      <td>116.833333</td>\n",
       "      <td>172.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>125.666667</td>\n",
       "      <td>364.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>132.200000</td>\n",
       "      <td>439.310339</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10989.0</td>\n",
       "      <td>120.000000</td>\n",
       "      <td>170.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>154.000000</td>\n",
       "      <td>163.714286</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>164.000000</td>\n",
       "      <td>174.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4132.0</td>\n",
       "      <td>123.200000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>126.400000</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>129.600000</td>\n",
       "      <td>91.388889</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>37528.0</td>\n",
       "      <td>168.500000</td>\n",
       "      <td>260.833333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>227.000000</td>\n",
       "      <td>277.777778</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>257.857143</td>\n",
       "      <td>191.935482</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>-1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86428.0</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>146.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>208.666667</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>115.000000</td>\n",
       "      <td>210.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   icustayid_1   Glucose_1  PaO2_FiO2_1  IV_Input_1  SOFA_1   Glucose_2  \\\n",
       "0      31005.0  116.833333   172.000000         1.0     9.0  125.666667   \n",
       "1      10989.0  120.000000   170.000000         1.0     5.0  154.000000   \n",
       "2       4132.0  123.200000   266.000000         1.0     8.0  126.400000   \n",
       "3      37528.0  168.500000   260.833333         0.0     3.0  227.000000   \n",
       "4      86428.0  115.000000   146.000000         1.0     7.0  115.000000   \n",
       "\n",
       "   PaO2_FiO2_2  IV_Input_2  SOFA_2   Glucose_3  PaO2_FiO2_3  IV_Input_3  \\\n",
       "0   364.000000         1.0     5.0  132.200000   439.310339         1.0   \n",
       "1   163.714286         1.0     7.0  164.000000   174.000000         1.0   \n",
       "2    94.000000         1.0     7.0  129.600000    91.388889         1.0   \n",
       "3   277.777778         1.0     4.0  257.857143   191.935482         1.0   \n",
       "4   208.666667         1.0     8.0  115.000000   210.000000         1.0   \n",
       "\n",
       "   SOFA_3  Died_within_48H  \n",
       "0     0.0              1.0  \n",
       "1     6.0              1.0  \n",
       "2     8.0             -1.0  \n",
       "3    10.0             -1.0  \n",
       "4    10.0              1.0  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "file = open('mimic3_MDTR_data_dict_3stage_V2.pickle', 'rb')\n",
    "mimic3_MDTR = pickle.load(file)\n",
    "MDTR_data = pd.read_csv('mimic3_MDTR_3stage_V2.csv')\n",
    "MDTR_data.head()\n",
    "DTR_data = pd.read_csv('mimic3_DTR_3stage_V2.csv')\n",
    "DTR_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b03650-db06-4e91-ac8b-bcc870187f00",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CEL: 3-Stage Mediation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6de86b4-574a-4f15-a346-0a0122464ae7",
   "metadata": {},
   "source": [
    "Under the 3-stage setting, we are interested in analyzing the treatment effect on the final outcome Died_within_48H observed at the end of the study by comparing the target treatment regime that provides IV input at all three stages and the control treatment regime that does not provide any treatment. Using the Q-learning based estimator proposed in [1], we examine the natural direct and indirect effects of the target treatment regime based on observational data. With the code in the following blocks, the estimated effect components are summarized in the following:\n",
    "\n",
    "| NDE   | NIE  | TE    |\n",
    "|-------|------|-------|\n",
    "| -.426 | .312 | -.114 |\n",
    "\n",
    "Specifically, when compared to no treatment, always giving IV input has a negative impact on the survival rate with an effect size of.114, among which the effect directly from actions to the final outcome is -.426 and the indirect effect of actions to the outcome passing through mediators is .312. The following is the bootstrapped estimation (bootstraped SE):\n",
    "\n",
    "| NDE   | NIE  | TE    |\n",
    "|-------|------|-------|\n",
    "| -.274(1.390) | .220 (1.295) | -.054 (.567)|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "681ab834-086a-438b-becc-0e101a17bde8",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_35388/2533832358.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/nas/longleaf/home/lge/CausalDM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearners\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mMediated_QLearning\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmediator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmimic3_MDTR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "from causaldm.learners import Mediated_QLearning\n",
    "state, action, mediator, reward = mimic3_MDTR.values()\n",
    "MediatedQLearn = Mediated_QLearning.Mediated_QLearning()\n",
    "N=len(state)\n",
    "regime_control = pd.DataFrame({'IV_Input_1':[0]*N,'IV_Input_2':[0]*N, 'IV_Input_3':[0]*N}).set_index(state.index)\n",
    "regime_target = pd.DataFrame({'IV_Input_1':[1]*N,'IV_Input_2':[1]*N, 'IV_Input_3':[1]*N}).set_index(state.index)\n",
    "MediatedQLearn.train(state, action, mediator, reward, T=3, dim_state = 2, dim_mediator = 1, \n",
    "                     regime_target = regime_target, regime_control = regime_control,bootstrap = False)\n",
    "NIE, NDE = MediatedQLearn.est_NDE_NIE()\n",
    "NIE, NDE, NIE+NDE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7113c82f-4488-4781-a62d-9eb23c216282",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0    0.647057\n",
       " dtype: float64,\n",
       " 0    0.761217\n",
       " dtype: float64)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MediatedQLearn.V_target, MediatedQLearn.V_control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c438fe5-fb92-4b6e-ae5e-3742590feb55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[ 0.2201],\n",
       "        [-0.2744],\n",
       "        [-0.0543]]),\n",
       " array([[1.2949],\n",
       "        [1.3891],\n",
       "        [0.567 ]]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MediatedQLearn.train(state, action, mediator, reward, T=3, dim_state = 2, dim_mediator = 1, \n",
    "                     regime_target = regime_target, regime_control = regime_control,bootstrap = True, n_bs = 500)\n",
    "boots_results, mean_effect, SE_effect = MediatedQLearn._predict_value_boots()\n",
    "mean_effect, SE_effect"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b3bb90-76b4-4005-b626-574ad2759383",
   "metadata": {},
   "source": [
    "## CPL: 3-Stage Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "92cc85ce-d2b7-445b-9950-e2a87ca7d2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners import QLearning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903e7f10-ce3d-4124-9d8f-4e1ab719e4b0",
   "metadata": {},
   "source": [
    "As an example, we use the **Q-learning** algorithm to evaluate policies based on the observed data, with the linear regression models defined as the following:\n",
    "\\begin{align}\n",
    "Q_1(s,a_1,\\boldsymbol{\\beta}) = &\\beta_{00}+\\beta_{01}*\\textrm{Glucose}_1+\\beta_{02}*\\textrm{PaO2_FiO2}_1\\\\\n",
    "                    &I(a_1=1)*\\{\\beta_{10}+\\beta_{11}*\\textrm{Glucose}_1+\\beta_{12}*\\textrm{PaO2_FiO2}_1\\},\\\\\n",
    "Q_2(s,a_2,\\boldsymbol{\\mu}) = &\\mu_{00}+\\mu_{01}*\\textrm{Glucose}_1+\\mu_{02}*\\textrm{PaO2_FiO2}_1+\\mu_{03}*\\textrm{SOFA}_1+\\\\\n",
    "                    &\\mu_{04}*\\textrm{Glucose}_2+\\mu_{05}*\\textrm{PaO2_FiO2}_2+\\\\\n",
    "                    &I(a_2=1)*\\{\\mu_{10}+\\mu_{11}*\\textrm{Glucose}_2+\\mu_{12}*\\textrm{PaO2_FiO2}_2+\\mu_{13}*\\textrm{SOFA}_1\\},\\\\\n",
    "Q_3(s,a_3,\\boldsymbol{\\theta}) = &\\theta_{00}+\\theta_{01}*\\textrm{Glucose}_1+\\theta_{02}*\\textrm{PaO2_FiO2}_1+\\theta_{03}*\\textrm{SOFA}_1+\\\\\n",
    "                    &\\theta_{04}*\\textrm{Glucose}_2+\\theta_{05}*\\textrm{PaO2_FiO2}_2+\\theta_{06}*\\textrm{SOFA}_2+\\\\\n",
    "                    &\\theta_{07}*\\textrm{Glucose}_3+\\theta_{08}*\\textrm{PaO2_FiO2}_3+\\\\\n",
    "                    &I(a_2=1)*\\{\\theta_{10}+\\theta_{11}*\\textrm{Glucose}_3+\\theta_{12}*\\textrm{PaO2_FiO2}_3\\}\n",
    "\\end{align}\n",
    "\n",
    "Using the code below, we evaluated two target polices (regimes). The first one is a fixed treatement regime that applies no treatment at all stages (Policy1), with an estimated value of .7982. Another is a fixed treatment regime that applies treatment at all stages (Policy2), with an estimated value of .6492. Therefore, the treatment effect of Policy2 comparing to Policy1 is -.1490, implying that receiving IV input increase the mortality rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "88152436-d5eb-4ffd-ac45-d40fe19bdcda",
   "metadata": {},
   "outputs": [],
   "source": [
    "DTR_data.rename(columns = {'Died_within_48H':'R',\n",
    "                            'Glucose_1':'S1_1', 'Glucose_2':'S1_2','Glucose_3':'S1_3',\n",
    "                            'PaO2_FiO2_1':'S3_1', 'PaO2_FiO2_2':'S3_2','PaO2_FiO2_3':'S3_3',\n",
    "                            'SOFA_1':'S4_1', 'SOFA_2':'S4_2', 'SOFA_3':'S4_3',\n",
    "                            'IV_Input_1':'A1','IV_Input_2':'A2', 'IV_Input_3':'A3'}, inplace = True)\n",
    "R = DTR_data['R'] #lower the better\n",
    "S = DTR_data[['S1_1','S1_2','S1_3','S3_1','S3_2','S3_3','S4_1','S4_2','S4_3']]\n",
    "A = DTR_data[['A1','A2', 'A3']]\n",
    "# specify the model you would like to use\n",
    "model_info = [{\"model\": \"R~S1_1+S3_1+A1+S1_1*A1+S3_1*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"R~S1_1+S3_1+S4_1+S1_2+S3_2+A2+S1_2*A2+S3_2*A2+S4_1*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"R~S1_1+S3_1+S4_1+S1_2+S3_2+S4_2+S1_3+S3_3+A3+S1_3*A3+S3_3*A3+S4_2*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "566b8bb8-80b1-4c29-8d92-3f273484094d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7981963882433065"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the policy with no treatment\n",
    "N=len(S)\n",
    "regime = pd.DataFrame({'A1':[0]*N,\n",
    "                      'A2':[0]*N,\n",
    "                     'A3':[0]*N}).set_index(S.index)\n",
    "#evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "QLearn.train(S, A, R, model_info, T=3, regime = regime, evaluate = True, mimic3_clip = True)\n",
    "QLearn.predict_value(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "10f6441a-273c-4022-802d-e8d2df2336f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.649210729138117"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating the policy that gives IV input at both stages\n",
    "N=len(S)\n",
    "regime = pd.DataFrame({'A1':[1]*N,\n",
    "                      'A2':[1]*N,\n",
    "                     'A3':[1]*N}).set_index(S.index)\n",
    "#evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "QLearn.train(S, A, R, model_info, T=3, regime = regime, evaluate = True, mimic3_clip = True)\n",
    "QLearn.predict_value(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54de121-0b8b-4412-9678-3941846423c0",
   "metadata": {},
   "source": [
    "## CPL: 3-Stage Policy Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f86ca-4a31-470d-b3b2-837b4c7dbcbf",
   "metadata": {},
   "source": [
    "Further, to find an optimal policy maximizing the expected value, we use the **Q-learning** algorithm again to do policy optimization. Using the regression model we specified above and the code in the following block, the estimated optimal policy is summarized as the following regime.\n",
    "\n",
    "- At stage 1:\n",
    "    1. We would recommend $A=0$ (IV_Input = 0) if $.0002*\\textrm{Glucose}_1+.0012*\\textrm{PaO2_FiO2}_1>.1101$\n",
    "    2. Else, we would recommend $A=1$ (IV_Input = 1).\n",
    "- At stage 2:\n",
    "    1. We would recommend $A=0$ (IV_Input = 0) if $.0005*\\textrm{Glucose}_2-.00002*\\textrm{PaO2_FiO2}_2+.0141*\\textrm{SOFA}_1<.1442$\n",
    "    2. Else, we would recommend $A=1$ (IV_Input = 1).\n",
    "- At stage 3:\n",
    "    1. We would recommend $A=0$ (IV_Input = 0) if $-.0009*\\textrm{Glucose}_2+.0016*\\textrm{PaO2_FiO2}_2-.0228*\\textrm{SOFA}_2<.4136$\n",
    "    2. Else, we would recommend $A=1$ (IV_Input = 1).\n",
    "    \n",
    "Appling the estimated optimal regime to individuals in the observed data, we summarize the regime pattern for each patients in the following table:\n",
    "\n",
    "| # patients | IV_Input 1 | IV_Input 2 | IV_Input 3 |\n",
    "|------------|------------|------------|------------|\n",
    "| 23         | 1          | 1          | 0          |\n",
    "| 10         | 1          | 0          | 0          |\n",
    "| 5          | 1          | 1          | 1          |\n",
    "| 4          | 0          | 0          | 0          |\n",
    "| 4          | 0          | 1          | 0          |\n",
    "| 4          | 0          | 1          | 1          |\n",
    "| 2          | 0          | 0          | 1          |\n",
    "| 2          | 1          | 0          | 1          |\n",
    "\n",
    "The estimated value of the estimated optimal policy is **.9274**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6a5a82ac-ef51-4899-b626-02cbee491120",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt_d: A3  A2  A1\n",
      "0   1   1     23\n",
      "    0   1     10\n",
      "1   1   1      5\n",
      "0   0   0      4\n",
      "    1   0      4\n",
      "1   1   0      4\n",
      "    0   0      2\n",
      "        1      2\n",
      "dtype: int64\n",
      "opt value: 0.9274371195908725\n"
     ]
    }
   ],
   "source": [
    "# initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "# train the policy\n",
    "QLearn.train(S, A, R, model_info, T=3, mimic3_clip = True)\n",
    "# get the summary of the fitted Q models using the following code\n",
    "#print(\"fitted model Q0:\",QLearn.fitted_model[0].summary())\n",
    "#print(\"fitted model Q1:\",QLearn.fitted_model[1].summary())\n",
    "#4. recommend action\n",
    "opt_d = QLearn.recommend_action(S).value_counts()\n",
    "#5. get the estimated value of the optimal regime\n",
    "V_hat = QLearn.predict_value(S)\n",
    "print(\"opt_d:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dcd03a-13db-4924-8340-3df258236487",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Zheng, W., & van der Laan, M. (2017). Longitudinal mediation analysis with time-varying mediators and exposures, with application to survival outcomes. Journal of causal inference, 5(2)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d544c0-900b-457a-bd9d-5517128a9fbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}