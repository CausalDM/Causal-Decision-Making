{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/lj/55rs1m_n5gv3_0cy812ml6lh0000gq/T/ipykernel_63654/2982377520.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../CausalDM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "A-Learning, also known as Advantage Learning, is one of the main approaches to learning the optimal regime and works similarly to Q-learning. However, while Q-learning requires positing regression models to fit the expected outcome, A-learning models the contrasts between treatments and control, which can directly inform the optimal decision. A detailed comparison between Q-learning and A-learning can be found in [1]. While [1] mainly focus on the case with binary treatment options, a complete review of A-learning with multiple treatment options can be found in [2]. Here, following the algorithm in [1], we consider contrast-based A-learning. However, there is an alternative regret-based A-learning introduced in [3]. Some recent extensions to conventional A-learning, such as deep A-learning [4] and high-dimensional A-Learning [5], will be added soon.\n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., 0,1,2,3,4, where 0 stands for the control group by convention), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n",
    "\n",
    "contrast-based A-learning, as the name suggested, aims to learn and estimate the constrast function, $C_{tj}(h_{t})$. Here, $h_{t}=\\{X_{1i},A_{1i},\\cdots,X_{ti}\\})$ includes all the information observed till step t. Furthermore, we also need to posit a model for the conditional expected outcome for the control option (treatment $0$), $Q_t(h_t,0)$, and the propensity function $\\omega(h_{t},a_{t})$. Detailed definitions are provided in the following. Suppose there are $m_t$ number of options, and the action space $\\mathcal{A}_t=\\{0,1,\\dots,m_t-1\\}$ for each step t. With decision point $t$, we define thoes key functions as follows:\n",
    "*   Q-function:\n",
    "    For the final step $T$, \n",
    "    \\begin{align}\n",
    "    Q_T(h_T,a_{T})=E[Y|H_{T}=h_{T}, A_{T}=a_{T}],\n",
    "    \\end{align}\n",
    "    \n",
    "    If there is a multi-stage case with total step $T>1$, for the step $t=1,\\cdots,T-1$,\n",
    "    \\begin{align}\n",
    "    Q_t(h_t,a_{t})=E[V_{t+1}|H_{t}=h_{t}, A_{t}=a_{t}],\n",
    "    \\end{align}\n",
    "    where \n",
    "    \\begin{align}\n",
    "    V_{t}(h_{t}) = \\max_{j\\in\\mathcal{A}_t}Q_{t}(h_t,j)\n",
    "    \\end{align}\n",
    "    Alternatively, with the contrast function defined in the follwing,\n",
    "    \\begin{align}\n",
    "    Q_t(h_t,j) = Q_t(h_t,0) + C_{tj}(h_t),\\quad j=0,\\dots,m_k-1,\\quad t=1,\\dots,T.\n",
    "    \\end{align}\n",
    "*   Contrast functions (optimal blip to zero functions)\n",
    "    \\begin{align}\n",
    "    C_{tj}(h_t)=Q_t(h_t,j)-Q_t(h_t,0),\\quad j=0,\\dots,m_k-1,\\quad t=1,\\dots,T.\n",
    "    \\end{align}\n",
    "*   Propensity score\n",
    "    \\begin{align}\n",
    "    \\omega_{t}(h_t,a_t)=P(A_t=a_t|H_t=h_t)\n",
    "    \\end{align}\n",
    "*   Optimal regime\n",
    "    \\begin{align}\n",
    "    d_t^{opt}(h_t)=\\arg\\max_{j\\in\\mathcal{A}_t}C_{tj}(h_t)\n",
    "    \\end{align}\n",
    "\n",
    "\n",
    "In the following, we would start from a simple case having only one decision point and then introduce the multistage case with multiple decision points. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Single Decision Point\n",
    "\n",
    "- **Basic Logic**: Positting models, $C_{j}(h,\\psi_{j})$,$Q(h,0,\\phi)$,and $\\omega(h,a,\\gamma)$, A-learning aims to estimate $\\psi_{j}$, $\\phi$, and $\\gamma$ by g-estimation. With the $\\hat{\\psi}_{j}$ in hand, the optimal decision is directly derived.\n",
    "\n",
    "- **Key Steps**:\n",
    "    1. Fitted a model $\\omega_{1}(h_1,a_1,\\gamma)$, which can be solved directly by existing approaches (i.e., logistic regression, .etc),\n",
    "    2. Substituting the $\\hat{\\gamma}$, we estimate the $\\hat{\\psi}_{j}$ and $\\gamma$ by solving the euqations in Appendix A.1 jointly.      \n",
    "    2. For each individual find the optimal action $a_{i}$ such that $a_{i} = \\arg\\max_{j\\in\\mathcal{A}}C_{j}(h,\\hat{\\psi_{j}})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Optimal Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import ALearning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklift.datasets import fetch_hillstrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous Y\n",
    "data, target, treatment = fetch_hillstrom(target_col='spend', return_X_y_t=True)\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "data = pd.concat([data,pd.get_dummies(data['zip_code'], prefix='zip_code')],axis=1)\n",
    "data = pd.concat([data,pd.get_dummies(data['channel'], prefix='channel')],axis=1)\n",
    "# now drop the original 'country' column (you don't need it anymore)\n",
    "data.drop(['zip_code'],axis=1, inplace=True)\n",
    "data.drop(['channel'],axis=1, inplace=True)\n",
    "data.drop(['history_segment'],axis=1, inplace=True)\n",
    "data.drop(['zip_code_Rural'],axis=1, inplace=True) # Rural as the reference group\n",
    "data.drop(['channel_Multichannel'],axis=1, inplace=True) # Multichannel as the reference group \n",
    "\n",
    "Y = np.array(target)\n",
    "X = np.hstack([np.ones((len(data),1)),np.array(data)])# add an intercept column\n",
    "# convert the categorical variable into integers with treatment 0 = No emails\n",
    "treatment.replace(['Womens E-Mail', 'No E-Mail', 'Mens E-Mail'],[1, 0, 2], inplace=True) \n",
    "treatment = np.array(treatment)\n",
    "#get the subset which has Y>0 == n=578\n",
    "X = X[Y>0]\n",
    "A = {}\n",
    "A[0] = treatment[Y>0]\n",
    "Y = Y[Y>0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 1.024084\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prop': {0: <statsmodels.discrete.discrete_model.MultinomialResultsWrapper at 0x7fc636b4ec10>},\n",
       " 'Q0': {0: array([ 7.0107e+00,  3.6316e+00, -1.0577e-02,  5.8950e+01,  2.9656e+01,\n",
       "         -2.5558e+01,  5.5264e+01,  2.5197e+01,  2.0701e+01,  2.4073e+01])},\n",
       " 'contrast': {0: {1: array([ 1.6399e+02, -3.6746e+00,  3.2799e-02, -9.1699e+01, -1.0692e+02,\n",
       "           2.6025e+01, -4.4841e+01,  1.0508e+01, -2.6366e+01, -3.3564e+01]),\n",
       "   2: array([ 7.8924e+01, -4.0368e+00,  6.0152e-03, -3.3001e+01, -2.3821e+01,\n",
       "           3.3728e+01, -5.1146e+01, -3.3706e+01, -1.2922e+01, -9.3451e+00])}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the learner\n",
    "ALearn = ALearning.ALearning()\n",
    "p = X.shape[1]\n",
    "model_info = [{'X_prop': list(range(p)),\n",
    "              'X_q0': list(range(p)),\n",
    "               'X_C':{1:list(range(p)),2:list(range(p))},\n",
    "              'action_space': [0,1,2]}] #A in [0,1,2]\n",
    "# train the policy\n",
    "ALearn.train(X, A, Y, model_info, T=1)\n",
    "# Fitted Model\n",
    "ALearn.fitted_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recommend Optimal Decisions and Get the Estimated Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt regime:    A0\n",
      "0   1\n",
      "1   0\n",
      "2   0\n",
      "3   1\n",
      "4   0\n",
      "opt value: 140.72832928963362\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = ALearn.recommend().head()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = ALearn.estimate_value(X,A)\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Multiple Decision Points\n",
    "\n",
    "- **Basic Logic**: Similar to Q learning, a backward approach was proposed to find the optimized treatment regime at each decision point. \n",
    "\n",
    "    At Decision $T$, similar as what we did previously with single decision point, we estimate the $\\psi_{Kj}$, $\\phi_K$ and $\\gamma_K$ by solving the eqautions in A.2 jointly, and the optimal decision at time $T$ is calculated accordingly. \n",
    "    \n",
    "    At Decision $t=T-1,\\dots,1$, we use similar trick as decision $T$, except for changing $Y$ in the estimating eqautions to some pseudo outcome $\\tilde{Y}_{t+1,i}$, such that:\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    \\tilde{Y}_{ti}=\\tilde{Y}_{t+1,i}+\\max_{j=0,\\dots,m_t-1}C_{tj}(H_{ti},\\hat{\\psi}_{tj})-\\sum_{j=1}^{m_k-1}\\mathbb{I}\\{A_{ti}=j\\}C_{tj}(H_{ti},\\hat{\\psi}_{tj}),\n",
    "    \\end{aligned}\n",
    "    $$ \n",
    "    where $\\tilde{Y}_{T+1,i} = Y_{i}$.\n",
    "    \n",
    "    Estimating $\\psi_{tj}$, $\\phi_t$ and $\\gamma_t$ iteratively for $t=T-1,\\cdots,1$, we calculated the optimal decision at time $t$, $a_{ti}$ as\n",
    "    $$\n",
    "    \\begin{aligned}\n",
    "    a_{ti}=\\arg\\max_{j=0,\\dots,m_t-1} C_{tj}(h_{ti};\\hat{\\psi}_{tj}).\n",
    "    \\end{aligned}\n",
    "    $$\n",
    "\n",
    "\n",
    "- **Key Steps**: \n",
    "    1. At the final decision point $t=T$, fitted a model $\\omega_{T}(h_{T},a,\\hat{\\gamma}_{T})$, and estimating $\\psi_{Tj}$, $\\phi_{T}$ by solving the equations in A.2 jointly;\n",
    "    2. For each individual $i$, calculated the pseudo-outcome $\\tilde{Y}_{Ti}$, and the optimal action $a_{Ti}$;\n",
    "    3. For decision point $t = T-1,\\cdots, 1$,\n",
    "        1. fitted a model $\\omega_{t}(h_{t},a,\\hat{\\gamma}_{t})$, and estimating $\\psi_{tj}$, $\\phi_{t}$ by solving the equations in A.3 jointly with the pseudo-outcome $\\tilde{Y}_{t+1}$\n",
    "        2. For each individual $i$, calculated the pseudo-outcome $\\tilde{Y}_{ti}$, and the optimal action $a_{ti}$;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Optimal Decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: there might be something wrong with the multiple step as the difference between A-learning and Q-learning is large\n",
    "\n",
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import ALearning\n",
    "from causaldm.test import shared_simulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare the dataset (dataset from the DTR book)\n",
    "import pandas as pd\n",
    "dataMDP = pd.read_csv(\"dataMDP_feasible.txt\", sep=',')\n",
    "Y = np.array(dataMDP['Y'])\n",
    "X = np.hstack([np.ones((len(Y),1)),np.array(dataMDP[['CD4_0','CD4_6','CD4_12']])])\n",
    "A = {}\n",
    "A[0] = np.array(dataMDP['A1'])\n",
    "A[1] = np.array(dataMDP['A2'])\n",
    "A[2] = np.array(dataMDP['A3'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.650933\n",
      "         Iterations 5\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.688144\n",
      "         Iterations 4\n",
      "Optimization terminated successfully.\n",
      "         Current function value: 0.637130\n",
      "         Iterations 5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'prop': {2: <statsmodels.discrete.discrete_model.MultinomialResultsWrapper at 0x7fc62c969b10>,\n",
       "  1: <statsmodels.discrete.discrete_model.MultinomialResultsWrapper at 0x7fc62c969ed0>,\n",
       "  0: <statsmodels.discrete.discrete_model.MultinomialResultsWrapper at 0x7fc62c969850>},\n",
       " 'Q0': {2: array([40.8924,  3.1455, -0.5109, -0.1229]),\n",
       "  1: array([158.0503,   2.9351,  -0.595 ]),\n",
       "  0: array([241.7725,   2.0474])},\n",
       " 'contrast': {2: {1: array([ 3.5872e+02, -1.0493e+00,  4.9347e-03, -5.2010e-02])},\n",
       "  1: {1: array([-214.568 ,    1.1057,   -0.62  ])},\n",
       "  0: {1: array([-9.8412e+01,  9.2479e-02])}}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_info = [{'X_prop': list(range(2)),\n",
    "              'X_q0': list(range(2)),\n",
    "               'X_C':{1:list(range(2))},\n",
    "              'action_space': [0,1]},\n",
    "             {'X_prop': list(range(3)),\n",
    "              'X_q0': list(range(3)),\n",
    "               'X_C':{1:list(range(3))},\n",
    "              'action_space': [0,1]},\n",
    "             {'X_prop': list(range(4)),\n",
    "              'X_q0': list(range(4)),\n",
    "               'X_C':{1:list(range(4))},\n",
    "              'action_space': [0,1]}]\n",
    "# train the policy\n",
    "ALearn.train(X, A, Y, model_info, T=3)\n",
    "# Fitted Model\n",
    "ALearn.fitted_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt regime:    A2  A1  A0\n",
      "0   0   0   0\n",
      "1   0   0   0\n",
      "2   0   0   0\n",
      "3   0   0   0\n",
      "4   0   0   0\n",
      "opt value: 1162.4662578531918\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = ALearn.recommend().head()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = ALearn.estimate_value(X,A)\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Infinite Horizon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Optimal Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Schulte, P. J., Tsiatis, A. A., Laber, E. B., & Davidian, M. (2014). Q-and A-learning methods for estimating optimal dynamic treatment regimes. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4), 640.\n",
    "2. Robins, J. M. (2004). Optimal structural nested models for optimal sequential decisions. In Proceedings of the second seattle Symposium in Biostatistics (pp. 189-326). Springer, New York, NY.\n",
    "3. Murphy, S. A. (2003). Optimal dynamic treatment regimes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(2), 331-355.\n",
    "4. Liang, S., Lu, W., & Song, R. (2018). Deep advantage learning for optimal dynamic treatment regime. Statistical theory and related fields, 2(1), 80-88.\n",
    "5. Shi, C., Fan, A., Song, R., & Lu, W. (2018). High-dimensional A-learning for optimal dynamic treatment regimes. Annals of statistics, 46(3), 925."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\frac{\\partial C_{j}(H_{i};\\psi_{j})}{\\partial \\psi_{j}}\\{\\mathbb{I}\\{A_{i}=j\\}-\\omega(H_{i},j;\\hat{\\gamma})\\}\\times \\Big\\{Y_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(H_{i;\\psi_{j'}})-Q(H_{i},0;\\phi)\\Big\\}=0\\\\\n",
    "&\\sum_{i=1}^n \\frac{\\partial Q(H_{i},0;\\phi)}{\\partial \\phi}\\Big\\{Y_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(H_{i};\\psi_{j'}) Q(H_{i}0;\\phi)\\Big\\}=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## A.2\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial C_{Tj}(H_{Ti};\\psi_{Tj})}{\\partial \\psi_{Tj}}\\{\\mathbb{I}\\{A_{Ti}=j\\}-\\omega_T(H_{Ti},j;\\gamma_T)\\}\\times \\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_{T})\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial Q_T(H_{Ti},0;\\phi_T)}{\\partial \\phi_T}\\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_T)\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial \\omega_T(H_{Ti},j;\\gamma_T)}{\\partial \\gamma_T}\\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_T)\\Big\\}\\right]=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## A.3\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial C_{tj}(H_{ti};\\psi_{tj})}{\\partial \\psi_{tj}}\\{\\mathbb{I}\\{A_{ti}=j\\}-\\omega_T(H_{ti},j;\\gamma_t)\\}\\times \\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_{t})\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial Q_t(H_{ti},0;\\phi_t)}{\\partial \\phi_t}\\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_t)\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial \\omega_t(H_{ti},j;\\gamma_t)}{\\partial \\gamma_t}\\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_t)\\Big\\}\\right]=0\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}