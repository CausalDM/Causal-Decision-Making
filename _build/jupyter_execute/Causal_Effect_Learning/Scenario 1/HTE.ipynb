{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "112864aa"
   },
   "source": [
    "# Heterogeneous Treatment Effect Estimation (Single Stage)\n",
    "In the previous section, we've introduced the estimation of average treatment effect, where we aims to estimate the difference of potential outcomes by executing action $A=1$ v.s. $A=0$. That is, \n",
    "\\begin{equation*}\n",
    "\\text{ATE}=\\mathbb{E}[R(1)-R(0)].\n",
    "\\end{equation*}\n",
    "\n",
    "In this section, we will focus on the estimation of heterogeneous treatment effect (HTE), which is also one of the main focuses in causal inference.\n",
    "\n",
    "\n",
    "\n",
    "## Main Idea\n",
    "Let's first consider the single stage setup, where the observed data can be written as a state-action-reward triplet $\\{S_i,A_i,R_i\\}_{i=1}^n$ with a total of $n$ trajectories. Heterogeneous treatment effect, as we can imagine from its terminology, aims to measure the heterogeneity of the treatment effect for different subjects. Specifically, we define HTE as $\\tau(s)$, where\n",
    "\\begin{equation*}\n",
    "\\tau(s)=\\mathbb{E}[R(1)-R(0)|S=s],\n",
    "\\end{equation*}\n",
    "\n",
    "where $S=s$ denotes the state information of a subject. \n",
    "\n",
    "The estimation of HTE is widely used in a lot of real cases such as precision medicine, advertising, recommendation systems, etc. For example, in adversiting system, the company would like to know the impact (such as annual income) of exposing an ad to a group of customers. In this case, $S$ contains all of the information of a specific customer, $A$ denotes the status of ads exposure ($A=1$ means exposed and $A=0$ means not), and $R$ denotes the reward one can observe when assigned to policy $A$. \n",
    "\n",
    "Suppose the ad is a picture of a dress that can lead the customers to a detail page on a shopping website. In this case, females are more likely to be interested to click the picture and look at the detail page of a dress, resulting in a higher conversion rate than males. The difference of customers preference in clothes can be regarded as the heterogeneity of the treatment effect. By looking at the HTE for each customer, we can clearly estimate the reward of ads exposure from a granular level. \n",
    "\n",
    "Another related concept is conditional averge treatment effect, which is defined as\n",
    "\\begin{equation*}\n",
    "\\text{CATE}=\\mathbb{E}[R(1)-R(0)|Z],\n",
    "\\end{equation*}\n",
    "\n",
    "where $Z$ is a collection of states with some specific characsteristics. For example, if the company is interested in the treatment effect of exposing the dress to female customers, $Z$ can be defined as ``female\", and the problem can be addressed under the structure CATE estimation.\n",
    "\n",
    "\n",
    "\n",
    "## Different approaches in single-stage HTE estimation\n",
    "Next, let's briefly summarize some state-of-the-art approaches in estimating the heterogeneous treatment effect. There are several review papers which summarize some commonly-used approaches in literature, some of which are also detailed in the following subsections here. For more details please refer to [1], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yfiI-lSZRuOP"
   },
   "source": [
    "### **1. S-learner**\n",
    "\n",
    "\n",
    "The first estimator we would like to introduce is the S-learner, also known as a ``single learner\". This is one of the most foundamental learners in HTE esitmation, and is very easy to implement.\n",
    "\n",
    "Under three common assumptions in causal inference, i.e. (1) consistency, (2) no unmeasured confounders (NUC), (3) positivity assumption, the heterogeneous treatment effect can be identified by the observed data, where\n",
    "\\begin{equation*}\n",
    "\\tau(s)=\\mathbb{E}[R|S,A=1]-\\mathbb{E}[R|S,A=0].\n",
    "\\end{equation*}\n",
    "\n",
    "The basic idea of S-learner is to fit a model for $\\mathbb{E}[R|S,A]$, and then construct a plug-in estimator based on the expression above. Specifically, the algorithm can be summarized as below:\n",
    "\n",
    "**Step 1:**  Estimate the combined response function $\\mu(s,a):=\\mathbb{E}[R|S=s,A=a]$ with any regression algorithm or supervised machine learning methods;\n",
    "\n",
    "**Step 2:**  Estimate HTE by \n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{S-learner}}(s)=\\hat\\mu(s,1)-\\hat\\mu(s,0).\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "eRpP5k9MBtzO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/wc/c5dcq1_d1gn8w9yg5728l9dm0000gn/T/ipykernel_16197/1742119828.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# import related packages\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mlightgbm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLGBMRegressor\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util_causaldm\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# import related packages\n",
    "from matplotlib import pyplot as plt;\n",
    "from lightgbm import LGBMRegressor;\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from causaldm._util_causaldm import *;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lovM_twTxuOj"
   },
   "outputs": [],
   "source": [
    "n = 10**3  # sample size in observed data\n",
    "n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n",
    "seed=223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhfJntzcVVy2"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n",
    "#data_target = get_data_simulation(n0, seed, policy=\"target\")\n",
    "\n",
    "# The true expected heterogeneous treatment effect\n",
    "HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 136,
     "status": "ok",
     "timestamp": 1674192535902,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "F4yvhP_yJ9DH",
    "outputId": "c15f5932-4ed6-4b8b-b225-3f71153f258d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-06f51ba5-8664-463b-8c33-36a5a7ba03f2\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034775</td>\n",
       "      <td>2.453145</td>\n",
       "      <td>1</td>\n",
       "      <td>7.167637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084880</td>\n",
       "      <td>-1.234459</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.553798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.144626</td>\n",
       "      <td>2.040543</td>\n",
       "      <td>1</td>\n",
       "      <td>5.956732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148426</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>1</td>\n",
       "      <td>1.095578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.120852</td>\n",
       "      <td>1.377594</td>\n",
       "      <td>1</td>\n",
       "      <td>4.323133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-2.022440</td>\n",
       "      <td>1.887551</td>\n",
       "      <td>0</td>\n",
       "      <td>6.797542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.411179</td>\n",
       "      <td>-1.655833</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.722846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.155706</td>\n",
       "      <td>-0.992197</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-1.510241</td>\n",
       "      <td>0.828438</td>\n",
       "      <td>0</td>\n",
       "      <td>4.167118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.744187</td>\n",
       "      <td>0.857147</td>\n",
       "      <td>0</td>\n",
       "      <td>4.458481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-06f51ba5-8664-463b-8c33-36a5a7ba03f2')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-06f51ba5-8664-463b-8c33-36a5a7ba03f2 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-06f51ba5-8664-463b-8c33-36a5a7ba03f2');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           S1        S2  A         R\n",
       "0    0.034775  2.453145  1  7.167637\n",
       "1    0.084880 -1.234459  0 -1.553798\n",
       "2   -0.144626  2.040543  1  5.956732\n",
       "3    0.148426 -0.021139  1  1.095578\n",
       "4   -0.120852  1.377594  1  4.323133\n",
       "..        ...       ... ..       ...\n",
       "995 -2.022440  1.887551  0  6.797542\n",
       "996  0.411179 -1.655833  0 -2.722846\n",
       "997  0.155706 -0.992197  0 -1.140100\n",
       "998 -1.510241  0.828438  0  4.167118\n",
       "999 -1.744187  0.857147  0  4.458481\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nxf-mXJmFrEl"
   },
   "outputs": [],
   "source": [
    "SandA = data_behavior.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1674192537431,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "h5G8dAwM-PGO",
    "outputId": "0edd30d5-5b02-4368-c8ff-7a19d503d261"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=5)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S-learner\n",
    "S_learner = LGBMRegressor(max_depth=5)\n",
    "#S_learner = LinearRegression()\n",
    "#SandA = np.hstack((S.to_numpy(),A.to_numpy().reshape(-1,1)))\n",
    "S_learner.fit(SandA, data_behavior['R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Vqsb5wLTaR0q"
   },
   "outputs": [],
   "source": [
    "HTE_S_learner = S_learner.predict(np.hstack(( data_behavior.iloc[:,0:2].to_numpy(),np.ones(n).reshape(-1,1)))) - S_learner.predict(np.hstack(( data_behavior.iloc[:,0:2].to_numpy(),np.zeros(n).reshape(-1,1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FA-F8Jc_T5Lz"
   },
   "source": [
    "To evaluate how well S-learner is in estimating heterogeneous treatment effect, we compare its estimates with the true value for the first 10 subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 146,
     "status": "ok",
     "timestamp": 1674192538647,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "GvHnTOxmT5Lz",
    "outputId": "727bcff5-982a-4432-fd1c-02513b7c4dc1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-learner:   [-0.1492  0.1687 -0.589  -0.0319 -0.8354 -0.5843 -0.4577 -2.0791]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"S-learner:  \",HTE_S_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1674192539458,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "g9NgyjQ7PqRf",
    "outputId": "1f592589-ed31-4c52-9efe-ec6e14120648"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of S-learner is : 0.2857192464627009 , \n",
      " The overall estimation variance of S-learner is : 4.079505077680185\n"
     ]
    }
   ],
   "source": [
    "Bias_S_learner = np.sum(HTE_S_learner-HTE_true)/n\n",
    "Variance_S_learner = np.sum((HTE_S_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of S-learner is :\", Bias_S_learner, \", \\n\", \"The overall estimation variance of S-learner is :\",Variance_S_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mVAZTZYTUKJ6"
   },
   "source": [
    "**Conclusion:** The performance of S-learner, at least in this toy example, is not very attractive. Although it is the easiest approach to implement, the over-simplicity tends to cover some information that can be better explored with some advanced approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cMny8Ri7RvqC"
   },
   "source": [
    "\n",
    "### **2. T-learner**\n",
    "The second learner is called T-learner, which denotes ``two learners\". Instead of fitting a single model to estimate the potential outcomes under both treatment and control groups, T-learner aims to learn different models for $\\mathbb{E}[R(1)|S]$ and $\\mathbb{E}[R(0)|S]$ separately, and finally combines them to obtain a final HTE estimator.\n",
    "\n",
    "Define the control response function as $\\mu_0(s)=\\mathbb{E}[R(0)|S=s]$, and the treatment response function as $\\mu_1(s)=\\mathbb{E}[R(1)|S=s]$. The algorithm of T-learner is summarized below:\n",
    "\n",
    "**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods;\n",
    "\n",
    "**Step 2:**  Estimate HTE by \n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{T-learner}}(s)=\\hat\\mu_1(s)-\\hat\\mu_0(s).\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X1VmlNjstdsN"
   },
   "outputs": [],
   "source": [
    "mu0 = LGBMRegressor(max_depth=3)\n",
    "mu1 = LGBMRegressor(max_depth=3)\n",
    "\n",
    "mu0.fit(data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2],data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] )\n",
    "mu1.fit(data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2],data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] )\n",
    "\n",
    "\n",
    "# estimate the HTE by T-learner\n",
    "HTE_T_learner = mu1.predict(data_behavior.iloc[:,0:2]) - mu0.predict(data_behavior.iloc[:,0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CUv_0SuBTi3e"
   },
   "source": [
    "Now let's take a glance at the performance of T-learner by comparing it with the true value for the first 10 subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 134,
     "status": "ok",
     "timestamp": 1674192542717,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "5OHVneDpTgMp",
    "outputId": "fd4b88a9-aff3-4d5e-e38f-fcfcde0d12ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-learner:   [ 1.869   1.8733  0.6596  0.3087 -0.2298 -0.5598 -2.2745 -1.8211]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"T-learner:  \",HTE_T_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ux89PwJagR5_"
   },
   "source": [
    "This is quite good! T-learner captures the overall trend of the treatment effect w.r.t. the heterogeneity of different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 130,
     "status": "ok",
     "timestamp": 1674192544008,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "SW8ONIdFPpvM",
    "outputId": "cc010434-5184-4919-f714-32c39294125d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of T-learner is : 0.29138198450323705 , \n",
      " The overall estimation variance of T-learner is : 1.810391408711312\n"
     ]
    }
   ],
   "source": [
    "Bias_T_learner = np.sum(HTE_T_learner-HTE_true)/n\n",
    "Variance_T_learner = np.sum((HTE_T_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of T-learner is :\", Bias_T_learner, \", \\n\", \"The overall estimation variance of T-learner is :\",Variance_T_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vOsw-rfxU415"
   },
   "source": [
    "**Conclusion:** In this toy example, the overall estimation variance of T-learner is smaller than that of S-learner. In some cases when the treatment effect is relatively complex, it's likely to yield better performance by fitting two models separately. \n",
    "\n",
    "However, in an extreme case when both $\\mu_0(s)$ and $\\mu_1(s)$ are nonlinear complicated function of state $s$ while their difference is just a constant, T-learner will overfit each model very easily, yielding a nonlinear treatment effect estimator. In this case, other estimators are often preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8lwheJQ8RxAw"
   },
   "source": [
    "### **3. X-learner**\n",
    "Next, let's introduce the X-learner. As a combination of S-learner and T-learner, the X-learner can use information from the control(treatment) group to derive better estimators for the treatment(control) group, which is provably more efficient than the above two.\n",
    "\n",
    "The basic\n",
    "\n",
    "\n",
    "**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods (same as T-learner);\n",
    "\n",
    "\n",
    "**Step 2:**  Obtain the imputed treatment effects for individuals\n",
    "\\begin{equation*}\n",
    "\\tilde{\\Delta}_i^1:=R_i^1-\\hat\\mu_0(S_i^1), \\quad \\tilde{\\Delta}_i^0:=\\hat\\mu_1(S_i^0)-R_i^0.\n",
    "\\end{equation*}\n",
    "\n",
    "**Step 3:**  Fit the imputed treatment effects to obtain $\\hat\\tau_1(s):=\\mathbb{E}[\\tilde{\\Delta}_i^1|S=s]$ and $\\hat\\tau_0(s):=\\mathbb{E}[\\tilde{\\Delta}_i^0|S=s]$;\n",
    "\n",
    "**Step 4:**  The final HTE estimator is given by\n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{X-learner}}(s)=g(s)\\hat\\tau_0(s)+(1-g(s))\\hat\\tau_1(s),\n",
    "\\end{equation*}\n",
    "\n",
    "where $g(s)$ is a weight function between $[0,1]$. A possible way is to use the propensity score model as an estimate of $g(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 159,
     "status": "ok",
     "timestamp": 1674193729053,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "sfb-mplOP9HJ",
    "outputId": "6dafd14c-ce0c-400a-e5b3-cdedbc6fd2b2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Fit two models under treatment and control separately, same as T-learner\n",
    "\n",
    "mu0 = LGBMRegressor(max_depth=3)\n",
    "mu1 = LGBMRegressor(max_depth=3)\n",
    "\n",
    "S_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2]\n",
    "S_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2]\n",
    "R_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] \n",
    "R_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] \n",
    "\n",
    "mu0.fit(S_T0, R_T0)\n",
    "mu1.fit(S_T1, R_T1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zb42ZMw3pkqm"
   },
   "outputs": [],
   "source": [
    "# Step 2: impute the potential outcomes that are unobserved in original data\n",
    "\n",
    "n_T0 = len(R_T0)\n",
    "n_T1 = len(R_T1)\n",
    "\n",
    "Delta0 = mu1.predict(S_T0) - R_T0\n",
    "Delta1 = R_T1 - mu0.predict(S_T1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127,
     "status": "ok",
     "timestamp": 1674193741179,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "pxYLjE0Ar2_5",
    "outputId": "57d194e3-7d83-44ac-f11d-63f236e1a29d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=2)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit tau_1(s) and tau_0(s)\n",
    "\n",
    "tau0 = LGBMRegressor(max_depth=2)\n",
    "tau1 = LGBMRegressor(max_depth=2)\n",
    "\n",
    "tau0.fit(S_T0, Delta0)\n",
    "tau1.fit(S_T1, Delta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 143,
     "status": "ok",
     "timestamp": 1674196134427,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "LRvEZ4uluT-U",
    "outputId": "e7aaf58d-c3ed-4e1a-b0bd-a102ef5b428c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 4: fit the propensity score model $\\hat{g}(s)$ and obtain the final HTE estimator by taking weighted average of tau0 and tau1\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "g = LogisticRegression()\n",
    "g.fit(data_behavior.iloc[:,0:2],data_behavior['A'])\n",
    "\n",
    "HTE_X_learner = g.predict_proba(data_behavior.iloc[:,0:2])[:,0]*tau0.predict(data_behavior.iloc[:,0:2]) + g.predict_proba(data_behavior.iloc[:,0:2])[:,1]*tau1.predict(data_behavior.iloc[:,0:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 122,
     "status": "ok",
     "timestamp": 1674196352807,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "Mz8I_J0h4N6B",
    "outputId": "54691238-e900-4e67-b045-64b4ee65d044"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-learner:   [ 1.9341  1.9235  0.2944  0.2013 -0.4147 -0.5626 -2.214  -1.5443]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"X-learner:  \",HTE_X_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XyxjzDBy4N6k"
   },
   "source": [
    "X-learner also performs OK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 150,
     "status": "ok",
     "timestamp": 1674196374598,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "rlbacETd4N6l",
    "outputId": "9f0ca2b7-a86f-4d6f-9d36-898940d0f4c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of X-learner is : 0.2827518068171628 , \n",
      " The overall estimation variance of X-learner is : 1.7686646616779012\n"
     ]
    }
   ],
   "source": [
    "Bias_X_learner = np.sum(HTE_X_learner-HTE_true)/n\n",
    "Variance_X_learner = np.sum((HTE_X_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of X-learner is :\", Bias_X_learner, \", \\n\", \"The overall estimation variance of X-learner is :\",Variance_X_learner)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EnxQ_Tkg4o_q"
   },
   "source": [
    "**Conclusion:** In this toy example, the overall estimation variance of X-learner is the smallest, followed by T-learner, and the worst is given by S-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "32szzPY4RyWO"
   },
   "source": [
    "### **4. R learner** \n",
    "The idea of classical R-learner came from Robinson 1988 [3] and was formalized by Nie and Wager in 2020 [2]. The main idea of R learner starts from the partially linear model setup, in which we assume that\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    R&=A\\tau(S)+g_0(S)+U,\\\\\n",
    "    A&=m_0(S)+V,\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "where $U$ and $V$ satisfies $\\mathbb{E}[U|D,X]=0$, $\\mathbb{E}[V|X]=0$.\n",
    "\n",
    "After several manipulations, it’s easy to get\n",
    "\\begin{equation}\n",
    "\tR-\\mathbb{E}[R|S]=\\tau(S)\\cdot(A-\\mathbb{E}[A|S])+\\epsilon.\n",
    "\\end{equation}\n",
    "Define $m_0(X)=\\mathbb{E}[A|S]$ and $l_0(X)=\\mathbb{E}[R|S]$. A natural way to estimate $\\tau(X)$ is given below, which is also the main idea of R-learner:\n",
    "\n",
    "**Step 1**: Regress $R$ on $S$ to obtain model $\\hat{\\eta}(S)=\\hat{\\mathbb{E}}[R|S]$; and regress $A$ on $S$ to obtain model $\\hat{m}(S)=\\hat{\\mathbb{E}}[A|S]$.\n",
    "\n",
    "**Step 2**: Regress outcome residual $R-\\hat{l}(S)$ on propensity score residual $A-\\hat{m}(S)$.\n",
    "\n",
    "That is,\n",
    "\\begin{equation}\n",
    "\t\\hat{\\tau}(S)=\\arg\\min_{\\tau}\\left\\{\\mathbb{E}_n\\left[\\left(\\{R_i-\\hat{\\eta}(S_i)\\}-\\{A_i-\\hat{m}(S_i)\\}\\cdot\\tau(S_i)\\right)^2\\right]\\right\\}\t\n",
    "\\end{equation}\n",
    "\n",
    "The easiest way to do so is to specify $\\hat{\\tau}(S)$ to the linear function class. In this case, $\\tau(S)=S\\beta$, and the problem becomes to estimate $\\beta$ by solving the following linear regression:\n",
    "\\begin{equation}\n",
    "\t\\hat{\\beta}=\\arg\\min_{\\beta}\\left\\{\\mathbb{E}_n\\left[\\left(\\{R_i-\\hat{\\eta}(S_i)\\}-\\{A_i-\\hat{m}(S_i)\\} S_i\\cdot \\beta\\right)^2\\right]\\right\\}.\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hQNnTeMk9A56"
   },
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2z2JRumRzdo"
   },
   "source": [
    "### **5. DR-learner**\n",
    "\n",
    "DR-learner is a two-stage doubly robust estimator for HTE estimation. Before Kennedy et al. 2020 [4], there are several related approaches trying to extend the doubly robust procedure to HTE estimation, such as [5, 6, 7]. Compared with the above three estimators, DR-learner is proved to be oracle efficient under some mild assumptions detailed in Theorem 2 of [4].\n",
    "\n",
    "The basic steps of DR-learner is given below:\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1}^n$ to construct estimates $\\hat{\\pi}$ for the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1}^n$ to construct estimates $\\hat\\mu_a(s)$ for $\\mu_a(s):=\\mathbb{E}[R|S=s,A=a]$;\n",
    "\n",
    "**Step 2**: Pseudo-outcome regression: \\\\\n",
    "Define $\\widehat{\\phi}(Z)$ as the pseudo-outcome where \n",
    "\\begin{equation}\n",
    "\\widehat{\\phi}(Z)=\\frac{A-\\hat{\\pi}(S)}{\\hat{\\pi}(S)\\{1-\\hat{\\pi}(S)\\}}\\Big\\{R-\\hat{\\mu}_A(S)\\Big\\}+\\hat{\\mu}_1(S)-\\hat{\\mu}_0(S),\n",
    "\\end{equation}\n",
    "and regress it on covariates $S$ in the test sample $I_2^n$, yielding \n",
    "\\begin{equation}\n",
    "\\widehat{\\tau}_{\\text{DR-learner}}(s)=\\widehat{\\mathbb{E}}_n[\\widehat{\\phi}(Z)|S=s].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RVkkilMalB4p"
   },
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YgKc3F0cR0Y4"
   },
   "source": [
    "### **6. Lp-R-learner**\n",
    "\n",
    "As an extension of R-learner, Lp-R-learner combined the idea of residual regression with local polynomial adaptation, and leveraged the idea of cross fitting to further relax the conditions needed to obtain the oracle convergence rate. For brevity of content, we will just introduce their main algorithm. For more details about its theory and real data performance please see the paper written by Kennedy [4]. \n",
    "\t\n",
    "Let $(I_{1a}^n, I_{1b}^n,I_{2}^n)$ denote three independent samples of $n$ observations of $Z_i = (S_i, A_i, R_i)$. Let $b:\\mathbb{R}^d\\rightarrow \\mathbb{R}^p$ denote the vector of basis functions consisting of all powers of each covariate, up to order $\\gamma$, and all interactions up to degree $\\gamma$ polynomials. Let $K_{hs}(S)=\\frac{1}{h^d}K\\left(\\frac{S-s}{h}\\right)$ for $k:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ a bounded kernel function with support $[-1,1]^d$, and $h$ is a bandwidth parameter.\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1a}^n$ to construct estimates $\\hat{\\pi}_a$ of the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1b}^n$ to construct estimates $\\hat{\\eta}$ of the regression function $\\eta=\\pi\\mu_1+(1-\\pi)\\mu_0$, and estimtes $\\hat{\\pi}_b$ of the propensity scores $\\pi$.\n",
    "\n",
    "**Step 2**: Localized double-residual regression: \\\\\n",
    "Define $\\hat{\\tau}_r(s)$ as the fitted value from a kernel-weighted least squares regression (in the test sample $I_2^n$) of outcome residual $(R-\\hat{\\eta})$ on basis terms $b$ scaled by the treatment residual $A-\\hat{\\pi}_b$, with weights $\\Big(\\frac{A-\\hat{\\pi}_a}{A-\\hat{\\pi}_b}\\Big)\\cdot K_{hs}$. Thus $\\hat{\\tau}_r(s)=b(0)^T\\hat{\\theta}$ for\n",
    "\\begin{equation}\n",
    "\t\t\\hat{\\theta}=\\arg\\min_{\\theta\\in\\mathbb{R}^p}\\mathbb{P}_n\\left(K_{hs}(S)\\Big\\{ \\frac{A-\\hat{\\pi}_a(S)}{A-\\hat{\\pi}_b(S)}\\Big\\} \\left[  \\big\\{R-\\hat{\\eta}(S)\\big\\}-\\theta^Tb(S-s_0)\\big\\{A-\\hat{\\pi}_b(S)\\big\\} \\right] \\right).\n",
    "\\end{equation}\n",
    "**Step 3**: Cross-fitting(optional): \\\\\n",
    "Repeat Step 1–2 twice, first using $(I^n_{1b} , I_2^n)$ for nuisance training and $I_{1a}^n$ as the test samplem and then using $(I^n_{1a} , I_2^n)$ for training and $I_{1b}^n$ as the test sample. Use the average of the resulting three estimators of $\\tau$ as the final estimator $\\hat{\\tau}_r$.\n",
    "\n",
    "In the theory section, Kennedy proved that Lp-R-learner, compared with traditional DR learner, can achieve the oracle convergence rate under milder conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HbJkVOVd9hQA"
   },
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SgWh47pKR1XR"
   },
   "source": [
    "### **7. Generalized Random Forest**\n",
    "\n",
    "Developed by Susan Athey, Julie Tibshirani and Stefan Wager, Generalized Random Forest aims to give the solution to a set of local moment equations:\n",
    "\\begin{equation}\n",
    "  \\mathbb{E}\\big[\\psi_{\\tau(s),\\nu(s)}(O_i)\\big| S_i=s\\big]=0,\n",
    "\\end{equation}\n",
    "where $\\tau(s)$ is the parameter we care about and $\\nu(s)$ is an optional nuisance parameter. In the problem of Heterogeneous Treatment Effect Evaluation, our parameter of interest $\\tau(s)=\\xi\\cdot \\beta(s)$ is identified by \n",
    "\\begin{equation}\n",
    "  \\psi_{\\beta(s),\\nu(s)}(R_i,A_i)=(R_i-\\beta(s)\\cdot A_i-c(s))(1 \\quad A_i^T)^T.\n",
    "\\end{equation}\n",
    "The induced estimator $\\hat{\\tau}(s)$ for $\\tau(s)$ can thus be solved by\n",
    "\\begin{equation}\n",
    "  \\hat{\\tau}(s)=\\xi^T\\left(\\sum_{i=1}^n \\alpha_i(s)\\big(A_i-\\bar{A}_\\alpha\\big)^{\\otimes 2}\\right)^{-1}\\sum_{i=1}^n \\alpha_i(s)\\big(A_i-\\bar{A}_\\alpha\\big)\\big(R_i-\\bar{R}_\\alpha\\big),\n",
    "\\end{equation}\n",
    "where $\\bar{A}_\\alpha=\\sum \\alpha_i(s)A_i$ and $\\bar{R}_\\alpha=\\sum \\alpha_i(s)R_i$, and we write $v^{\\otimes 2}=vv^T$.\n",
    "\n",
    "Notice that this formula is just a weighted version of R-learner introduced above. However, instead of using ordinary kernel weighting functions that are prone to a strong curse of dimensionality, GRF uses an adaptive weighting function $\\alpha_i(s)$ derived from a forest designed to express heterogeneity in the specified quantity of interest. \n",
    "    \n",
    "To be more specific, in order to obtain $\\alpha_i(s)$, GRF first grows a set of $B$ trees indexed by $1,\\dots,B$. Then for each such tree, define $L_b(s)$ as the set of training samples falling in the same ``leaf\" as x. The weights $\\alpha_i(s)$ then capture the frequency with which the $i$-th training example falls into the same leaf as $s$:\n",
    "\\begin{equation}\n",
    "  \\alpha_{bi}(s)=\\frac{\\boldsymbol{1}\\big(\\{S_i\\in L_b(s)\\}\\big)}{\\big|L_b(s)\\big|},\\quad \\alpha_i(s)=\\frac{1}{B}\\sum_{b=1}^B \\alpha_{bi}(s).\n",
    "\\end{equation}\n",
    "\n",
    "To sum up, GRF aims to leverage the splitting result of a series of trees to decide the ``localized” weight for HTE estimation at each point $x_0$. Compared with kernel functions, we may expect tree-based weights to be more flexible and better performed in real settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gk0nYH559XIL"
   },
   "outputs": [],
   "source": [
    "# example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1098b550"
   },
   "source": [
    "## References\n",
    "1. Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 4156–4165.\n",
    "\n",
    "2. Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299–319, 2021.\n",
    "\n",
    "3. Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econometric Society, pages 931–954, 1988.\n",
    "\n",
    "4. Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020\n",
    "\n",
    "5. M. J. van der Laan. Statistical inference for variable importance. The International Journal of Biostatistics, 2(1), 2006.\n",
    "\n",
    "6. S. Lee, R. Okui, and Y.-J. Whang. Doubly robust uniform confidence band for the conditional average treatment effect function. Journal of Applied Econometrics, 32(7):1207–1225, 2017.\n",
    "\n",
    "7. D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036, 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t5C8Ca6zwatg"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPfEijufHctRhNHNbZtdcZQ",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}