{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112864aa",
   "metadata": {
    "id": "112864aa"
   },
   "source": [
    "# Heterogeneous Treatment Effect Estimation (Single Stage)\n",
    "In the previous section, we've introduced the estimation of average treatment effect, where we aims to estimate the difference of potential outcomes by executing action $A=1$ v.s. $A=0$. That is, \n",
    "\\begin{equation*}\n",
    "\\text{ATE}=\\mathbb{E}[R(1)-R(0)].\n",
    "\\end{equation*}\n",
    "\n",
    "In this section, we will focus on the estimation of heterogeneous treatment effect (HTE), which is also one of the main focuses in causal inference.\n",
    "\n",
    "\n",
    "\n",
    "## Main Idea\n",
    "Let's first consider the single stage setup, where the observed data can be written as a state-action-reward triplet $\\{S_i,A_i,R_i\\}_{i=1}^n$ with a total of $n$ trajectories. Heterogeneous treatment effect, as we can imagine from its terminology, aims to measure the heterogeneity of the treatment effect for different subjects. Specifically, we define HTE as $\\tau(s)$, where\n",
    "\\begin{equation*}\n",
    "\\tau(s)=\\mathbb{E}[R(1)-R(0)|S=s],\n",
    "\\end{equation*}\n",
    "\n",
    "where $S=s$ denotes the state information of a subject. \n",
    "\n",
    "The estimation of HTE is widely used in a lot of real cases such as precision medicine, advertising, recommendation systems, etc. For example, in adversiting system, the company would like to know the impact (such as annual income) of exposing an ad to a group of customers. In this case, $S$ contains all of the information of a specific customer, $A$ denotes the status of ads exposure ($A=1$ means exposed and $A=0$ means not), and $R$ denotes the reward one can observe when assigned to policy $A$. \n",
    "\n",
    "Suppose the ad is a picture of a dress that can lead the customers to a detail page on a shopping website. In this case, females are more likely to be interested to click the picture and look at the detail page of a dress, resulting in a higher conversion rate than males. The difference of customers preference in clothes can be regarded as the heterogeneity of the treatment effect. By looking at the HTE for each customer, we can clearly estimate the reward of ads exposure from a granular level. \n",
    "\n",
    "Another related concept is conditional averge treatment effect, which is defined as\n",
    "\\begin{equation*}\n",
    "\\text{CATE}=\\mathbb{E}[R(1)-R(0)|Z],\n",
    "\\end{equation*}\n",
    "\n",
    "where $Z$ is a collection of states with some specific characsteristics. For example, if the company is interested in the treatment effect of exposing the dress to female customers, $Z$ can be defined as ``female\", and the problem can be addressed under the structure CATE estimation.\n",
    "\n",
    "\n",
    "\n",
    "## Different approaches in single-stage HTE estimation\n",
    "Next, let's briefly summarize some state-of-the-art approaches in estimating the heterogeneous treatment effect. There are several review papers which summarize some commonly-used approaches in literature, some of which are also detailed in the following subsections here. For more details please refer to [1], etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "yfiI-lSZRuOP",
   "metadata": {
    "id": "yfiI-lSZRuOP"
   },
   "source": [
    "### **1. S-learner**\n",
    "\n",
    "\n",
    "The first estimator we would like to introduce is the S-learner, also known as a ``single learner\". This is one of the most foundamental learners in HTE esitmation, and is very easy to implement.\n",
    "\n",
    "Under three common assumptions in causal inference, i.e. (1) consistency, (2) no unmeasured confounders (NUC), (3) positivity assumption, the heterogeneous treatment effect can be identified by the observed data, where\n",
    "\\begin{equation*}\n",
    "\\tau(s)=\\mathbb{E}[R|S,A=1]-\\mathbb{E}[R|S,A=0].\n",
    "\\end{equation*}\n",
    "\n",
    "The basic idea of S-learner is to fit a model for $\\mathbb{E}[R|S,A]$, and then construct a plug-in estimator based on the expression above. Specifically, the algorithm can be summarized as below:\n",
    "\n",
    "**Step 1:**  Estimate the combined response function $\\mu(s,a):=\\mathbb{E}[R|S=s,A=a]$ with any regression algorithm or supervised machine learning methods;\n",
    "\n",
    "**Step 2:**  Estimate HTE by \n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{S-learner}}(s)=\\hat\\mu(s,1)-\\hat\\mu(s,0).\n",
    "\\end{equation*}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eRpP5k9MBtzO",
   "metadata": {
    "id": "eRpP5k9MBtzO"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'lightgbm'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# import related packages\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pyplot \u001b[38;5;28;01mas\u001b[39;00m plt;\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mlightgbm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LGBMRegressor;\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinear_model\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LinearRegression\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util_causaldm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'lightgbm'"
     ]
    }
   ],
   "source": [
    "# import related packages\n",
    "from matplotlib import pyplot as plt;\n",
    "from lightgbm import LGBMRegressor;\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from causaldm._util_causaldm import *;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lovM_twTxuOj",
   "metadata": {
    "id": "lovM_twTxuOj"
   },
   "outputs": [],
   "source": [
    "n = 10**3  # sample size in observed data\n",
    "n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n",
    "seed=223"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "JhfJntzcVVy2",
   "metadata": {
    "id": "JhfJntzcVVy2"
   },
   "outputs": [],
   "source": [
    "# Get data\n",
    "data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n",
    "#data_target = get_data_simulation(n0, seed, policy=\"target\")\n",
    "\n",
    "# The true expected heterogeneous treatment effect\n",
    "HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "F4yvhP_yJ9DH",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674886535547,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "F4yvhP_yJ9DH",
    "outputId": "63cbd2fe-e619-4465-8e1e-471e1eafc156"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "  <div id=\"df-1e5d17ce-a8ef-4495-b1c4-234bc6cd91d1\">\n",
       "    <div class=\"colab-df-container\">\n",
       "      <div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>S2</th>\n",
       "      <th>A</th>\n",
       "      <th>R</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.034775</td>\n",
       "      <td>2.453145</td>\n",
       "      <td>1</td>\n",
       "      <td>7.167637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.084880</td>\n",
       "      <td>-1.234459</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.553798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.144626</td>\n",
       "      <td>2.040543</td>\n",
       "      <td>1</td>\n",
       "      <td>5.956732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.148426</td>\n",
       "      <td>-0.021139</td>\n",
       "      <td>1</td>\n",
       "      <td>1.095578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.120852</td>\n",
       "      <td>1.377594</td>\n",
       "      <td>1</td>\n",
       "      <td>4.323133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>-2.022440</td>\n",
       "      <td>1.887551</td>\n",
       "      <td>0</td>\n",
       "      <td>6.797542</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>0.411179</td>\n",
       "      <td>-1.655833</td>\n",
       "      <td>0</td>\n",
       "      <td>-2.722846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>0.155706</td>\n",
       "      <td>-0.992197</td>\n",
       "      <td>0</td>\n",
       "      <td>-1.140100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>-1.510241</td>\n",
       "      <td>0.828438</td>\n",
       "      <td>0</td>\n",
       "      <td>4.167118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>-1.744187</td>\n",
       "      <td>0.857147</td>\n",
       "      <td>0</td>\n",
       "      <td>4.458481</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 4 columns</p>\n",
       "</div>\n",
       "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1e5d17ce-a8ef-4495-b1c4-234bc6cd91d1')\"\n",
       "              title=\"Convert this dataframe to an interactive table.\"\n",
       "              style=\"display:none;\">\n",
       "\n",
       "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
       "       width=\"24px\">\n",
       "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
       "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
       "  </svg>\n",
       "      </button>\n",
       "\n",
       "  <style>\n",
       "    .colab-df-container {\n",
       "      display:flex;\n",
       "      flex-wrap:wrap;\n",
       "      gap: 12px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert {\n",
       "      background-color: #E8F0FE;\n",
       "      border: none;\n",
       "      border-radius: 50%;\n",
       "      cursor: pointer;\n",
       "      display: none;\n",
       "      fill: #1967D2;\n",
       "      height: 32px;\n",
       "      padding: 0 0 0 0;\n",
       "      width: 32px;\n",
       "    }\n",
       "\n",
       "    .colab-df-convert:hover {\n",
       "      background-color: #E2EBFA;\n",
       "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
       "      fill: #174EA6;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert {\n",
       "      background-color: #3B4455;\n",
       "      fill: #D2E3FC;\n",
       "    }\n",
       "\n",
       "    [theme=dark] .colab-df-convert:hover {\n",
       "      background-color: #434B5C;\n",
       "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
       "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
       "      fill: #FFFFFF;\n",
       "    }\n",
       "  </style>\n",
       "\n",
       "      <script>\n",
       "        const buttonEl =\n",
       "          document.querySelector('#df-1e5d17ce-a8ef-4495-b1c4-234bc6cd91d1 button.colab-df-convert');\n",
       "        buttonEl.style.display =\n",
       "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
       "\n",
       "        async function convertToInteractive(key) {\n",
       "          const element = document.querySelector('#df-1e5d17ce-a8ef-4495-b1c4-234bc6cd91d1');\n",
       "          const dataTable =\n",
       "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
       "                                                     [key], {});\n",
       "          if (!dataTable) return;\n",
       "\n",
       "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
       "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
       "            + ' to learn more about interactive tables.';\n",
       "          element.innerHTML = '';\n",
       "          dataTable['output_type'] = 'display_data';\n",
       "          await google.colab.output.renderOutput(dataTable, element);\n",
       "          const docLink = document.createElement('div');\n",
       "          docLink.innerHTML = docLinkHtml;\n",
       "          element.appendChild(docLink);\n",
       "        }\n",
       "      </script>\n",
       "    </div>\n",
       "  </div>\n"
      ],
      "text/plain": [
       "           S1        S2  A         R\n",
       "0    0.034775  2.453145  1  7.167637\n",
       "1    0.084880 -1.234459  0 -1.553798\n",
       "2   -0.144626  2.040543  1  5.956732\n",
       "3    0.148426 -0.021139  1  1.095578\n",
       "4   -0.120852  1.377594  1  4.323133\n",
       "..        ...       ... ..       ...\n",
       "995 -2.022440  1.887551  0  6.797542\n",
       "996  0.411179 -1.655833  0 -2.722846\n",
       "997  0.155706 -0.992197  0 -1.140100\n",
       "998 -1.510241  0.828438  0  4.167118\n",
       "999 -1.744187  0.857147  0  4.458481\n",
       "\n",
       "[1000 rows x 4 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Nxf-mXJmFrEl",
   "metadata": {
    "id": "Nxf-mXJmFrEl"
   },
   "outputs": [],
   "source": [
    "SandA = data_behavior.iloc[:,0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5G8dAwM-PGO",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1674886537525,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "h5G8dAwM-PGO",
    "outputId": "42ee6b7c-003c-4df1-a24f-a865a52c73e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# S-learner\n",
    "S_learner = LGBMRegressor(max_depth=5)\n",
    "#S_learner = LinearRegression()\n",
    "#SandA = np.hstack((S.to_numpy(),A.to_numpy().reshape(-1,1)))\n",
    "S_learner.fit(SandA, data_behavior['R'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Vqsb5wLTaR0q",
   "metadata": {
    "id": "Vqsb5wLTaR0q"
   },
   "outputs": [],
   "source": [
    "HTE_S_learner = S_learner.predict(np.hstack(( data_behavior.iloc[:,0:2].to_numpy(),np.ones(n).reshape(-1,1)))) - S_learner.predict(np.hstack(( data_behavior.iloc[:,0:2].to_numpy(),np.zeros(n).reshape(-1,1))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "FA-F8Jc_T5Lz",
   "metadata": {
    "id": "FA-F8Jc_T5Lz"
   },
   "source": [
    "To evaluate how well S-learner is in estimating heterogeneous treatment effect, we compare its estimates with the true value for the first 10 subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "GvHnTOxmT5Lz",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1674886538906,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "GvHnTOxmT5Lz",
    "outputId": "a810e5e0-8009-421c-fdc4-61ac2f009461"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S-learner:   [-0.1492  0.1687 -0.589  -0.0319 -0.8354 -0.5843 -0.4577 -2.0791]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"S-learner:  \",HTE_S_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g9NgyjQ7PqRf",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1674886540572,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "g9NgyjQ7PqRf",
    "outputId": "fa861f34-db34-4766-cc95-aa7448c0f12f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of S-learner is :      0.2857192464627009 , \n",
      " The overall estimation variance of S-learner is : 4.079505077680185 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_S_learner = np.sum(HTE_S_learner-HTE_true)/n\n",
    "Variance_S_learner = np.sum((HTE_S_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of S-learner is :     \", Bias_S_learner, \", \\n\", \"The overall estimation variance of S-learner is :\",Variance_S_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mVAZTZYTUKJ6",
   "metadata": {
    "id": "mVAZTZYTUKJ6"
   },
   "source": [
    "**Conclusion:** The performance of S-learner, at least in this toy example, is not very attractive. Although it is the easiest approach to implement, the over-simplicity tends to cover some information that can be better explored with some advanced approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cMny8Ri7RvqC",
   "metadata": {
    "id": "cMny8Ri7RvqC"
   },
   "source": [
    "\n",
    "### **2. T-learner**\n",
    "The second learner is called T-learner, which denotes ``two learners\". Instead of fitting a single model to estimate the potential outcomes under both treatment and control groups, T-learner aims to learn different models for $\\mathbb{E}[R(1)|S]$ and $\\mathbb{E}[R(0)|S]$ separately, and finally combines them to obtain a final HTE estimator.\n",
    "\n",
    "Define the control response function as $\\mu_0(s)=\\mathbb{E}[R(0)|S=s]$, and the treatment response function as $\\mu_1(s)=\\mathbb{E}[R(1)|S=s]$. The algorithm of T-learner is summarized below:\n",
    "\n",
    "**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods;\n",
    "\n",
    "**Step 2:**  Estimate HTE by \n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{T-learner}}(s)=\\hat\\mu_1(s)-\\hat\\mu_0(s).\n",
    "\\end{equation*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "X1VmlNjstdsN",
   "metadata": {
    "id": "X1VmlNjstdsN"
   },
   "outputs": [],
   "source": [
    "mu0 = LGBMRegressor(max_depth=3)\n",
    "mu1 = LGBMRegressor(max_depth=3)\n",
    "\n",
    "mu0.fit(data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2],data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] )\n",
    "mu1.fit(data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2],data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] )\n",
    "\n",
    "\n",
    "# estimate the HTE by T-learner\n",
    "HTE_T_learner = mu1.predict(data_behavior.iloc[:,0:2]) - mu0.predict(data_behavior.iloc[:,0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "CUv_0SuBTi3e",
   "metadata": {
    "id": "CUv_0SuBTi3e"
   },
   "source": [
    "Now let's take a glance at the performance of T-learner by comparing it with the true value for the first 10 subjects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5OHVneDpTgMp",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1674886545022,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "5OHVneDpTgMp",
    "outputId": "ca6350c6-f489-4ad4-83b0-7faa09da733c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T-learner:   [ 1.869   1.8733  0.6596  0.3087 -0.2298 -0.5598 -2.2745 -1.8211]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"T-learner:  \",HTE_T_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Ux89PwJagR5_",
   "metadata": {
    "id": "Ux89PwJagR5_"
   },
   "source": [
    "This is quite good! T-learner captures the overall trend of the treatment effect w.r.t. the heterogeneity of different subjects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "SW8ONIdFPpvM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1674886545618,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "SW8ONIdFPpvM",
    "outputId": "591d494b-d567-44f6-84ff-0fdd6dcedc80"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of T-learner is :      0.29138198450323705 , \n",
      " The overall estimation variance of T-learner is : 1.810391408711312 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_T_learner = np.sum(HTE_T_learner-HTE_true)/n\n",
    "Variance_T_learner = np.sum((HTE_T_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of T-learner is :     \", Bias_T_learner, \", \\n\", \"The overall estimation variance of T-learner is :\",Variance_T_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vOsw-rfxU415",
   "metadata": {
    "id": "vOsw-rfxU415"
   },
   "source": [
    "**Conclusion:** In this toy example, the overall estimation variance of T-learner is smaller than that of S-learner. In some cases when the treatment effect is relatively complex, it's likely to yield better performance by fitting two models separately. \n",
    "\n",
    "However, in an extreme case when both $\\mu_0(s)$ and $\\mu_1(s)$ are nonlinear complicated function of state $s$ while their difference is just a constant, T-learner will overfit each model very easily, yielding a nonlinear treatment effect estimator. In this case, other estimators are often preferred."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8lwheJQ8RxAw",
   "metadata": {
    "id": "8lwheJQ8RxAw"
   },
   "source": [
    "### **3. X-learner**\n",
    "Next, let's introduce the X-learner. As a combination of S-learner and T-learner, the X-learner can use information from the control(treatment) group to derive better estimators for the treatment(control) group, which is provably more efficient than the above two.\n",
    "\n",
    "The basic\n",
    "\n",
    "\n",
    "**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods (same as T-learner);\n",
    "\n",
    "\n",
    "**Step 2:**  Obtain the imputed treatment effects for individuals\n",
    "\\begin{equation*}\n",
    "\\tilde{\\Delta}_i^1:=R_i^1-\\hat\\mu_0(S_i^1), \\quad \\tilde{\\Delta}_i^0:=\\hat\\mu_1(S_i^0)-R_i^0.\n",
    "\\end{equation*}\n",
    "\n",
    "**Step 3:**  Fit the imputed treatment effects to obtain $\\hat\\tau_1(s):=\\mathbb{E}[\\tilde{\\Delta}_i^1|S=s]$ and $\\hat\\tau_0(s):=\\mathbb{E}[\\tilde{\\Delta}_i^0|S=s]$;\n",
    "\n",
    "**Step 4:**  The final HTE estimator is given by\n",
    "\\begin{equation*}\n",
    "\\hat{\\tau}_{\\text{X-learner}}(s)=g(s)\\hat\\tau_0(s)+(1-g(s))\\hat\\tau_1(s),\n",
    "\\end{equation*}\n",
    "\n",
    "where $g(s)$ is a weight function between $[0,1]$. A possible way is to use the propensity score model as an estimate of $g(s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sfb-mplOP9HJ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1674886549285,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "sfb-mplOP9HJ",
    "outputId": "77718744-99ae-4b5a-cf6e-b2ed16ae837d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 1: Fit two models under treatment and control separately, same as T-learner\n",
    "\n",
    "import numpy as np\n",
    "mu0 = LGBMRegressor(max_depth=3)\n",
    "mu1 = LGBMRegressor(max_depth=3)\n",
    "\n",
    "S_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2]\n",
    "S_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2]\n",
    "R_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] \n",
    "R_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] \n",
    "\n",
    "mu0.fit(S_T0, R_T0)\n",
    "mu1.fit(S_T1, R_T1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zb42ZMw3pkqm",
   "metadata": {
    "id": "zb42ZMw3pkqm"
   },
   "outputs": [],
   "source": [
    "# Step 2: impute the potential outcomes that are unobserved in original data\n",
    "\n",
    "n_T0 = len(R_T0)\n",
    "n_T1 = len(R_T1)\n",
    "\n",
    "Delta0 = mu1.predict(S_T0) - R_T0\n",
    "Delta1 = R_T1 - mu0.predict(S_T1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pxYLjE0Ar2_5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1674886552480,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "pxYLjE0Ar2_5",
    "outputId": "a00f4b4b-166e-49f3-9162-73f6f3f6a590"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMRegressor(max_depth=2)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Step 3: Fit tau_1(s) and tau_0(s)\n",
    "\n",
    "tau0 = LGBMRegressor(max_depth=2)\n",
    "tau1 = LGBMRegressor(max_depth=2)\n",
    "\n",
    "tau0.fit(S_T0, Delta0)\n",
    "tau1.fit(S_T1, Delta1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "LRvEZ4uluT-U",
   "metadata": {
    "id": "LRvEZ4uluT-U"
   },
   "outputs": [],
   "source": [
    "# Step 4: fit the propensity score model $\\hat{g}(s)$ and obtain the final HTE estimator by taking weighted average of tau0 and tau1\n",
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "g = LogisticRegression()\n",
    "g.fit(data_behavior.iloc[:,0:2],data_behavior['A'])\n",
    "\n",
    "HTE_X_learner = g.predict_proba(data_behavior.iloc[:,0:2])[:,0]*tau0.predict(data_behavior.iloc[:,0:2]) + g.predict_proba(data_behavior.iloc[:,0:2])[:,1]*tau1.predict(data_behavior.iloc[:,0:2])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Mz8I_J0h4N6B",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1674886656726,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "Mz8I_J0h4N6B",
    "outputId": "cf1fd349-34e0-47f3-d532-3f5c12282d2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X-learner:   [ 1.9341  1.9235  0.2944  0.2013 -0.4147 -0.5626 -2.214  -1.5443]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"X-learner:  \",HTE_X_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "XyxjzDBy4N6k",
   "metadata": {
    "id": "XyxjzDBy4N6k"
   },
   "source": [
    "From the result above we can see that X-learner can roughly catch the trend of treatment effect w.r.t. the change of baseline information $S$. In this synthetic example, X-learner also performs slightly better than T-learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rlbacETd4N6l",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 344,
     "status": "ok",
     "timestamp": 1674886659778,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "rlbacETd4N6l",
    "outputId": "31c64ed7-ff61-45ce-bdce-c0276ce1ef31"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of X-learner is :      0.2827518068171628 , \n",
      " The overall estimation variance of X-learner is : 1.7686646616779012 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_X_learner = np.sum(HTE_X_learner-HTE_true)/n\n",
    "Variance_X_learner = np.sum((HTE_X_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of X-learner is :     \", Bias_X_learner, \", \\n\", \"The overall estimation variance of X-learner is :\",Variance_X_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EnxQ_Tkg4o_q",
   "metadata": {
    "id": "EnxQ_Tkg4o_q"
   },
   "source": [
    "**Conclusion:** In this toy example, the overall estimation variance of X-learner is the smallest, followed by T-learner, and the worst is given by S-learner."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32szzPY4RyWO",
   "metadata": {
    "id": "32szzPY4RyWO"
   },
   "source": [
    "### **4. R learner**\n",
    "The idea of classical R-learner came from Robinson 1988 [3] and was formalized by Nie and Wager in 2020 [2]. The main idea of R learner starts from the partially linear model setup, in which we assume that\n",
    "\\begin{equation}\n",
    "  \\begin{aligned}\n",
    "    R&=A\\tau(S)+g_0(S)+U,\\\\\n",
    "    A&=m_0(S)+V,\n",
    "  \\end{aligned}\n",
    "\\end{equation}\n",
    "where $U$ and $V$ satisfies $\\mathbb{E}[U|D,X]=0$, $\\mathbb{E}[V|X]=0$.\n",
    "\n",
    "After several manipulations, it’s easy to get\n",
    "\\begin{equation}\n",
    "\tR-\\mathbb{E}[R|S]=\\tau(S)\\cdot(A-\\mathbb{E}[A|S])+\\epsilon.\n",
    "\\end{equation}\n",
    "Define $m_0(X)=\\mathbb{E}[A|S]$ and $l_0(X)=\\mathbb{E}[R|S]$. A natural way to estimate $\\tau(X)$ is given below, which is also the main idea of R-learner:\n",
    "\n",
    "**Step 1**: Regress $R$ on $S$ to obtain model $\\hat{\\eta}(S)=\\hat{\\mathbb{E}}[R|S]$; and regress $A$ on $S$ to obtain model $\\hat{m}(S)=\\hat{\\mathbb{E}}[A|S]$.\n",
    "\n",
    "**Step 2**: Regress outcome residual $R-\\hat{l}(S)$ on propensity score residual $A-\\hat{m}(S)$.\n",
    "\n",
    "That is,\n",
    "\\begin{equation}\n",
    "\t\\hat{\\tau}(S)=\\arg\\min_{\\tau}\\left\\{\\mathbb{E}_n\\left[\\left(\\{R_i-\\hat{\\eta}(S_i)\\}-\\{A_i-\\hat{m}(S_i)\\}\\cdot\\tau(S_i)\\right)^2\\right]\\right\\}\t\n",
    "\\end{equation}\n",
    "\n",
    "The easiest way to do so is to specify $\\hat{\\tau}(S)$ to the linear function class. In this case, $\\tau(S)=S\\beta$, and the problem becomes to estimate $\\beta$ by solving the following linear regression:\n",
    "\\begin{equation}\n",
    "\t\\hat{\\beta}=\\arg\\min_{\\beta}\\left\\{\\mathbb{E}_n\\left[\\left(\\{R_i-\\hat{\\eta}(S_i)\\}-\\{A_i-\\hat{m}(S_i)\\} S_i\\cdot \\beta\\right)^2\\right]\\right\\}.\n",
    "\\end{equation}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hQNnTeMk9A56",
   "metadata": {
    "id": "hQNnTeMk9A56"
   },
   "outputs": [],
   "source": [
    "# a demo code of R-learner\n",
    "\n",
    "def Rlearner(df, outcome, treatment, controls, n_folds, y_model, ps_model, Rlearner_model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.dataframe\n",
    "        data\n",
    "    outcome : str\n",
    "        outcome label.\n",
    "    treatment : str\n",
    "        treatment label.\n",
    "    controls : list\n",
    "        list of all controls.\n",
    "    n_folds : int\n",
    "        number of folds for cross-fitting.\n",
    "    y_model : sklearn class\n",
    "        the model for outcome regression learner.\n",
    "    ps_model : sklearn class\n",
    "        the model for general propensity score learner.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Rlearner_pred : Length: n, dtype: float64\n",
    "        Estimated Heterogeneous Treatemnt Effect by Simple R-learner with linear regression\n",
    "    \"\"\"\n",
    "\n",
    "    # =============================================================================\n",
    "    # # estimate with R-learner\n",
    "    # =============================================================================\n",
    "\n",
    "    print('estimate with R-learner')\n",
    "\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    # estimate p(x) by GBDT(Gradient Boosting Decision Tree)\n",
    "    # estimate m(x) by Random Forest\n",
    "    n_controls=len(controls)\n",
    "    folds=np.random.randint(1,n_folds+1,size=df.shape[0])\n",
    "    \n",
    "    y_learner=[y_model]*n_folds\n",
    "    ps_learner=[ps_model]*n_folds\n",
    "\n",
    "\n",
    "    y_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "    ps_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "    \n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        fold=i+1\n",
    "        #y_learner for outcome prediction\n",
    "        y_learner[i].fit(df[folds!=fold][controls],df[folds!=fold][outcome])\n",
    "        y_pred.loc[folds==fold]=y_learner[i].predict(df[folds==fold][controls])\n",
    "\n",
    "        #ps_learner for propensity score prediction\n",
    "        ps_learner[i].fit(df[folds!=fold][controls],df[folds!=fold][treatment])\n",
    "        ps_pred.loc[folds==fold]=ps_learner[i].predict_proba(df[folds==fold][controls])[:,1]\n",
    "\n",
    "        #model performance output\n",
    "        print('fold {},testing r2 y_learner: {:.3f}, ps_learner: {:.3f}'.format(fold, \n",
    "                        y_learner[i].score(df[folds==fold][controls],df[folds==fold][outcome]),\n",
    "                        ps_learner[i].score(df[folds==fold][controls],df[folds==fold][treatment])\n",
    "                                            ))\n",
    "      \n",
    "    x_residual=df[controls]\n",
    "    x_residual['Intercept']=1\n",
    "    \n",
    "    y_residual=df[outcome]-y_pred\n",
    "    ps_residual=df[treatment]-ps_pred\n",
    "    x_tilde=ps_residual.to_numpy().reshape(-1,1)*(x_residual.to_numpy())\n",
    "    \n",
    "    data=pd.DataFrame(x_tilde)\n",
    "    data['y_residual']=y_residual\n",
    "    \n",
    "    # R learner: conducting regressison on residuals: (Y-y_pred)~(A-ps_pred)*X'*beta\n",
    "    # any parametric/nonparametric regression method is fine\n",
    "    Rlearner_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "\n",
    "    #Rlearner_model=GradientBoostingRegressor(n_estimators=50, max_depth=5)\n",
    "    #Rlearner_model=LinearRegression(fit_intercept=False) # almost failed: testing r2 R-learner: 0.041\n",
    "    #Rlearner_model=ElasticNet() # almost failed\n",
    "    #Rlearner_model=Lasso() # almost failed\n",
    "    R_learner=[Rlearner_model]*n_folds   \n",
    "    \n",
    "    for i in range(n_folds):\n",
    "        fold=i+1\n",
    "        #R_learner for residual regression\n",
    "        R_learner[i].fit(data[folds!=fold][range(n_controls+1)],data[folds!=fold]['y_residual'])\n",
    "        Rlearner_pred.loc[folds==fold]=R_learner[i].predict(x_residual[folds==fold])\n",
    "\n",
    "        #model performance output\n",
    "        print('fold {}, training r2 R-learner: {:.3f}, testing r2 R-learner: {:.3f}'.format(fold, R_learner[i].score(data[folds!=fold][range(n_controls+1)],data[folds!=fold]['y_residual']), R_learner[i].score(data[folds==fold][range(n_controls+1)],data[folds==fold]['y_residual'])  ))\n",
    "    \n",
    "    return Rlearner_pred\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jYIe491FKQAs",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 599,
     "status": "ok",
     "timestamp": 1674887417554,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "jYIe491FKQAs",
    "outputId": "bff8cb7b-dbd9-4007-d77d-3ab20409a8d3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate with R-learner\n",
      "fold 1,testing r2 y_learner: 0.943, ps_learner: 0.927\n",
      "fold 2,testing r2 y_learner: 0.961, ps_learner: 0.952\n",
      "fold 3,testing r2 y_learner: 0.961, ps_learner: 0.965\n",
      "fold 4,testing r2 y_learner: 0.943, ps_learner: 0.936\n",
      "fold 5,testing r2 y_learner: 0.954, ps_learner: 0.940\n",
      "fold 1, training r2 R-learner: 0.663, testing r2 R-learner: 0.506\n",
      "fold 2, training r2 R-learner: 0.642, testing r2 R-learner: 0.655\n",
      "fold 3, training r2 R-learner: 0.657, testing r2 R-learner: 0.550\n",
      "fold 4, training r2 R-learner: 0.698, testing r2 R-learner: 0.425\n",
      "fold 5, training r2 R-learner: 0.638, testing r2 R-learner: 0.700\n"
     ]
    }
   ],
   "source": [
    "# R-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model = LogisticRegression()\n",
    "Rlearner_model = LGBMRegressor(max_depth=2)\n",
    "\n",
    "HTE_R_learner = Rlearner(data_behavior, outcome, treatment, controls, n_folds, y_model, ps_model, Rlearner_model)\n",
    "HTE_R_learner = HTE_R_learner.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "D_B2JzoeEVkM",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 331,
     "status": "ok",
     "timestamp": 1674887445154,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "D_B2JzoeEVkM",
    "outputId": "9550e64d-4d25-441f-f105-514a6846a9d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R-learner:   [-0.4971  0.0231 -1.0514 -0.0037 -1.0943 -1.4128 -1.1436 -1.4714]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"R-learner:  \",HTE_R_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2FvnH_FtEVkj",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 292,
     "status": "ok",
     "timestamp": 1674887465909,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "2FvnH_FtEVkj",
    "outputId": "dd2383fd-395c-4204-9eb0-346c39882475"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of R-learner is :      0.010664510462813687 , \n",
      " The overall estimation variance of R-learner is : 3.3201771635462656 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_R_learner = np.sum(HTE_R_learner-HTE_true)/n\n",
    "Variance_R_learner = np.sum((HTE_R_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of R-learner is :     \", Bias_R_learner, \", \\n\", \"The overall estimation variance of R-learner is :\",Variance_R_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "EWhausRhExr5",
   "metadata": {
    "id": "EWhausRhExr5"
   },
   "source": [
    "**Conclusion:** It's amazing to see that the bias of R-learner is significantly smaller than all other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "J2z2JRumRzdo",
   "metadata": {
    "id": "J2z2JRumRzdo"
   },
   "source": [
    "### **5. DR-learner**\n",
    "\n",
    "DR-learner is a two-stage doubly robust estimator for HTE estimation. Before Kennedy et al. 2020 [4], there are several related approaches trying to extend the doubly robust procedure to HTE estimation, such as [5, 6, 7]. Compared with the above three estimators, DR-learner is proved to be oracle efficient under some mild assumptions detailed in Theorem 2 of [4].\n",
    "\n",
    "The basic steps of DR-learner is given below:\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1}^n$ to construct estimates $\\hat{\\pi}$ for the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1}^n$ to construct estimates $\\hat\\mu_a(s)$ for $\\mu_a(s):=\\mathbb{E}[R|S=s,A=a]$;\n",
    "\n",
    "**Step 2**: Pseudo-outcome regression: \\\\\n",
    "Define $\\widehat{\\phi}(Z)$ as the pseudo-outcome where \n",
    "\\begin{equation}\n",
    "\\widehat{\\phi}(Z)=\\frac{A-\\hat{\\pi}(S)}{\\hat{\\pi}(S)\\{1-\\hat{\\pi}(S)\\}}\\Big\\{R-\\hat{\\mu}_A(S)\\Big\\}+\\hat{\\mu}_1(S)-\\hat{\\mu}_0(S),\n",
    "\\end{equation}\n",
    "and regress it on covariates $S$ in the test sample $I_2^n$, yielding \n",
    "\\begin{equation}\n",
    "\\widehat{\\tau}_{\\text{DR-learner}}(s)=\\widehat{\\mathbb{E}}_n[\\widehat{\\phi}(Z)|S=s].\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "RVkkilMalB4p",
   "metadata": {
    "id": "RVkkilMalB4p"
   },
   "outputs": [],
   "source": [
    "# A demo code of DR-learner\n",
    "\n",
    "def DRlearner(df, outcome, treatment, controls, y_model, ps_model, n_folds=5):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.dataframe\n",
    "        data\n",
    "    outcome : str\n",
    "        outcome label.\n",
    "    treatment : str\n",
    "        treatment label.\n",
    "    controls : list\n",
    "        list of all controls.\n",
    "    y_model : sklearn class\n",
    "        the model for outcome regression learner.\n",
    "    ps_model : sklearn class\n",
    "        the model for general propensity score learner.\n",
    "    n_folds : int\n",
    "        number of folds for cross-fitting.\n",
    "    Returns\n",
    "    -------\n",
    "    TE_DR : Length: n, dtype: float64\n",
    "        Estimated Heterogeneous Treatemnt Effect by DR-learner\n",
    "    \"\"\"\n",
    "    # =============================================================================\n",
    "    # # estimate with DR-learner\n",
    "    # =============================================================================\n",
    "    print('estimate with DR-learner')\n",
    "\n",
    "    import pandas as pd\n",
    "    import subprocess,os,pdb\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "    from sklearn.linear_model import Lasso,ElasticNet\n",
    "    from scipy import stats\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import Lasso,ElasticNet\n",
    "\n",
    "    import pdb\n",
    "    import numpy as np\n",
    "    from scipy.sparse import diags\n",
    "    \n",
    "    dt_len=np.shape(df)[0]\n",
    "    \n",
    "    np.random.seed(525)\n",
    "    folds=np.random.randint(1,n_folds+1,size=df.shape[0])\n",
    "    \n",
    "    y_learner=[y_model]*n_folds\n",
    "    ps_learner=[ps_model]*n_folds\n",
    "    \n",
    "    y_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "    ps_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "\n",
    "    Y1_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "    Y0_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "    ps_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "\n",
    "    df['T_1']=1\n",
    "    df['T_0']=0\n",
    "    \n",
    "    \n",
    "    # estimate classical DR \n",
    "    for i in range(n_folds):\n",
    "        fold=i+1\n",
    "        #baselearner for outcome prediction\n",
    "        y_learner[i].fit(df[folds!=fold][controls+[treatment]],df[folds!=fold][outcome])\n",
    "\n",
    "\n",
    "        Y1_pred.loc[folds==fold]=y_learner[i].predict(df[folds==fold][controls+['T_1']])\n",
    "        Y0_pred.loc[folds==fold]=y_learner[i].predict(df[folds==fold][controls+['T_0']])\n",
    "\n",
    "        ps_learner[i].fit(df[folds!=fold][controls],df[folds!=fold][treatment])\n",
    "\n",
    "        ps_pred.loc[folds==fold]=ps_learner[i].predict_proba(df[folds==fold][controls])[:,1]\n",
    "\n",
    "        print('fold {}, testing r2 baselearner: {:.3f}, pslearner: {:.3f}'.format(fold, \n",
    "                        y_learner[i].score(df[folds!=fold][controls+[treatment]],df[folds!=fold][outcome]),\n",
    "                        ps_learner[i].score(df[folds!=fold][controls],df[folds!=fold][treatment])\n",
    "                                            ))\n",
    "\n",
    "\n",
    "\n",
    "    #gps_pred[np.where(gps_pred<1e-2)[0]]=1e-2\n",
    "    #gps_pred[np.where(gps_pred>1-1e-2)[0]]=1-1e-2\n",
    "\n",
    "    \n",
    "    # DR estimator\n",
    "    TE_DR=Y1_pred-Y0_pred+df[treatment]*(df[outcome]-Y1_pred)/ps_pred-(1-df[treatment])*(df[outcome]-Y0_pred)/(1-ps_pred)\n",
    "    \n",
    "    \n",
    "    return TE_DR\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i_F-3H7NFBHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 429,
     "status": "ok",
     "timestamp": 1674887880730,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "i_F-3H7NFBHZ",
    "outputId": "f5540588-bcd6-4a5d-ead2-b2ee4123f4f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "estimate with DR-learner\n",
      "fold 1, testing r2 baselearner: 0.980, pslearner: 0.943\n",
      "fold 2, testing r2 baselearner: 0.978, pslearner: 0.947\n",
      "fold 3, testing r2 baselearner: 0.975, pslearner: 0.942\n",
      "fold 4, testing r2 baselearner: 0.978, pslearner: 0.946\n",
      "fold 5, testing r2 baselearner: 0.978, pslearner: 0.940\n"
     ]
    }
   ],
   "source": [
    "# DR-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model = LogisticRegression()\n",
    "Rlearner_model = LGBMRegressor(max_depth=2)\n",
    "\n",
    "HTE_DR_learner = DRlearner(data_behavior, outcome, treatment, controls, y_model, ps_model)\n",
    "HTE_DR_learner = HTE_DR_learner.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0-u2xNvpFBHZ",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 302,
     "status": "ok",
     "timestamp": 1674887889142,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "0-u2xNvpFBHZ",
    "outputId": "483ae696-65b5-4597-f61f-b81181f5bb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DR-learner:   [-1.2566  0.0408 -0.8131 -0.0906 -0.5665 -0.7341 -0.6459 -1.272 ]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"DR-learner:  \",HTE_DR_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "Yvb360k8FBHa",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 282,
     "status": "ok",
     "timestamp": 1674887909243,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "Yvb360k8FBHa",
    "outputId": "40eb6c84-483a-439f-f8da-978dddd16049"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of DR-learner is :      0.29436318987432813 , \n",
      " The overall estimation variance of DR-learner is : 4.011818461500106 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_DR_learner = np.sum(HTE_DR_learner-HTE_true)/n\n",
    "Variance_DR_learner = np.sum((HTE_DR_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of DR-learner is :     \", Bias_DR_learner, \", \\n\", \"The overall estimation variance of DR-learner is :\",Variance_DR_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "YgKc3F0cR0Y4",
   "metadata": {
    "id": "YgKc3F0cR0Y4"
   },
   "source": [
    "### **6. Lp-R-learner**\n",
    "\n",
    "As an extension of R-learner, Lp-R-learner combined the idea of residual regression with local polynomial adaptation, and leveraged the idea of cross fitting to further relax the conditions needed to obtain the oracle convergence rate. For brevity of content, we will just introduce their main algorithm. For more details about its theory and real data performance please see the paper written by Kennedy [4]. \n",
    "\t\n",
    "Let $(I_{1a}^n, I_{1b}^n,I_{2}^n)$ denote three independent samples of $n$ observations of $Z_i = (S_i, A_i, R_i)$. Let $b:\\mathbb{R}^d\\rightarrow \\mathbb{R}^p$ denote the vector of basis functions consisting of all powers of each covariate, up to order $\\gamma$, and all interactions up to degree $\\gamma$ polynomials. Let $K_{hs}(S)=\\frac{1}{h^d}K\\left(\\frac{S-s}{h}\\right)$ for $k:\\mathbb{R}^d\\rightarrow \\mathbb{R}$ a bounded kernel function with support $[-1,1]^d$, and $h$ is a bandwidth parameter.\n",
    "\n",
    "**Step 1**: Nuisance training: \\\\\n",
    "(a)  Using $I_{1a}^n$ to construct estimates $\\hat{\\pi}_a$ of the propensity scores $\\pi$; \\\\\n",
    "(b)  Using $I_{1b}^n$ to construct estimates $\\hat{\\eta}$ of the regression function $\\eta=\\pi\\mu_1+(1-\\pi)\\mu_0$, and estimtes $\\hat{\\pi}_b$ of the propensity scores $\\pi$.\n",
    "\n",
    "**Step 2**: Localized double-residual regression: \\\\\n",
    "Define $\\hat{\\tau}_r(s)$ as the fitted value from a kernel-weighted least squares regression (in the test sample $I_2^n$) of outcome residual $(R-\\hat{\\eta})$ on basis terms $b$ scaled by the treatment residual $A-\\hat{\\pi}_b$, with weights $\\Big(\\frac{A-\\hat{\\pi}_a}{A-\\hat{\\pi}_b}\\Big)\\cdot K_{hs}$. Thus $\\hat{\\tau}_r(s)=b(0)^T\\hat{\\theta}$ for\n",
    "\\begin{equation}\n",
    "\t\t\\hat{\\theta}=\\arg\\min_{\\theta\\in\\mathbb{R}^p}\\mathbb{P}_n\\left(K_{hs}(S)\\Big\\{ \\frac{A-\\hat{\\pi}_a(S)}{A-\\hat{\\pi}_b(S)}\\Big\\} \\left[  \\big\\{R-\\hat{\\eta}(S)\\big\\}-\\theta^Tb(S-s_0)\\big\\{A-\\hat{\\pi}_b(S)\\big\\} \\right] \\right).\n",
    "\\end{equation}\n",
    "**Step 3**: Cross-fitting(optional): \\\\\n",
    "Repeat Step 1–2 twice, first using $(I^n_{1b} , I_2^n)$ for nuisance training and $I_{1a}^n$ as the test samplem and then using $(I^n_{1a} , I_2^n)$ for training and $I_{1b}^n$ as the test sample. Use the average of the resulting three estimators of $\\tau$ as the final estimator $\\hat{\\tau}_r$.\n",
    "\n",
    "In the theory section, Kennedy proved that Lp-R-learner, compared with traditional DR learner, can achieve the oracle convergence rate under milder conditions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "HbJkVOVd9hQA",
   "metadata": {
    "id": "HbJkVOVd9hQA"
   },
   "outputs": [],
   "source": [
    "# A demo code of Lp-R-learner\n",
    "\n",
    "def LpRlearner(df, outcome, treatment, controls, y_model, ps_model_a, ps_model_b, s, LpRlearner_model, degree = 1):\n",
    "    \"\"\"\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.dataframe\n",
    "        data\n",
    "    outcome : str\n",
    "        outcome label.\n",
    "    treatment : str\n",
    "        treatment label.\n",
    "    controls : list\n",
    "        list of all controls.\n",
    "    y_model : sklearn class\n",
    "        the model for outcome regression learner.\n",
    "    ps_model_a : sklearn class\n",
    "        the model for general propensity score learner in fold 1a.\n",
    "    ps_model_b : sklearn class\n",
    "        the model for general propensity score learner in fold 1b.\n",
    "        s:  float64\n",
    "            bandwidth of gauss kernel function in deciding the weight of regression\n",
    "   LpRlearner_model:  sklearn class\n",
    "        the model for residual regression learner in fold 2.\n",
    "    n_folds : int\n",
    "        number of folds for cross-fitting. Set as a fixed number, 3, as indicated in the paper    \n",
    "    Returns\n",
    "    -------\n",
    "    LpRlearner_pred : Length: n, dtype: float64\n",
    "        Estimated Heterogeneous Treatemnt Effect by Lp-R-learner with kernel-weighted polynomial regression\n",
    "    \"\"\"\n",
    "    # =============================================================================\n",
    "    # # estimate with Lp-R-learner\n",
    "    # =============================================================================\n",
    "\n",
    "\n",
    "    print('estimate with Lp-R-learner')\n",
    "\n",
    "\n",
    "    from sklearn.ensemble import RandomForestRegressor\n",
    "    from sklearn.linear_model import Lasso,LogisticRegression\n",
    "    from sklearn.metrics import r2_score\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "\n",
    "    from sklearn.metrics import r2_score\n",
    "    from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, GradientBoostingClassifier\n",
    "    from sklearn.linear_model import Lasso,ElasticNet\n",
    "    from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "    n_all = len(df)\n",
    "    \n",
    "    n_folds = 3\n",
    "    folds=np.random.randint(1,n_folds+1,size=df.shape[0])\n",
    "    poly = PolynomialFeatures(degree = degree)\n",
    "    \n",
    "\n",
    "    \n",
    "    tau=np.zeros((n_all,3))\n",
    "    LpRlearner_pred = pd.Series(np.zeros(n_all))\n",
    "\n",
    "    \n",
    "    for j in range(n_all):  \n",
    "        \n",
    "        y_learner=[y_model]*n_folds\n",
    "        ps_learner_a=[ps_model_a]*n_folds\n",
    "        ps_learner_b=[ps_model_b]*n_folds\n",
    "        Lp_R_learner=[LpRlearner_model]*n_folds\n",
    "\n",
    "        y_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "        ps_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "        LpRlearner_pred=pd.Series(index=df.index,dtype=np.float64)\n",
    "\n",
    "        for i in range(n_folds):\n",
    "            fold=i+1\n",
    "\n",
    "            # define the three-folds cross fitting index according to Kennedy's paper\n",
    "            fold1a=fold\n",
    "            fold1b=(fold+1)%n_folds\n",
    "            fold2=(fold+2)%n_folds  \n",
    "            if (fold1a == 0):\n",
    "                fold1a = fold1a + n_folds\n",
    "            if (fold1b == 0):\n",
    "                fold1b = fold1b + n_folds\n",
    "            if (fold2 == 0):\n",
    "                fold2 = fold2 + n_folds\n",
    "\n",
    "                \n",
    "            # step 1: nuisance training\n",
    "            ps_learner_a[fold1a-1].fit(df[folds==fold1a][controls],df[folds==fold1a][treatment])\n",
    "\n",
    "            y_learner[fold1b-1].fit(df[folds==fold1b][controls],df[folds==fold1b][outcome])\n",
    "            ps_learner_b[fold1b-1].fit(df[folds==fold1b][controls],df[folds==fold1b][treatment])\n",
    "\n",
    "            \n",
    "            #1st stage model performance output\n",
    "            #print('fold {},training r2 y_learner: {:.3f}, ps_learner: {:.3f}'.format(fold1a,y_learner[fold1b-1].score(df[folds==fold1b][controls],df[folds==fold1b][outcome]), ps_learner_a[fold1a-1].score(df[folds==fold1a][controls],df[folds==fold1a][treatment])  ))\n",
    "            #print('fold {},testing r2 y_learner: {:.3f}, ps_learner: {:.3f}'.format(fold1a,y_learner[fold1b-1].score(df[folds!=fold1b][controls],df[folds!=fold1b][outcome]),ps_learner_a[fold1a-1].score(df[folds!=fold1a][controls],df[folds!=fold1a][treatment])))\n",
    "            \n",
    "     \n",
    "            x0=df[controls].iloc[j].to_numpy()#.reshape(-1,1)  ############## define another vector in argument line##\n",
    "            X=df[controls][folds==fold2].to_numpy()\n",
    "            \n",
    "            #print(np.shape(X))\n",
    "            n=len(df[folds==fold2])\n",
    "\n",
    "\n",
    "            # choose h to ensure the support to be in between [-1,1]^d\n",
    "            h=0\n",
    "            for k in range(n):\n",
    "                temp=np.max(abs(X[k,:]-x0))\n",
    "                if (temp>h):\n",
    "                    h=temp\n",
    "            h=np.ceil(h)\n",
    "            #print('the value of h is {:.3f}'.format(h) )\n",
    "            \n",
    "            # step 2: kernel-weighted least squares regression\n",
    "            # kernel calculation\n",
    "            # use gauss kernel to determine the weight of regression\n",
    "            Kernel_X=np.exp(-sum( ((x - y)/h)**2 for (x, y) in zip(X.transpose(), x0) ) / s**2)\n",
    "\n",
    "            ps_a=ps_model_a.predict_proba(df[folds==fold2][controls])[:,1]\n",
    "            ps_b=ps_model_b.predict_proba(df[folds==fold2][controls])[:,1]\n",
    "\n",
    "\n",
    "            ps_a[np.where(ps_a<1e-5)]=1e-5\n",
    "            ps_b[np.where(ps_b<1e-5)]=1e-5\n",
    "            ps_a[np.where(1-ps_a<1e-5)]=1-1e-5\n",
    "            ps_b[np.where(1-ps_b<1e-5)]=1-1e-5\n",
    "\n",
    "            weight=Kernel_X * (df[folds==fold2][treatment]-ps_a) / (df[folds==fold2][treatment]-ps_b)\n",
    "\n",
    "            \n",
    "            # polynomial regression at point x0\n",
    "            X_poly = poly.fit_transform(df[folds==fold2][controls]-x0)\n",
    "            y_residual = df[outcome][folds==fold2]-y_model.predict(df[folds==fold2][controls])\n",
    "            x_tilde = X_poly * (df[folds==fold2][treatment]-ps_model_b.predict_proba(df[folds==fold2][controls])[:,1]).to_numpy().reshape(-1,1)\n",
    "            \n",
    "            p = np.shape(X_poly)[1]\n",
    "            \n",
    "            #poly.fit(X_poly_train,y_train)\n",
    "            Lp_R_learner[fold2-1].fit(x_tilde, y_residual, sample_weight=weight)\n",
    "            \n",
    "            Theta = Lp_R_learner[fold2-1].coef_\n",
    "            #LpRlearner_pred[0].loc[folds==(fold+2)]=LpRlearner_model.predict(X_poly)\n",
    "            \n",
    "\n",
    "            #model performance output\n",
    "            X_poly_test = poly.fit_transform(df[folds!=fold2][controls]-x0)\n",
    "            y_residual_test = df[outcome][folds!=fold2]-y_model.predict(df[folds!=fold2][controls])\n",
    "            x_tilde_test = X_poly_test * (df[folds!=fold2][treatment]-ps_model_b.predict_proba(df[folds!=fold2][controls])[:,1]).to_numpy().reshape(-1,1)\n",
    "            #print('fold {},training r2 of Lp-R-learner_model: {:.3f},testing r2 of Lp-R-learner_model: {:.3f}'.format(fold1a, Lp_R_learner[fold2-1].score(x_tilde,y_residual),Lp_R_learner[fold2-1].score(x_tilde_test,y_residual_test)))\n",
    "            \n",
    "            tau[j,i]=Theta[0] #the intercept of the linear regression\n",
    "            \n",
    "    LpRlearner_pred = np.sum(tau,axis=1)/n_folds        \n",
    "            \n",
    "    return LpRlearner_pred\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "DPyK1J2AQvXU",
   "metadata": {
    "id": "DPyK1J2AQvXU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "r703l-q0GeyE",
   "metadata": {
    "id": "r703l-q0GeyE"
   },
   "outputs": [],
   "source": [
    "# Lp-R-learner for HTE estimation\n",
    "outcome = 'R'\n",
    "treatment = 'A'\n",
    "controls = ['S1','S2']\n",
    "n_folds = 5\n",
    "y_model = LGBMRegressor(max_depth=2)\n",
    "ps_model_a = LogisticRegression()\n",
    "ps_model_b = LogisticRegression()\n",
    "s = 1\n",
    "LpRlearner_model = LinearRegression()\n",
    "\n",
    "HTE_Lp_R_learner = LpRlearner(data_behavior, outcome, treatment, controls, y_model, ps_model_a, ps_model_b, s, LpRlearner_model, degree = 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "oZEfhLMIGeyg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1674888500401,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "oZEfhLMIGeyg",
    "outputId": "881f6057-eb19-4e09-876c-3501a6d61f12"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lp_R-learner:   [-0.731   0.4297 -0.751   0.28   -0.4724 -0.5424 -2.8415 -1.7592]\n",
      "true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Lp_R-learner:  \",HTE_Lp_R_learner[0:8])\n",
    "print(\"true value: \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nSGfmmgEGeyg",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 326,
     "status": "ok",
     "timestamp": 1674888502449,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "nSGfmmgEGeyg",
    "outputId": "b5ca1dab-69b8-4fbb-9261-b7a952792fe1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of Lp_R-learner is :      -0.20878981121875018 , \n",
      " The overall estimation variance of Lp_R-learner is : 0.8367172393247593 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_Lp_R_learner = np.sum(HTE_Lp_R_learner-HTE_true)/n\n",
    "Variance_Lp_R_learner = np.sum((HTE_Lp_R_learner-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of Lp_R-learner is :     \", Bias_Lp_R_learner, \", \\n\", \"The overall estimation variance of Lp_R-learner is :\",Variance_Lp_R_learner,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bla210-HHT-E",
   "metadata": {
    "id": "bla210-HHT-E"
   },
   "source": [
    "**Conclusion**: It will cost more time to use Lp-R-learner than other approaches. However, the overall estimation variance of Lp-R-learner is incredibly smaller than other approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "SgWh47pKR1XR",
   "metadata": {
    "id": "SgWh47pKR1XR"
   },
   "source": [
    "### **7. Generalized Random Forest**\n",
    "\n",
    "Developed by Susan Athey, Julie Tibshirani and Stefan Wager, Generalized Random Forest [8] aims to give the solution to a set of local moment equations:\n",
    "\\begin{equation}\n",
    "  \\mathbb{E}\\big[\\psi_{\\tau(s),\\nu(s)}(O_i)\\big| S_i=s\\big]=0,\n",
    "\\end{equation}\n",
    "where $\\tau(s)$ is the parameter we care about and $\\nu(s)$ is an optional nuisance parameter. In the problem of Heterogeneous Treatment Effect Evaluation, our parameter of interest $\\tau(s)=\\xi\\cdot \\beta(s)$ is identified by \n",
    "\\begin{equation}\n",
    "  \\psi_{\\beta(s),\\nu(s)}(R_i,A_i)=(R_i-\\beta(s)\\cdot A_i-c(s))(1 \\quad A_i^T)^T.\n",
    "\\end{equation}\n",
    "The induced estimator $\\hat{\\tau}(s)$ for $\\tau(s)$ can thus be solved by\n",
    "\\begin{equation}\n",
    "  \\hat{\\tau}(s)=\\xi^T\\left(\\sum_{i=1}^n \\alpha_i(s)\\big(A_i-\\bar{A}_\\alpha\\big)^{\\otimes 2}\\right)^{-1}\\sum_{i=1}^n \\alpha_i(s)\\big(A_i-\\bar{A}_\\alpha\\big)\\big(R_i-\\bar{R}_\\alpha\\big),\n",
    "\\end{equation}\n",
    "where $\\bar{A}_\\alpha=\\sum \\alpha_i(s)A_i$ and $\\bar{R}_\\alpha=\\sum \\alpha_i(s)R_i$, and we write $v^{\\otimes 2}=vv^T$.\n",
    "\n",
    "Notice that this formula is just a weighted version of R-learner introduced above. However, instead of using ordinary kernel weighting functions that are prone to a strong curse of dimensionality, GRF uses an adaptive weighting function $\\alpha_i(s)$ derived from a forest designed to express heterogeneity in the specified quantity of interest. \n",
    "    \n",
    "To be more specific, in order to obtain $\\alpha_i(s)$, GRF first grows a set of $B$ trees indexed by $1,\\dots,B$. Then for each such tree, define $L_b(s)$ as the set of training samples falling in the same ``leaf\" as x. The weights $\\alpha_i(s)$ then capture the frequency with which the $i$-th training example falls into the same leaf as $s$:\n",
    "\\begin{equation}\n",
    "  \\alpha_{bi}(s)=\\frac{\\boldsymbol{1}\\big(\\{S_i\\in L_b(s)\\}\\big)}{\\big|L_b(s)\\big|},\\quad \\alpha_i(s)=\\frac{1}{B}\\sum_{b=1}^B \\alpha_{bi}(s).\n",
    "\\end{equation}\n",
    "\n",
    "To sum up, GRF aims to leverage the splitting result of a series of trees to decide the ``localized” weight for HTE estimation at each point $x_0$. Compared with kernel functions, we may expect tree-based weights to be more flexible and better performed in real settings.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fZT7U8YnNLGo",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 5332,
     "status": "ok",
     "timestamp": 1674856183930,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "fZT7U8YnNLGo",
    "outputId": "831d509c-aff4-47af-8722-50993d8c69dd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Collecting econml\n",
      "  Downloading econml-0.14.0-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m62.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from econml) (1.3.5)\n",
      "Collecting sparse\n",
      "  Downloading sparse-0.13.0-py2.py3-none-any.whl (77 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 KB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: scipy>1.4.0 in /usr/local/lib/python3.8/dist-packages (from econml) (1.7.3)\n",
      "Requirement already satisfied: scikit-learn<1.2,>0.22.0 in /usr/local/lib/python3.8/dist-packages (from econml) (1.0.2)\n",
      "Requirement already satisfied: lightgbm in /usr/local/lib/python3.8/dist-packages (from econml) (2.2.3)\n",
      "Requirement already satisfied: joblib>=0.13.0 in /usr/local/lib/python3.8/dist-packages (from econml) (1.2.0)\n",
      "Requirement already satisfied: statsmodels>=0.10 in /usr/local/lib/python3.8/dist-packages (from econml) (0.12.2)\n",
      "Collecting shap<0.41.0,>=0.38.1\n",
      "  Downloading shap-0.40.0-cp38-cp38-manylinux2010_x86_64.whl (571 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.1/571.1 KB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from econml) (1.21.6)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn<1.2,>0.22.0->econml) (3.1.0)\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (0.56.4)\n",
      "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (2.2.0)\n",
      "Collecting slicer==0.0.7\n",
      "  Downloading slicer-0.0.7-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: tqdm>4.25.0 in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (4.64.1)\n",
      "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.8/dist-packages (from shap<0.41.0,>=0.38.1->econml) (21.3)\n",
      "Requirement already satisfied: patsy>=0.5 in /usr/local/lib/python3.8/dist-packages (from statsmodels>=0.10->econml) (0.5.3)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->econml) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->econml) (2.8.2)\n",
      "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.8/dist-packages (from numba->shap<0.41.0,>=0.38.1->econml) (6.0.0)\n",
      "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.8/dist-packages (from numba->shap<0.41.0,>=0.38.1->econml) (0.39.1)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.8/dist-packages (from numba->shap<0.41.0,>=0.38.1->econml) (57.4.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>20.9->shap<0.41.0,>=0.38.1->econml) (3.0.9)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.8/dist-packages (from patsy>=0.5->statsmodels>=0.10->econml) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.8/dist-packages (from importlib-metadata->numba->shap<0.41.0,>=0.38.1->econml) (3.11.0)\n",
      "Installing collected packages: slicer, sparse, shap, econml\n",
      "Successfully installed econml-0.14.0 shap-0.40.0 slicer-0.0.7 sparse-0.13.0\n"
     ]
    }
   ],
   "source": [
    "# import the package for Causal Random Forest\n",
    "! pip install econml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "gk0nYH559XIL",
   "metadata": {
    "id": "gk0nYH559XIL"
   },
   "outputs": [],
   "source": [
    "# A demo code of Causal Random Forest\n",
    "from econml.grf import CausalForest, CausalIVForest, RegressionForest\n",
    "from econml.dml import CausalForestDML\n",
    "est = CausalForest(criterion='het', n_estimators=400, min_samples_leaf=5, max_depth=None,\n",
    "                    min_var_fraction_leaf=None, min_var_leaf_on_val=True,\n",
    "                    min_impurity_decrease = 0.0, max_samples=0.45, min_balancedness_tol=.45,\n",
    "                    warm_start=False, inference=True, fit_intercept=True, subforest_size=4,\n",
    "                    honest=True, verbose=0, n_jobs=-1, random_state=1235)\n",
    "\n",
    "\n",
    "est.fit(data_behavior.iloc[:,0:2], data_behavior['A'], data_behavior['R'])\n",
    "\n",
    "HTE_GRF = est.predict(data_behavior.iloc[:,0:2], interval=False, alpha=0.05)\n",
    "HTE_GRF = HTE_GRF.flatten()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cNcRW6yBOQJy",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1674856737901,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "cNcRW6yBOQJy",
    "outputId": "20de20f4-e2a7-4d40-a65c-1479a00a26e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generalized Random Forest:   [-1.2344  1.612  -0.7801  0.6886 -0.6297  0.2293  0.4417 -0.819 ]\n",
      "true value:                  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"
     ]
    }
   ],
   "source": [
    "print(\"Generalized Random Forest:  \",HTE_GRF[0:8])\n",
    "print(\"true value:                 \",HTE_true[0:8].to_numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "_djs10pxOQJ1",
   "metadata": {
    "id": "_djs10pxOQJ1"
   },
   "source": [
    "Causal Forest performs just okay in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0p9XqW8DOQJ2",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 151,
     "status": "ok",
     "timestamp": 1674856797696,
     "user": {
      "displayName": "Yang Xu",
      "userId": "12270366590264264299"
     },
     "user_tz": 300
    },
    "id": "0p9XqW8DOQJ2",
    "outputId": "21bb4ee5-8ae0-471b-a723-7e2efb278d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The overall estimation bias of Generalized Random Forest is :      0.7068579121479526 , \n",
      " The overall estimation variance of Generalized Random Forest is : 5.198946462195658 . \n",
      "\n"
     ]
    }
   ],
   "source": [
    "Bias_GRF = np.sum(HTE_GRF-HTE_true)/n\n",
    "Variance_GRF = np.sum((HTE_GRF-HTE_true)**2)/n\n",
    "print(\"The overall estimation bias of Generalized Random Forest is :     \", Bias_GRF, \", \\n\", \"The overall estimation variance of Generalized Random Forest is :\",Variance_GRF ,\". \\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {
    "id": "1098b550"
   },
   "source": [
    "## References\n",
    "1. Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 4156–4165.\n",
    "\n",
    "2. Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299–319, 2021.\n",
    "\n",
    "3. Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econometric Society, pages 931–954, 1988.\n",
    "\n",
    "4. Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020\n",
    "\n",
    "5. M. J. van der Laan. Statistical inference for variable importance. The International Journal of Biostatistics, 2(1), 2006.\n",
    "\n",
    "6. S. Lee, R. Okui, and Y.-J. Whang. Doubly robust uniform confidence band for the conditional average treatment effect function. Journal of Applied Econometrics, 32(7):1207–1225, 2017.\n",
    "\n",
    "7. D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036, 2019.\n",
    "\n",
    "8. Susan Athey, Julie Tibshirani, and Stefan Wager. Generalized random forests. The Annals of Statistics, 47(2):1148–1178, 2019."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}