{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fef41c9",
   "metadata": {},
   "source": [
    "# Ooline Policy Learning in Non-Markovian Environments \n",
    "\n",
    "An important extension of the [Markov assumption-based RL](section:online_RL) is to the non-Markovian environment. \n",
    "We provide a brief introduction in this chapter. \n",
    "\n",
    "## Model\n",
    "\n",
    "Recall the model we introduce in [online RL with MDP](section:online_RL). \n",
    "In comparison, we essentially cannot assume the MA and CMIA conditions any longer, and hence need to use all historical information in decision making. \n",
    "We make the comparison in {numref}`POMDP_comparison`. \n",
    "\n",
    "```{image} POMDP_comparison.png\n",
    ":name: POMDP_comparison\n",
    ":alt: d2ope\n",
    ":width: 800px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "The extension is valuable when either (i) the system dynamic depends on multiple or infinite lagged time points and hence it is infeasible to summarize historical information in a fixed-dimensional vector (the DTR problem that we studied in Paradigm 2), or when (ii) the underlying model is an MDP yet the state is not observable, which corresponds to the well-known Partially observable Markov decision process (POMDP). \n",
    "Note that the model of POMDP is actually slightly different, which we summarize in {numref}`POMDP`. \n",
    "Fortunately, unlike in the offline setting {cite:p}`shi2022off`, in the online setup, even with unobservable variables, there would be no causal bias, since the action selection does not depend on unmeasured variables. \n",
    "\n",
    "```{image} POMDP.png\n",
    ":name: POMDP\n",
    ":alt: d2ope\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "## Policy learning\n",
    "\n",
    "It is challenging to learn the optimal policy in a POMDP due to the model complexity. Below, we mainly introduce three classes of approaches. \n",
    "\n",
    "1. When the horizon is short (e.g., 2 or 3), it is still feasible to learn a time-dependent policy that directly utilizes the vector of all historical information for decision making. This is the online version of the DTR problem and was recently studied in {cite:t}`hu2020dtr`. However, when the horiozn is longer, such an approach quickly becomes computationally infeasible and certain dimension reduction method is then needed. \n",
    "2. In POMDP, a classic approach is to infer the underlying state via the history information, and then use the inferred state distribution as a *belief state* for decision making {cite:p}`spaan2012partially`. \n",
    "3. In recent years, there is also a growing literature on applying memory-based NN architecture for directly learning the policy from a sequence of history transition tuples {cite:p}`zhu2017improving, meng2021memory`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4871ef2-de01-4755-bd15-b8353e8f0dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}