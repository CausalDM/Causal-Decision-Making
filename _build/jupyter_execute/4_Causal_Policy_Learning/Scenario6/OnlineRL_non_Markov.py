#!/usr/bin/env python
# coding: utf-8

# # Ooline Policy Learning in Non-Markov Environments 
# 
# In this section, we introduce the formulation of the Markov Decision Process and a few related concepts which will be used repeatedly in this chapter. 
# We will proceed under the potential outcome framework, which provides a unique causal perspectiy, is different from the conventional notations {cite:p}`sutton2018reinforcement`, and is largely based on {cite:t}`shi2020reinforcement`. 
# Some of the assumptions to be discussed (such as the sequential randomization assumption) are imposed implicitly in the RL literature. 
# By writting these assumptions out, we aim to provide a more formal theoretical ground as well as to connect RL to the causal inference literautre. 

# ## References
# 
# ```{bibliography}
# :filter: docname in docnames
# ```
