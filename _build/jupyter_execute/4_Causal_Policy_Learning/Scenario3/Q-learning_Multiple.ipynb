{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "31f6d7fc",
   "metadata": {},
   "source": [
    "# Q-Learning (Multiple Stages)\n",
    "\n",
    "## Main Idea\n",
    "Early in 2000, as a classic method of Reinforcement Learning, Q-learning was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. Q-learning with finite decision points is mainly a regression modeling problem based on positing regression models for outcome at each decision point. The target of Q-learning is to find an optimal policy $\\pi$ that can maximize the expected reward received at the end of the final decision point. In other words, by training a model with the observed data, we hope to find an optimal policy to predict the optimal action for each individual to maximize rewards. For example, considering the motivating example **Personalized Incentives**, Q-learning aims to find the best policy to assign different incentives ($A$) to different users to optimize the return-on-investment ($R$). Overall, Q-learning is practical and easy to understand, as it allows straightforward implementation of diverse established regression methods. \n",
    "\n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest R is **continuous** and **non-negative**, where the larger the $R$ the better.\n",
    "\n",
    "## Algorithm Details\n",
    "For multistage cases, we apply a backward iterative approach, which means that we start from the final decision point T and work our way backward to the initial decision point. At the final step $T$, it is again a standard regression modeling problem that is the same as what we did for the single decision point case. Particularly, we posit a model $Q_{T}(h_{T},a_{T})$ for the expectation of potential outcome $R(\\bar{a}_T)$, and then the optimal policy at step $T$ is derived as $\\text{arg max}_{\\pi_{T}}Q_{T}(h_{T},\\pi_{T}(h_{T}))$. For the decision point $T-1$ till the decision point $1$, a new term is introduced, which is the pseudo-outcome $\\tilde{R}_{t}$.\n",
    "    \\begin{align}\n",
    "    \\tilde{R}_{t} = \\text{max}_{\\pi_{t}}\\hat{Q}_{t}(h_{t},\\pi_{t}(h_{t}),\\hat{\\boldsymbol{\\beta}}_{t})\n",
    "    \\end{align}\n",
    "    By doing so, the pseudo-outcome taking the **delayed effect** into account to help explore the optimal policy. Then, for each decision point $t<T$, with the $\\tilde{R}_{t+1}$ calculated, we repeat the regression modeling step for $\\tilde{R}_{t+1}$. After obtaining the fitted model $\\hat{Q}_{t}(h_{t},a_{t},\\hat{\\boldsymbol{\\beta}}_{t})$, the optimal policy is obtained as $\\arg \\max_{\\pi_{t}}Q_{t}(h_{t},\\pi_{t}(h_{t}))$.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. At the final decision point $t=T$, fitted a model $Q_{T}(h_{T},a_{T},\\boldsymbol{\\beta}_{T})$;\n",
    "2. For each individual $i$, calculated the pseudo-outcome $\\tilde{R}_{Ti}=\\text{max}_{\\pi}\\hat{Q}_{T}(h_{Ti},\\pi(h_{Ti}),\\hat{\\boldsymbol{\\beta}}_{T})$, and the optimal action $d^{opt}_{T}(\\boldsymbol{s}_{i})=\\text{arg max}_{a}\\hat{Q}_{T}(h_{Ti},a,\\hat{\\boldsymbol{\\beta}}_{T})$;\n",
    "3. For decision point $t = T-1,\\cdots, 1$,\n",
    "    1. fitted a model $\\hat{Q}_{t}(h_{t},a_{t},\\hat{\\boldsymbol{\\beta}}_{t})$ for the pseudo-outcome $\\tilde{R}_{t+1}$\n",
    "    2. For each individual $i$, calculated the pseudo-outcome $\\tilde{R}_{ti}=\\text{max}_{\\pi}\\hat{Q}_{t}(h_{ti},\\pi(h_{ti}),\\hat{\\boldsymbol{\\beta}}_{t})$, and the optimal action $d^{opt}_{t}(\\boldsymbol{s}_{i})=\\text{arg max}_{a}\\hat{Q}_{t}(h_{ti},a,\\hat{\\boldsymbol{\\beta}}_{t})$;\n",
    "    \n",
    "**Policy Evaluation:**    \n",
    "We use the backward iteration as what we did in policy learning. However, here for each round, the pseudo outcome is not the maximum of Q values. Instead, the pseudo outcome at decision point t is defined as below:\n",
    "\\begin{align}\n",
    "\\tilde{R}_{ti} = \\hat{Q}_{t}(h_{ti},d_{t}(h_{ti}),\\hat{\\boldsymbol{\\beta}}_{t}),\n",
    "\\end{align} where $d$ is the fixed regime that we want to evaluate.\n",
    "The estimated value of the policy is then the average of $\\tilde{R}_{1i}$.\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilize bootstrap resampling to get the estimated value of the regime and the corresponding estimated standard error.\n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378f78e6",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d69a7ed",
   "metadata": {},
<<<<<<< Updated upstream
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'causaldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# TODO: feasible set\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QLearning\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtest\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m shared_simulation\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'causaldm'"
     ]
    }
   ],
=======
   "outputs": [],
>>>>>>> Stashed changes
   "source": [
    "# TODO: feasible set\n",
    "from causaldm.learners.CPL13.disc import QLearning\n",
    "from causaldm.test import shared_simulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "758b8f5b",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataMDP_feasible.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 4>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#Important!! reset the index is required\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m dataMDP \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdataMDP_feasible.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m,\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;66;03m#.reset_index(drop=True) \u001b[39;00m\n\u001b[0;32m      5\u001b[0m R \u001b[38;5;241m=\u001b[39m dataMDP[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mY\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      6\u001b[0m S \u001b[38;5;241m=\u001b[39m dataMDP[[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCD4_0\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCD4_6\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCD4_12\u001b[39m\u001b[38;5;124m'\u001b[39m]]\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:933\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    930\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    932\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m--> 933\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1217\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1213\u001b[0m     mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1214\u001b[0m \u001b[38;5;66;03m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1215\u001b[0m \u001b[38;5;66;03m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[38;5;66;03m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1217\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1226\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\pandas\\io\\common.py:789\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    784\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    785\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    786\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 789\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    796\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    797\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    798\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataMDP_feasible.txt'"
     ]
    }
   ],
   "source": [
    "#prepare the dataset (dataset from the DTR book)\n",
    "import pandas as pd\n",
    "#Important!! reset the index is required\n",
    "dataMDP = pd.read_csv(\"dataMDP_feasible.txt\", sep=',')#.reset_index(drop=True) \n",
    "R = dataMDP['Y']\n",
    "S = dataMDP[['CD4_0','CD4_6','CD4_12']]\n",
    "A = dataMDP[['A1','A2','A3']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c9fdf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "# specify the model you would like to use\n",
    "# If want to include all the variable in S and A with no specific model structure, then use \"Y~.\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~CD4_0+A1+CD4_0*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+A2+CD4_6*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+CD4_12+A3+CD4_12*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]\n",
    "\n",
    "# train the policy\n",
    "QLearn.train(S, A, R, model_info, T=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5412b981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted model Q0: Intercept    167.898024\n",
      "CD4_0          2.102009\n",
      "A1            -1.116478\n",
      "CD4_0:A1       0.002859\n",
      "dtype: float64\n",
      "fitted model Q1: Intercept    171.676661\n",
      "CD4_0          2.454044\n",
      "CD4_6         -0.288382\n",
      "A2            -8.921595\n",
      "CD4_6:A2       0.015938\n",
      "dtype: float64\n",
      "fitted model Q2: Intercept    158.553900\n",
      "CD4_0          2.477566\n",
      "CD4_6         -0.551396\n",
      "CD4_12         0.334465\n",
      "A3           182.312429\n",
      "CD4_12:A3     -0.703112\n",
      "dtype: float64\n",
      "opt regime: A3  A2  A1\n",
      "0   1   1     550\n",
      "    0   1     450\n",
      "dtype: int64\n",
      "opt value: 1113.3004201781748\n"
     ]
    }
   ],
   "source": [
    "#4. recommend action\n",
    "opt_d = QLearn.recommend_action(S).value_counts()\n",
    "#5. get the estimated value of the optimal regime\n",
    "V_hat = QLearn.predict_value(S)\n",
    "print(\"fitted model Q0:\",QLearn.fitted_model[0].params)\n",
    "print(\"fitted model Q1:\",QLearn.fitted_model[1].params)\n",
    "print(\"fitted model Q2:\",QLearn.fitted_model[2].params)\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c1853758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A3  A2  A1\n",
       "0   1   1     550\n",
       "    0   1     450\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QLearn.recommend_action(S).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83cfd0de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 1112.9663576650953 Value_std: 3.6050454629164577\n"
     ]
    }
   ],
   "source": [
    "# Optional: we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~CD4_0+A1+CD4_0*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+A2+CD4_0*A2+CD4_6*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+CD4_12+A3+CD4_0*A3+CD4_6*A3+CD4_12*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]\n",
    "QLearn.train(S, A, R, model_info, T=3, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(S)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "064be465",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a67ac6e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "979.4518636939481"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify the fixed regime to be tested\n",
    "# For example, regime d = 1 for all subjects at all decision points\\\n",
    "N=len(S)\n",
    "# !! IMPORTANT: INDEX SHOULD BE THE SAME AS THAT OF THE S,R,A\n",
    "regime = pd.DataFrame({'A1':[1]*N,\n",
    "                      'A2':[1]*N,\n",
    "                      'A3':[1]*N}).set_index(S.index)\n",
    "#evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~CD4_0+A1+CD4_0*A1\",\n",
    "              'action_space':{'A1':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+A2+CD4_6*A2\",\n",
    "              'action_space':{'A2':[0,1]}},\n",
    "             {\"model\": \"Y~CD4_0+CD4_6+CD4_12+A3+CD4_12*A3\",\n",
    "              'action_space':{'A3':[0,1]}}]\n",
    "QLearn.train(S, A, R, model_info, T=3, regime = regime, evaluate = True)\n",
    "QLearn.predict_value(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15b43f9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 979.0772776227565 Value_std: 4.2034780374001155\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "QLearn.train(S, A, R, model_info, T=3, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(S)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "712d44d1",
   "metadata": {},
   "source": [
    "ðŸ’¥ Placeholder for C.I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b18cf4",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb1ded2",
   "metadata": {},
   "source": [
    "!! Already tested for accuracy using the data provided in DTR book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc81bc10-d747-482b-8d59-53f436814cd2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.11.3"
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}