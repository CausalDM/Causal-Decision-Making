{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9912c1a6-4e22-4295-9332-1750111ced59",
   "metadata": {},
   "source": [
    "# Outcome Weighted Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ea3ecfb-9f63-4507-8230-4c3881af9f57",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_81112/2987427551.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'..'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../CausalDM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the package, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0e2cfb-8a8f-4fc0-b356-1756c1614a35",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "\n",
    "A natural idea for policy learning is to stay close to the behaviour policy in those areas where it performs well. \n",
    "Outcome Weighted Learning (OWL) shares similar ideas. \n",
    "OWL was first proposed in [1] under the binary treatment case, and extended in [3] to allow multiple treatments. \n",
    "The foundation of OWL is built on the relationship that, maximizing $V(\\pi)$ is equivalent to solve\n",
    "\n",
    "\\begin{align}\n",
    "    \\text{arg min}_{\\pi} \\mathbb{E}\\Big[ \\frac{Y_i}{b(A_i|X_i)}\\mathbb{I}(A_i \\neq \\pi(X_i))\\Big]. \n",
    "\\end{align}\n",
    "\n",
    "When $Y_i$ is non-negative, this goal corresponds to the objective function of a cost-sensitive classification problem with ${Y_i}/{b(A_i|X_i)}$ as the weight, \n",
    "$A_i$ as the true label, \n",
    "and $\\pi$ as the classifier to be learned. \n",
    "Intuitively, a large value of $Y_i$ implies a large weight that encourages the policy to take the same action as observed; \n",
    "while a small reward has the opposite effect. \n",
    "This is why the estimator is called *outcome weighted*. \n",
    "$b(A_i|X_i)$ is used to remove the sampling bias. \n",
    "\n",
    "Based on the relationship, OWL has the following key steps:\n",
    "1. Estimate the weight of data point $i$ as $w_i = (Y_i + c) / b(A_i|X_i)$\n",
    "    1. Here $c$ is a constant such that $Y_i + c$ are all non-negative, which is required to use cost-sensitive classification algorithms. Note that such a shift will not affect the solution of (1), though with finite sample it may cause instability. \n",
    "    2. With binary treatment, we implement the approach in [2] to estimate a shift constant and hence the algorithm is adaptive. \n",
    "2. Solve the policy with a user-specified cost-sensitive classifier. The theory is developed based on SVM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeea3a99-bbe9-4bb5-92c4-b9cc70e3712c",
   "metadata": {},
   "source": [
    "## When should I use OWL?\n",
    "* TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4ea592-750e-457e-b834-c65d50d14481",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ba6dbfa2-6eda-447d-b316-ad0185c055e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import OWL\n",
    "from causaldm.test import shared_simulation\n",
    "from causaldm.test import OWL_simu\n",
    "from causaldm.metric import metric\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b9b8d4d-c11d-4da9-8d8f-1cfae06693fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate sample data\n",
    "instance = OWL_simu.generate_test_case(setup = 'case1', N = 1000, seed = 0, p = 5, sigma = 1)\n",
    "X, A, Y = instance['XAY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "06e7910f-5f96-44fd-867b-77d0758d31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the learner\n",
    "owl = OWL.OutcomeWeightedLearning()\n",
    "# specify the classifier you would like to use\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score\n",
    "clf = SVC(kernel='linear') # fit_intercept = True, \n",
    "# Cs = np.logspace(-6, -1, 10)\n",
    "# clf = GridSearchCV(estimator=clf, param_grid=dict(C=Cs),\n",
    "#                    n_jobs=-1)\n",
    "\n",
    "# specify the assignment_prob probability, if your data is from an experiment \n",
    "assignment_prob = np.ones(len(A)) / 0.5\n",
    "\n",
    "# train the policy\n",
    "owl.train(X, A, Y, classifier = clf, assignment_prob = assignment_prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "84b9b462-bfaa-4773-8e2f-bc44e5cf8a05",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 1, 0, 1, 0, 0, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# recommend action\n",
    "owl.recommend_action(X)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d76233-24cc-4510-9bf1-238d845aefe6",
   "metadata": {},
   "source": [
    "## Sparse OWL\n",
    "\n",
    "In many applications, we have a large number of features. [4] extend OWL to these use cases by assuming a sparsity structure, i.e., most features do not have effect in the policy. Under this assumption, [4] develops a penalized policy learner and proved its consistency as well as asymptotic distribution. Notably, one can achieve variable selection in the meantime. \n",
    "\n",
    "TBD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef43693c-5b1c-4427-9dee-8fb8639f4b85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b559e589-d6aa-4083-9384-0434b649257b",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Zhao, Yingqi, et al. \"Estimating individualized treatment rules using outcome weighted learning.\" Journal of the American Statistical Association 107.499 (2012): 1106-1118.\n",
    "2. Liu, Ying, et al. \"Augmented outcome‚Äêweighted learning for estimating optimal dynamic treatment regimens.\" Statistics in medicine 37.26 (2018): 3776-3788.\n",
    "3. Lou, Zhilan, Jun Shao, and Menggang Yu. \"Optimal treatment assignment to maximize expected outcome with multiple treatments.\" Biometrics 74.2 (2018): 506-516.\n",
    "4. Song, Rui, et al. \"On sparse representation for optimal individualized treatment selection with penalized outcome weighted learning.\" Stat 4.1 (2015): 59-68."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fe7cb1b-fb93-4425-a62b-b5bcec664ec8",
   "metadata": {},
   "source": [
    "## A1: Derivations\n",
    "\n",
    "\\begin{align*}\n",
    "V(\\pi)\n",
    "&= \\mathbb{E}_{A_i \\sim b(X_i)}\\Big[ \\frac{\\mathbb{I}(A_i = \\pi(X_i))}{b(A_i|X_i)}Y_i\\Big]\\\\\n",
    "&= \\mathbb{E}_{A_i \\sim b(X_i)}\\Big[ \\frac{1 - \\mathbb{I}(A_i \\neq \\pi(X_i))}{b(A_i|X_i)}Y_i\\Big]\\\\\n",
    "&= \\text{const} - \\mathbb{E}_{A_i \\sim b(X_i)}\\Big[ \\frac{\\mathbb{I}(A_i \\neq \\pi(X_i))}{b(A_i|X_i)}Y_i\\Big]\\\\\n",
    "&= \\text{const} - \\mathbb{E}_{A_i \\sim b(X_i)}\\Big[ \\frac{Y_i}{b(A_i|X_i)}\\mathbb{I}(A_i \\neq \\pi(X_i))\\Big]. \n",
    "\\end{align*}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}