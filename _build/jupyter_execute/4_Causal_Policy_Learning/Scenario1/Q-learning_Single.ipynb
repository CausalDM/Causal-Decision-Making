{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112864aa",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "## Main Idea\n",
    "Early in 2000, as a classic method of Reinforcement Learning, Q-learning was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. Q-learning with finite decision points is mainly a regression modeling problem based on positing regression models for outcome at each decision point. The target of Q-learning is to find an optimal policy $\\pi$ that can maximize the expected reward received. In other words, by training a model with the observed data, we hope to find an optimal policy to predict the optimal action for each individual to maximize rewards. For example, considering the motivating example **Personalized Incentives**, Q-learning aims to find the best policy to assign different incentives ($A$) to different users to optimize the return-on-investment ($R$). Overall, Q-learning is practical and easy to understand, as it allows straightforward implementation of diverse established regression methods. \n",
    "\n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest R is **continuous** and **non-negative**, where the larger the $R$ the better.\n",
    "\n",
    "\n",
    "\n",
    "## Algorithm Details\n",
    "Q-learning with a single decision point is mainly a regression modeling problem, as the major component is to find the relationship between the expectation of potential reward $R(a)$ and $\\{\\boldsymbol{s},a\\}$. Let's first define a Q-function, such that\n",
    "\\begin{align}\n",
    "    Q(\\boldsymbol{s},a) = E(R(a)|\\boldsymbol{S}=\\boldsymbol{s}).\n",
    "\\end{align} Then, to find the optimal policy is equivalent to solve\n",
    "\\begin{align}\n",
    "    \\text{arg max}_{\\pi}Q(\\boldsymbol{s}_{i},\\pi(\\boldsymbol{s}_{i})).\n",
    "\\end{align} \n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. Fitted a model $\\hat{Q}(\\boldsymbol{s},a,\\hat{\\boldsymbol{\\beta}})$, which can be solved directly by existing approaches (i.e., OLS, .etc),\n",
    "2. For each individual find the optimal action $d^{opt}(\\boldsymbol{s}_{i})$ such that $d^{opt}(\\boldsymbol{s}_{i}) = \\text{arg max}_{a}\\hat{Q}(\\boldsymbol{s}_{i},a,\\hat{\\boldsymbol{\\beta}})$.\n",
    "\n",
    "**Policy Evaluation:**    \n",
    "1. Fitted the Q function $\\hat{Q}(\\boldsymbol{s},a,\\hat{\\boldsymbol{\\beta}})$, based on the sampled dataset\n",
    "2. Estimated the value of a given regime $d$ (i.e., $V(d)$) using the estimated Q function, such that, $\\hat{E}(R_{i}[d(\\boldsymbol{s}_{i})]) = \\hat{Q}(\\boldsymbol{s}_{i},d(\\boldsymbol{s}_{i}),\\hat{\\boldsymbol{\\beta}})$, and $\\hat{V}(d) = \\frac{1}{N}\\sum_{i=1}^{N}\\hat{E}(R_{i}[d(\\boldsymbol{s}_{i})])$.\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilize bootstrap resampling to get the estimated value of the regime and the corresponding estimated standard error. For each round of bootstrapping, we first resample a dataset of the same size as the original dataset, then fit the Q function based on the sampled dataset, and finally estimate the value of a given regime based on the estimated Q function. \n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3559b08",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be9bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import learner\n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners.CPL13.disc import QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e46da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "S,A,R = get_data(target_col = 'spend', binary_trt = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b4c2e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. specify the model you would like to use\n",
    "# If want to include all the variable in S and A with no specific model structure, then use \"Y~.\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e83e1f",
   "metadata": {},
   "source": [
    "By specifing the model_info, we assume a regression model that:\n",
    "\\begin{align}\n",
    "Q(\\boldsymbol{s},a,\\boldsymbol{\\beta}) &= \\beta_{00}+\\beta_{01}*recency+\\beta_{02}*history\\\\\n",
    "&+I(a=1)*\\{\\beta_{10}+\\beta_{11}*recency+\\beta_{12}*history\\} \\\\\n",
    "&+I(a=2)*\\{\\beta_{20}+\\beta_{21}*recency+\\beta_{22}*history\\} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b7df28e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: <statsmodels.regression.linear_model.RegressionResultsWrapper at 0x7ff088f97940>}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2. initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "#3. train the policy\n",
    "QLearn.train(S, A, R, model_info, T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "840f98ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted model: Intercept            94.202956\n",
      "C(A)[T.1]            23.239801\n",
      "C(A)[T.2]            20.611375\n",
      "recency               4.526133\n",
      "C(A)[T.1]:recency    -4.152892\n",
      "C(A)[T.2]:recency    -4.843148\n",
      "history               0.000549\n",
      "C(A)[T.1]:history     0.007584\n",
      "C(A)[T.2]:history     0.000416\n",
      "dtype: float64\n",
      "opt regime: A\n",
      "1    371\n",
      "0    207\n",
      "dtype: int64\n",
      "opt value: 126.48792828230138\n"
     ]
    }
   ],
   "source": [
    "#4. recommend action\n",
    "opt_d = QLearn.recommend_action(S).value_counts()\n",
    "#5. get the estimated value of the optimal regime\n",
    "V_hat = QLearn.predict_value(S)\n",
    "print(\"fitted model:\",QLearn.fitted_model[0].params)\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111442",
   "metadata": {},
   "source": [
    "**Interpretation:** the fitted model is \n",
    "\\begin{align}\n",
    "Q(\\boldsymbol{s},a,\\boldsymbol{\\beta}) &= 94.20+4.53*recency+0.0005*history\\\\\n",
    "&+I(a=1)*\\{23.24-4.15*recency+0.0076*history\\} \\\\\n",
    "&+I(a=2)*\\{20.61-4.84*recency+0.0004history\\}. \n",
    "\\end{align}\n",
    "Therefore, the estimated optimal regime is:\n",
    "1. We would recommend $A=0$ (No E-mail) if $23.24-4.15*recency+0.0076*history<0$ and $20.61-4.84*recency+0.0004history<0$\n",
    "2. Else, we would recommend $A=1$ (Womens E-mail) if $23.24-4.15*recency+0.0076*history>20.61-4.84*recency+0.0004history$\n",
    "3. Else, we would recommend $A=2$ (Mens E-Mail).\n",
    "\n",
    "The estimated value for the estimated optimal regime is 126.49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e22463b6",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'QLearning' object has no attribute 'fit'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_51110/3664914869.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n\u001b[1;32m      6\u001b[0m               'action_space':{'A':[0,1,2]}}]\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mQLearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbootstrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_bs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mfitted_params\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mfitted_value\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue_std\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mQLearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_value_boots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Value_hat:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue_avg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Value_std:'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue_std\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/learners/CPL13/disc/QLearning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, A, Y, model_info, T, regime, evaluate, bootstrap, n_bs, mimic3_clip)\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m                     \u001b[0mregime_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_sample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregime\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregime_sample\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboots\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'QLearning' object has no attribute 'fit'"
     ]
    }
   ],
   "source": [
    "# Optional: \n",
    "#we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]\n",
    "QLearn.train(S, A, R, model_info, T=1, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(S)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e820db",
   "metadata": {},
   "source": [
    "**Interpretation:** Based on the boostrap with 200 replicates, the 'Value_hat' is the estimated optimal value, and the 'Value_std' is the corresponding standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff8447",
   "metadata": {},
   "source": [
    "##### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dae136c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.40675465960962"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. specify the fixed regime to be tested (For example, regime d = 'No E-Mail' for all subjects)\n",
    "# !! IMPORTANTï¼š index shold be the same as that of the S\n",
    "N=len(S)\n",
    "regime = pd.DataFrame({'A':[0]*N}).set_index(S.index)\n",
    "#2. evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]\n",
    "QLearn.train(S, A, R, model_info, T=1, regime = regime, evaluate = True)\n",
    "QLearn.predict_value(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b2af9",
   "metadata": {},
   "source": [
    "**Interpretation:** the estimated value of the regime that always sends no emails ($A=0$) is 116.41, under the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "961357b8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 116.22558023415299 Value_std: 10.45840507437804\n"
     ]
    }
   ],
   "source": [
    "# Optional: Boostrap\n",
    "QLearn.train(S, A, R, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(S)\n",
    "# bootstrap average and the std of estimate value\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b691752",
   "metadata": {},
   "source": [
    "**Interpretation:** the 'Value_hat' is the bootstrapped estimated value of the regime that always sends no emails, and the 'Value_std' is the correspoding bootstrapped standard error, under the specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cff012d",
   "metadata": {},
   "source": [
    "- ðŸ’¥1. estimate the standard error for the binary case with sandwich formula;\n",
    "- ðŸ’¥2. inference for the estimated optimal regime: projected confidence interval? m-out-of-n CI?...."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afda9c-713c-49e2-96ba-decd4267e7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}