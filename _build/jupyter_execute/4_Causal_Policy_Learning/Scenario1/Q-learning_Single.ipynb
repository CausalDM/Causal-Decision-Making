{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "112864aa",
   "metadata": {},
   "source": [
    "# Q-Learning (Single Stage)\n",
    "\n",
    "## Main Idea\n",
    "Early in 2000, as a classic method of Reinforcement Learning, Q-learning was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. Q-learning with finite decision points is mainly a regression modeling problem based on positing regression models for outcome at each decision point. The target of Q-learning is to find an optimal policy $\\pi$ that can maximize the expected reward received. In other words, by training a model with the observed data, we hope to find an optimal policy to predict the optimal action for each individual to maximize rewards. For example, considering the motivating example **Personalized Incentives**, Q-learning aims to find the best policy to assign different incentives ($A$) to different users to optimize the return-on-investment ($R$). Overall, Q-learning is practical and easy to understand, as it allows straightforward implementation of diverse established regression methods. \n",
    "\n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest R is **continuous** and **non-negative**, where the larger the $R$ the better.\n",
    "\n",
    "\n",
    "\n",
    "## Algorithm Details\n",
    "Q-learning with a single decision point is mainly a regression modeling problem, as the major component is to find the relationship between the expectation of potential reward $R(a)$ and $\\{\\boldsymbol{s},a\\}$. Let's first define a Q-function, such that\n",
    "\\begin{align}\n",
    "    Q(\\boldsymbol{s},a) = E(R(a)|\\boldsymbol{S}=\\boldsymbol{s}).\n",
    "\\end{align} Then, to find the optimal policy is equivalent to solve\n",
    "\\begin{align}\n",
    "    \\text{arg max}_{\\pi}Q(\\boldsymbol{s}_{i},\\pi(\\boldsymbol{s}_{i})).\n",
    "\\end{align} \n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. Fitted a model $\\hat{Q}(\\boldsymbol{s},a,\\hat{\\boldsymbol{\\beta}})$, which can be solved directly by existing approaches (i.e., OLS, .etc),\n",
    "2. For each individual find the optimal action $d^{opt}(\\boldsymbol{s}_{i})$ such that $d^{opt}(\\boldsymbol{s}_{i}) = \\text{arg max}_{a}\\hat{Q}(\\boldsymbol{s}_{i},a,\\hat{\\boldsymbol{\\beta}})$.\n",
    "\n",
    "**Policy Evaluation:**    \n",
    "1. Fitted the Q function $\\hat{Q}(\\boldsymbol{s},a,\\hat{\\boldsymbol{\\beta}})$, based on the sampled dataset\n",
    "2. Estimated the value of a given regime $d$ (i.e., $V(d)$) using the estimated Q function, such that, $\\hat{E}(R_{i}[d(\\boldsymbol{s}_{i})]) = \\hat{Q}(\\boldsymbol{s}_{i},d(\\boldsymbol{s}_{i}),\\hat{\\boldsymbol{\\beta}})$, and $\\hat{V}(d) = \\frac{1}{N}\\sum_{i=1}^{N}\\hat{E}(R_{i}[d(\\boldsymbol{s}_{i})])$.\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilize bootstrap resampling to get the estimated value of the regime and the corresponding estimated standard error. For each round of bootstrapping, we first resample a dataset of the same size as the original dataset, then fit the Q function based on the sampled dataset, and finally estimate the value of a given regime based on the estimated Q function. \n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3559b08",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6be9bfe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'causaldm'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# import learner\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util_causaldm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m QLearning\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'causaldm'"
     ]
    }
   ],
   "source": [
    "# import learner\n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners import QLearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e46da2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "S,A,R = get_data(target_col = 'spend', binary_trt = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c2e64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. specify the model you would like to use\n",
    "# If want to include all the variable in S and A with no specific model structure, then use \"Y~.\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e83e1f",
   "metadata": {},
   "source": [
    "By specifing the model_info, we assume a regression model that:\n",
    "\\begin{align}\n",
    "Q(\\boldsymbol{s},a,\\boldsymbol{\\beta}) &= \\beta_{00}+\\beta_{01}*recency+\\beta_{02}*history\\\\\n",
    "&+I(a=1)*\\{\\beta_{10}+\\beta_{11}*recency+\\beta_{12}*history\\} \\\\\n",
    "&+I(a=2)*\\{\\beta_{20}+\\beta_{21}*recency+\\beta_{22}*history\\} \n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7df28e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2. initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "#3. train the policy\n",
    "QLearn.train(S, A, R, model_info, T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840f98ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted model: Intercept            94.202956\n",
      "C(A)[T.1]            23.239801\n",
      "C(A)[T.2]            20.611375\n",
      "recency               4.526133\n",
      "C(A)[T.1]:recency    -4.152892\n",
      "C(A)[T.2]:recency    -4.843148\n",
      "history               0.000549\n",
      "C(A)[T.1]:history     0.007584\n",
      "C(A)[T.2]:history     0.000416\n",
      "dtype: float64\n",
      "opt regime: A\n",
      "1    371\n",
      "0    207\n",
      "dtype: int64\n",
      "opt value: 126.48792828230047\n"
     ]
    }
   ],
   "source": [
    "#4. recommend action\n",
    "opt_d = QLearn.recommend_action(S).value_counts()\n",
    "#5. get the estimated value of the optimal regime\n",
    "V_hat = QLearn.predict_value(S)\n",
    "print(\"fitted model:\",QLearn.fitted_model[0].params)\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6111442",
   "metadata": {},
   "source": [
    "**Interpretation:** the fitted model is \n",
    "\\begin{align}\n",
    "Q(\\boldsymbol{s},a,\\boldsymbol{\\beta}) &= 94.20+4.53*recency+0.0005*history\\\\\n",
    "&+I(a=1)*\\{23.24-4.15*recency+0.0076*history\\} \\\\\n",
    "&+I(a=2)*\\{20.61-4.84*recency+0.0004history\\}. \n",
    "\\end{align}\n",
    "Therefore, the estimated optimal regime is:\n",
    "1. We would recommend $A=0$ (No E-mail) if $23.24-4.15*recency+0.0076*history<0$ and $20.61-4.84*recency+0.0004history<0$\n",
    "2. Else, we would recommend $A=1$ (Womens E-mail) if $23.24-4.15*recency+0.0076*history>20.61-4.84*recency+0.0004history$\n",
    "3. Else, we would recommend $A=2$ (Mens E-Mail).\n",
    "\n",
    "The estimated value for the estimated optimal regime is 126.49."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e22463b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 132.31352423343282 Value_std: 7.37323834017361\n"
     ]
    }
   ],
   "source": [
    "# Optional: \n",
    "#we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]\n",
    "QLearn.train(S, A, R, model_info, T=1, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(S)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e820db",
   "metadata": {},
   "source": [
    "**Interpretation:** Based on the boostrap with 200 replicates, the estimated optimal value is 132.31 with a standard error of 7.37."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eff8447",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae136c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.40675465960642"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. specify the fixed regime to be tested (For example, regime d = 'No E-Mail' for all subjects)\n",
    "# !! IMPORTANTï¼š index shold be the same as that of the S\n",
    "N=len(S)\n",
    "regime = pd.DataFrame({'A':[0]*N}).set_index(S.index)\n",
    "#2. evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':[0,1,2]}}]\n",
    "QLearn.train(S, A, R, model_info, T=1, regime = regime, evaluate = True)\n",
    "QLearn.predict_value(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573b2af9",
   "metadata": {},
   "source": [
    "**Interpretation:** the estimated value of the regime that always sends no emails ($A=0$) is 116.41, under the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "961357b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 115.95548975939548 Value_std: 10.502988081748748\n"
     ]
    }
   ],
   "source": [
    "# Optional: Boostrap\n",
    "QLearn.train(S, A, R, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.predict_value_boots(S)\n",
    "# bootstrap average and the std of estimate value\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b691752",
   "metadata": {},
   "source": [
    "**Interpretation:** the bootstrapped estimated value of the regime that always sends no emails is 115.96 with a bootstrapped standard error 10.50, under the specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1098b550",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45df2637",
   "metadata": {},
   "source": [
    "!!Functions are already tested with the data and results provided in the DTR book\n",
    "\n",
    "TODO: \n",
    "    1. estimate the standard error for the binary case with sandwich formula;\n",
    "    2. inference for the estimated optimal regime: projected confidence interval? m-out-of-n CI?...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29afda9c-713c-49e2-96ba-decd4267e7cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}