{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "537968f8",
   "metadata": {},
   "source": [
    "# A-Learning\n",
    "\n",
    "## Main Idea\n",
    "A-Learning, also known as Advantage Learning, is one of the main approaches to learning the optimal regime and works similarly to Q-learning. However, while Q-learning requires positing regression models to fit the expected outcome, A-learning models the contrasts between treatments and control, directly informing the optimal decision. For example, in the case of **Personalized Incentives**, A-learning aims to find the optimal incentive ($A$) for each user by modeling the difference in expected return-on-investment ($R$) between treatments. A detailed comparison between Q-learning and A-learning can be found in [1]. While [1] mainly focus on the case with binary treatment options, a complete review of A-learning with multiple treatment options can be found in [2]. Here, following the algorithm in [1], we consider contrast-based A-learning. However, there is an alternative regret-based A-learning introduced in [3]. Some recent extensions to conventional A-learning, such as deep A-learning [4] and high-dimensional A-Learning [5], will be added soon. Overall, A-learning is doubly-robust. In other words, it is less sensitive and more robust to model misspecification. \n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., 0,1,2,3,4, where 0 stands for the control group by convention), and the outcome of interest R is **continuous** and **non-negative**, where the larger the $R$ the better. \n",
    "\n",
    "## Algorithm Details\n",
    "Suppose there are $m$ number of options, and the action space $\\mathcal{A}=\\{0,1,\\dots,m-1\\}$. Contrast-based A-learning, as the name suggested, aims to learn and estimate the constrast function, $C_{j}(\\boldsymbol{S})$ for each treatment $j=1,2,\\cdots, m-1$. Furthermore, we also need to posit a model for the conditional expected potential outcome for the control option (treatment $0$), $Q(\\boldsymbol{S},0)$, and the propensity function $\\omega(\\boldsymbol{S},A)$, if the true values are not specified. Detailed definitions are provided in the following.\n",
    "*   Q-function:\n",
    "    \\begin{align}\n",
    "    Q(\\boldsymbol{s},a)=E[R(a)|\\boldsymbol{S}=\\boldsymbol{s}],\n",
    "    \\end{align}\n",
    "    Alternatively, with the contrast function $C_j(\\boldsymbol{S})$ which will be defined later,\n",
    "    \\begin{align}\n",
    "    Q(\\boldsymbol{s},j) = Q(\\boldsymbol{s},0) + C_{j}(\\boldsymbol{s}),\\quad j=0,\\dots,m-1.\n",
    "    \\end{align}\n",
    "*   Contrast functions (optimal blip to zero functions)\n",
    "    \\begin{align}\n",
    "    C_{j}(\\boldsymbol{s})=Q(\\boldsymbol{s},j)-Q(\\boldsymbol{s},0),\\quad j=0,\\dots,m-1,\n",
    "    \\end{align}\n",
    "    where $C_{0}(\\boldsymbol{s}) = 0$.\n",
    "*   Propensity score\n",
    "    \\begin{align}\n",
    "    \\omega(\\boldsymbol{s},a)=P(A=a|\\boldsymbol{S}=\\boldsymbol{s})\n",
    "    \\end{align}\n",
    "*   Optimal regime\n",
    "    \\begin{align}\n",
    "    d^{opt}(\\boldsymbol{s})=\\arg\\max_{j\\in\\mathcal{A}}C_{j}(\\boldsymbol{s})\n",
    "    \\end{align}\n",
    "Positting models, $C_{j}(\\boldsymbol{s},\\boldsymbol{\\psi}_{j})$,$Q(\\boldsymbol{s},0,\\boldsymbol{\\phi})$,and $\\omega(\\boldsymbol{s},a,\\boldsymbol{\\gamma})$, A-learning aims to estimate $\\boldsymbol{\\psi}_{j}$, $\\boldsymbol{\\phi}$, and $\\boldsymbol{\\gamma}$ by g-estimation. With the $\\hat{\\boldsymbol{\\psi}}_{j}$ in hand, the optimal decision $d^{opt}(\\boldsymbol{s})$ can be directly derived.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. Fitted a model $\\omega(\\boldsymbol{s},a,\\boldsymbol{\\gamma})$, which can be solved directly by existing approaches (i.e., logistic regression, .etc),\n",
    "2. Substituting the $\\hat{\\boldsymbol{\\gamma}}$, we estimate the $\\hat{\\boldsymbol{\\psi}}_{j}$ and $\\hat{\\boldsymbol{\\gamma}}$ by solving the euqations in Appendix A.1 jointly.      \n",
    "3. For each individual find the optimal action $d^{opt}(\\boldsymbol{s}_{i})$ such that $d^{opt}(\\boldsymbol{s}_{i}) = \\arg\\max_{j\\in\\mathcal{A}}C_{j}(h,\\hat{\\boldsymbol{\\psi}_{j}})$.\n",
    "    \n",
    "**Policy Evaluation:**    \n",
    "1. Fitted the functions $\\omega(\\boldsymbol{s},a,\\boldsymbol{\\gamma})$ï¼Œ $\\hat{Q}(\\boldsymbol{s},0,\\hat{\\boldsymbol{\\beta}})$, and $\\hat{C}_{j}(\\boldsymbol{s},\\hat{\\boldsymbol{\\psi}}_{j})$, based on the sampled dataset\n",
    "2. Estimated the value of a given regime $d$ using the estimated functions, such that, $\\hat{R}_{i} = \\hat{Q}(\\boldsymbol{s}_{i},0,\\hat{\\boldsymbol{\\beta}})+I\\{d(\\boldsymbol{s}_{i})=j\\}\\hat{C}_{j}(\\boldsymbol{s}_i,\\hat{\\boldsymbol{\\psi}}_{j})$, and the estimated value is the average of $\\hat{R}_{i}$.\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilze the boostrap resampling to get the estimated value of the regime and the corresponding estimated standard error. Basically, for each round of bootstrap, we resample a dataset of the same size as the original dataset with replacement, fitted the $Q(\\boldsymbol{s},0)$ function and contrast functions based on the sampled dataset, and estimated the value of a given regime using the estimated $Q(\\boldsymbol{s},0)$ function and contrast functions function. \n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def68fc9",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "906333b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import learner\n",
    "from causaldm._util_causaldm import *\n",
    "from causaldm.learners.CPL13.disc import ALearning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e97bb07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data\n",
    "S,A,R = get_data(target_col = 'spend', binary_trt = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72c95691",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data into 2d numpy array\n",
    "R = np.array(R)\n",
    "S = np.hstack([np.ones((len(S),1)),np.array(S)])# add an intercept column\n",
    "A = np.array(A)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7c2ad4d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. specify the model you would like to use\n",
    "model_info = [{'X_prop': [0,1,2], #[0,1,2] here stands for the intercept, recency and history\n",
    "              'X_q0': [0,1,2],\n",
    "               'X_C':{1:[0,1,2],2:[0,1,2]},\n",
    "              'action_space': {'A':[0,1,2]}}] #A in [0,1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8f79e7",
   "metadata": {},
   "source": [
    "By specifing the model_info, we assume  the following models:\n",
    "\\begin{align}\n",
    "Q(\\boldsymbol{s},0,\\boldsymbol{\\phi}) &= \\phi_{00}+\\phi_{01}*recency+\\phi_{02}*history,\\\\\n",
    "C_{1}(\\boldsymbol{s},\\boldsymbol{\\psi}_{0}) &= 0\\\\\n",
    "C_{1}(\\boldsymbol{s},\\boldsymbol{\\psi}_{1}) &= \\psi_{10}+\\psi_{11}*recency+\\psi_{12}*history,\\\\\n",
    "C_{2}(\\boldsymbol{s},\\boldsymbol{\\psi}_{2}) &= \\psi_{20}+\\psi_{21}*recency+\\psi_{22}*history,\\\\\n",
    "\\omega(\\boldsymbol{s},a=j,\\boldsymbol{\\gamma}) &= \\frac{exp(\\gamma_{j0}+\\gamma_{j1}*recency+\\gamma_{j2}*history)}{\\sum_{j=0}^{2}exp(\\gamma_{j0}+\\gamma_{j1}*recency+\\gamma_{j2}*history)},\\\\\n",
    "\\end{align}\n",
    "where $\\gamma_{00}=\\gamma_{01}=\\gamma_{02}=0$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2fbf64f",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ALearning' object has no attribute 'get_pseudo_Y'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_51041/15241541.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mALearn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mALearning\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mALearning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m#3. train the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mALearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/learners/CPL13/disc/ALearning.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, A, Y, model_info, T, true_prop, regime, evaluate, bootstrap, n_bs)\u001b[0m\n\u001b[1;32m     95\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboots_fitted\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfitted_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/learners/CPL13/disc/ALearning.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, X, A, Y, T, regime)\u001b[0m\n\u001b[1;32m    102\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mT\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 104\u001b[0;31m                 \u001b[0mpseudo_Y_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_pseudo_Y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregime\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    105\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpseudo_Y_prev\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'ALearning' object has no attribute 'get_pseudo_Y'"
     ]
    }
   ],
   "source": [
    "#2. initialize the learner\n",
    "ALearn = ALearning.ALearning()\n",
    "#3. train the policy\n",
    "ALearn.train(S, A, R, model_info, T=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "06ee8472",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted contrast model: {0: {1: array([ 2.3389e+01, -4.0295e+00,  5.3272e-03]), 2: array([ 2.1025e+01, -4.7135e+00, -2.7582e-03])}}\n",
      "opt regime: A\n",
      "1    376\n",
      "0    202\n",
      "dtype: int64\n",
      "opt value: 126.18615811062617\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = ALearn.recommend_action(S).value_counts()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = ALearn.predict_value(S)\n",
    "print(\"fitted contrast model:\",ALearn.fitted_model['contrast'])\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e653ae2",
   "metadata": {},
   "source": [
    "**Interpretation:** the fitted contrast models are \n",
    "\\begin{align}\n",
    "C_{0}(\\boldsymbol{s},\\boldsymbol{\\psi}_{0}) &= 0\\\\\n",
    "C_{1}(\\boldsymbol{s},\\boldsymbol{\\psi}_{1}) &= 23.39-4.03*recency+.005*history,\\\\\n",
    "C_{2}(\\boldsymbol{s},\\boldsymbol{\\psi}_{2}) &= 21.03-4.71*recency-.003*history,\\\\\n",
    "\\end{align}\n",
    "Therefore, the estimated optimal regime is:\n",
    "1. We would recommend $A=0$ (No E-mail) if $23.39-4.03*recency+.005*history<0$ and $21.03-4.71*recency-.003*history<0$\n",
    "2. Else, we would recommend $A=1$ (Womens E-mail) if $23.39-4.03*recency+.005*history>21.03-4.71*recency-.003*history$\n",
    "3. Else, we would recommend $A=2$ (Mens E-Mail).\n",
    "\n",
    "The estimated value for the estimated optimal regime is 126.19."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "47dde07e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 133.07058862299368 Value_std: 8.114870661497804\n"
     ]
    }
   ],
   "source": [
    "# Optional: \n",
    "#we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "ALearn = ALearning.ALearning()\n",
    "model_info = [{'X_prop': [0,1,2], #[0,1,2] here stands for the intercept, recency and history\n",
    "              'X_q0': [0,1,2],\n",
    "               'X_C':{1:[0,1,2],2:[0,1,2]},\n",
    "              'action_space': {'A':[0,1,2]}}] #A in [0,1,2]\n",
    "ALearn.train(S, A, R, model_info, T=1, bootstrap = True, n_bs = 100)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.predict_value_boots(S)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3f808",
   "metadata": {},
   "source": [
    "**Interpretation:** Based on the boostrap with 200 replicates, the 'Value_hat' is the estimated value of the optimal policy, and the 'Value_std' is the corresponding standard error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc625f58",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "43f47b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "116.37488160184644"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#1. specify the fixed regime to be tested\n",
    "# For example, regime d = 0 for all subjects\n",
    "N=len(S)\n",
    "regime = pd.DataFrame({'A':[0]*N})\n",
    "#2. evaluate the regime\n",
    "ALearn = ALearning.ALearning()\n",
    "model_info = [{'X_prop': [0,1,2], #[0,1,2] here stands for the intercept, recency and history\n",
    "              'X_q0': [0,1,2],\n",
    "               'X_C':{1:[0,1,2],2:[0,1,2]},\n",
    "              'action_space': {'A':[0,1,2]}}] #A in [0,1,2]\n",
    "ALearn.train(S, A, R, model_info, T=1, regime = regime, evaluate = True)\n",
    "ALearn.predict_value(S)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e373ffc",
   "metadata": {},
   "source": [
    "**Interpretation:** the estimated value of the regime that always sends no emails ($A=0$) is 116.37, under the specified model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2ddcac0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 117.37346454667266 Value_std: 9.739458412078504\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "ALearn.train(S, A, R, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.predict_value_boots(S)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2845956f",
   "metadata": {},
   "source": [
    "**Interpretation:** the 'Value_hat' is the bootstrapped estimated value of the regime that always sends no emails, and the 'Value_std' is the correspoding bootstrapped standard error, under the specified model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "678b0a84",
   "metadata": {},
   "source": [
    "ðŸ’¥ Placeholder for C.I."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb76d35c",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Schulte, P. J., Tsiatis, A. A., Laber, E. B., & Davidian, M. (2014). Q-and A-learning methods for estimating optimal dynamic treatment regimes. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4), 640.\n",
    "2. Robins, J. M. (2004). Optimal structural nested models for optimal sequential decisions. In Proceedings of the second seattle Symposium in Biostatistics (pp. 189-326). Springer, New York, NY.\n",
    "3. Murphy, S. A. (2003). Optimal dynamic treatment regimes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(2), 331-355.\n",
    "4. Liang, S., Lu, W., & Song, R. (2018). Deep advantage learning for optimal dynamic treatment regime. Statistical theory and related fields, 2(1), 80-88.\n",
    "5. Shi, C., Fan, A., Song, R., & Lu, W. (2018). High-dimensional A-learning for optimal dynamic treatment regimes. Annals of statistics, 46(3), 925."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf1a1aa",
   "metadata": {},
   "source": [
    "## A.1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\frac{\\partial C_{j}(\\boldsymbol{S}_{i};\\boldsymbol{\\psi}_{j})}{\\partial \\boldsymbol{\\psi}_{j}}\\{\\mathbb{I}\\{A_{i}=j\\}-\\omega(\\boldsymbol{S}_{i},j;\\hat{\\boldsymbol{\\gamma}})\\}\\times \\Big\\{R_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(\\boldsymbol{S}_{i;\\boldsymbol{\\psi}_{j'}})-Q(\\boldsymbol{S}_{i},0;\\boldsymbol{\\phi})\\Big\\}=0\\\\\n",
    "&\\sum_{i=1}^n \\frac{\\partial Q(\\boldsymbol{S}_{i},0;\\boldsymbol{\\phi})}{\\partial \\boldsymbol{\\phi}}\\Big\\{R_i-\\sum_{j'=1}^{m-1} \\mathbb{I}\\{A_{i}=j'\\}C_{j'}(\\boldsymbol{S}_{i};\\boldsymbol{\\psi}_{j'}) Q(\\boldsymbol{S}_{i},0;\\boldsymbol{\\phi})\\Big\\}=0\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c94a2-47ee-45a8-8bd8-a2a983da3138",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}