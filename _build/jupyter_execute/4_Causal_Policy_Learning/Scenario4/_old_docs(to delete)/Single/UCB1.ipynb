{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da27717f",
   "metadata": {},
   "source": [
    "# UCB1\n",
    "\n",
    "## Main Idea\n",
    "As the name suggested, the UCB algorithm estimates the upper confidence bound $U_{i}^{t}$ of the mean rewards based on the observations and then choose the action has the highest estimates. The class of UCB-based algorithms is firstly introduced by Auer et al. [2]. Generally, at each round $t$, $U_{i}^{t}$ is calculated as the sum of the estimated reward (exploitation) and the estimated confidence radius (exploration) of item $i$ based on $\\mathcal{H}_{t}$. Then, $A_{t}$ is selected as \n",
    "\\begin{equation}\n",
    "    A_t = argmax_{a \\in \\mathcal{A}} E(R_t \\mid a,\\{ U_{i}^{t}\\}_{i=1}^{N}).\n",
    "\\end{equation} \n",
    "\n",
    "Doing so, either the item with a large average reward or the item with limited exploration will be selected.\n",
    "\n",
    "\n",
    "## Algorithms Details\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. The UCB1 algorithm start with initializing the estimated upper confidence bound $U_a^{0}$ and the count of being pulled $C_a^{0}$ for each action $a$ as 0. At each round $t$, we greedily select an action $A_t$ as \n",
    "\\begin{align}\n",
    "A_t = arg max_{a\\in \\mathcal{A}} U_{a}^{t}.\n",
    "\\end{align}\n",
    "\n",
    "After observing the rewards corresponding to the selected action $A_t$, we first update the total number of being pulled for $A_t$ accordingly. Then, we estimate the upper confidence bound for each action $a$ as\n",
    "\\begin{align}\n",
    "U_{a}^{t+1} = \\frac{1}{C_a^{t+1}}\\sum_{t'=0}^{t}R_{t'}I(A_{t'}=a) + \\sqrt{\\frac{2*log(t+1)}{C_a^{t+1}}} \n",
    "\\end{align}, where $R_{t'}$ is the reward received at round $t'$. Intuitively, $U_{a}^{t}$ is the sum of the sample average reward of action $a$ for expolitation and a confidence radius for exploration.\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Initializing the $\\boldsymbol{U}^0$ and $\\boldsymbol{C}^0$ for $K$ items as 0\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "\n",
    "    2.1. select action $A_t$ as the arm with the maximum $U_a^t$\n",
    "    \n",
    "    2.2. Received the reward R, and update $C$ and $U$ with\n",
    "    \\begin{align}\n",
    "    C_{A_{t}}^{t+1} &= C_{A_{t}}^{t} + 1 \\\\\n",
    "    U_{A_{t}}^{t+1} &= \\frac{1}{C_a^{t+1}}\\sum_{t'=0}^{t}R_{t'}I(A_{t'}=a) + \\sqrt{\\frac{2*log(t+1)}{C_a^{t+1}}} \n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ea26f1d",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "98d5089c",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_52266/3636065689.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/nas/longleaf/home/lge/CausalDM'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;31m# code used to import the learner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "357df43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Single import UCB1\n",
    "from causaldm.learners.Online.Single import Env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82b3bed6",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 5\n",
    "with_intercept = True\n",
    "p=3\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_theta = sigma_gamma = np.identity(p)\n",
    "mu_theta = np.zeros(p)\n",
    "seed = 0\n",
    "sigma = 1\n",
    "\n",
    "env = Env.Single_Gaussian_Env(T, K, p, sigma\n",
    "                         , mu_theta, Sigma_theta\n",
    "                        , seed = 42, with_intercept = True\n",
    "                         , X_mu = X_mu, X_Sigma = X_sigma)\n",
    "UCB_agent = UCB1.UCB1(K)\n",
    "A = UCB_agent.take_action()\n",
    "t = 0\n",
    "R = env.get_reward(t,A)\n",
    "UCB_agent.receive_reward(t,A,R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6be5c2b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.1249,  0.    ,  0.    ,  0.    ,  0.    ])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "UCB_agent.Rs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51268d4e",
   "metadata": {},
   "source": [
    "**Interpretation:** A sentence to include the analysis result: the estimated optimal regime is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09757b68",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Russo, D. J., Van Roy, B., Kazerouni, A., Osband, I., & Wen, Z. (2018). A tutorial on thompson sampling. Foundations and Trends® in Machine Learning, 11(1), 1-96.\n",
    "\n",
    "[2] Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235–256.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f759c579",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}