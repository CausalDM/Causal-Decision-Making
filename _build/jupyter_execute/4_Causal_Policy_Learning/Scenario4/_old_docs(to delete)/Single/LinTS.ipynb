{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "99f0f99f",
   "metadata": {},
   "source": [
    "# LinTS\n",
    "\n",
    "## Main Idea\n",
    "(Need to rewrite)Thompson Sampling, also known as posterior sampling, solves the exploration-exploitation dilemma by selecting an action according to its posterior distribution [8].  At each round $t$, the agent sample the rewards from the corresponding posterior distributions and then select the action with the highest sampled reward greedily. It has been shown that, when the true reward distribution is known, a TS algorithm with the true reward distribution as the prior is nearly optimal [9]. However, such a distribution is always unknown in practice. Therefore, one of the major objectives of TS-based algorithms is to find an informative prior to guide the exploration.\n",
    "\n",
    "\n",
    "## Algorithms Details\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. Noticing that feature information are commonly avialable, the LinTS algorithm consdiers modeling the mean reward with items' features. As an example, considering the Gaussian reward, we assume that \n",
    "\\begin{align}\n",
    "\\theta_{i} = \\boldsymbol{x}_i^T \\boldsymbol{\\gamma}.\n",
    "\\end{align}\n",
    "As for the Bernoulli reward, we assume that \n",
    "\\begin{align}\n",
    "\\theta_{i} = logistic(\\boldsymbol{x}_i^T \\boldsymbol{\\gamma})\n",
    "\\end{align}, where $logistic(x) \\equiv 1 / (1 + exp^{-1}(x))$.\n",
    "Similar as the standard TS algorithm, the LinTS algorithm starts with specifying a prior distribution of the parameter $\\boldsymbol{\\gamma}$, and a variance of the reward, based on the domian knowledge. At each round $t$, the agent will samples a vector of $\\tilde{\\boldsymbol{\\gamma}}^{t}$ from thecorresponding posterior distribution, and the mean reward $\\tilde{\\boldsymbol{\\theta}}^{t}$ is then calculated accordingly. The action $a$ with the greatest $\\tilde{\\theta}_{a}^{t}$ is then selected. Finally, the posterior distribution would be updated after receiving the feedback at the end of each round. Note that the posterior updating step differs for different pairs of prior distribution of the mean reward and reward distribution. Note that code can be easily modified to different specifications of the prior/reward distribution.\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Specifying a prior distirbution of $\\boldsymbol{\\gamma}$, and the variance of the reward distribution.\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "    - sample a $\\tilde{\\boldsymbol{\\gamma}}^{t}$ from the posterior distribution of $\\boldsymbol{\\gamma}$ or the prior distribution of it if in round $0$\n",
    "    - calculated the $\\tilde{\\boldsymbol{\\theta}}^{t}$ based on the assumed linear relationship\n",
    "    - select action $A_t$ which has the greatest $\\tilde{\\theta}_{a}$, i.e. $A_t = argmax_{a \\in \\mathcal{A}} \\tilde{\\theta}_{a}^{t}$\n",
    "    - receive the rewad $R$, and update the posterior distirbution of $\\boldsymbol{\\gamma}$ accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "690f815e",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0755b547",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      5\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 6\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/nas/longleaf/home/lge/CausalDM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/nas/longleaf/home/lge/CausalDM'"
     ]
    }
   ],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af138495",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Single import LinTS\n",
    "from causaldm.learners.Online.Single import Env\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47c0009b",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 5\n",
    "with_intercept = True\n",
    "p=3\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_theta = sigma_gamma = np.identity(p)\n",
    "mu_theta = np.zeros(p)\n",
    "seed = 0\n",
    "sigma = 1\n",
    "\n",
    "env = Env.Single_Gaussian_Env(T, K, p, sigma\n",
    "                         , mu_theta, Sigma_theta\n",
    "                        , seed = 42, with_intercept = True\n",
    "                         , X_mu = X_mu, X_Sigma = X_sigma)\n",
    "LinTS_Gaussian_agent = LinTS.LinTS_Gaussian(sigma = 1\n",
    "                                         , prior_theta_u = np.zeros(p), prior_theta_cov = np.identity(p)\n",
    "                                         , K = K, p = p)\n",
    "A = LinTS_Gaussian_agent.take_action(env.Phi)\n",
    "t = 0\n",
    "R = env.get_reward(t,A)\n",
    "LinTS_Gaussian_agent.receive_reward(t,A,R, env.Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c11f480f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinTS_Gaussian_agent.cnts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f2f1e3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 2000\n",
    "K = 5\n",
    "with_intercept = True\n",
    "p=3\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_theta = sigma_gamma = np.identity(p)\n",
    "mu_theta = np.zeros(p)\n",
    "seed = 0\n",
    "phi_beta = 1/4\n",
    "\n",
    "env = Env.Single_Bernoulli_Env(T, K, p, phi_beta\n",
    "                         , mu_theta, Sigma_theta\n",
    "                        , seed = 42, with_intercept = True\n",
    "                         , X_mu = X_mu, X_Sigma = X_sigma)\n",
    "LinTS_Bernoulli_agent = LinTS.LinTS_Bernoulli(K = K, p = p , alpha = 1, retrain_freq = 1)\n",
    "A = LinTS_Bernoulli_agent.take_action(env.Phi)\n",
    "t = 0\n",
    "R = env.get_reward(t,A)\n",
    "LinTS_Bernoulli_agent.receive_reward(t,A,R, env.Phi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "35cfbf5b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 1., 0., 0., 0.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LinTS_Bernoulli_agent.cnts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5891e2b",
   "metadata": {},
   "source": [
    "**Interpretation:** A sentence to include the analysis result: the estimated optimal regime is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e47845",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Agrawal, S., & Goyal, N. (2013, May). Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning (pp. 127-135). PMLR.\n",
    "\n",
    "[2] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., & Boutilier, C. (2020, June). Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics (pp. 2066-2076). PMLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}