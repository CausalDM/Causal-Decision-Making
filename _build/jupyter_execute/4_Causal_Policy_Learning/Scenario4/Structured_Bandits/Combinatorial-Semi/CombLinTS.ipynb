{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3c50dd25",
   "metadata": {},
   "source": [
    "# CombLinTS\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is scalable when the features are used. It outperforms algorithms based on other frameworks, such as UCB, in practice.\n",
    "- **Disadvantage**: It is susceptible to model misspecification.\n",
    "- **Application Situation**: Useful when presenting a list of items, each of which will generate a partial outcome (reward). The outcome is continuous.\n",
    "\n",
    "## Main Idea\n",
    "Noticing that feature information are common in practice, Wen et al. (2015) considers a generalization across items to reach a lower regret bound independent of $N$, by assuming a linear generalization model for $\\boldsymbol{\\theta}$. Specifically, we assume that\n",
    "\\begin{equation}\n",
    "\\theta_{i} = \\boldsymbol{s}_{i,t}^{T}\\boldsymbol{\\gamma}.\n",
    "\\end{equation}\n",
    "At each round $t$, **CombLinTS** samples $\\tilde{\\boldsymbol{\\gamma}}_{t}$ from the updated posterior distribution $N(\\hat{\\boldsymbol{\\gamma}}_{t},\\hat{\\Sigma}_{t})$ and get the $\\tilde{\\theta}_{i}^{t}$ as $\\boldsymbol{s}_{i,t}^{T}\\tilde{\\boldsymbol{\\gamma}}_{t}$, where $\\hat{\\boldsymbol{\\gamma}}_{t}$ and $\\hat{\\Sigma}_{t}$ are updated by the Kalman Filtering algorithm[1]. Note that when the outcome distribution $\\mathcal{P}$ is Gaussian, the updated posterior distribution is the exact posterior distribution of $\\boldsymbol{\\gamma}$ as **CombLinTS** assumes a Gaussian Prior. \n",
    "\n",
    "It's also important to note that, if necessary, the posterior updating step can be simply changed to accommodate various prior/reward distribution specifications. Further, for simplicity, we consider the most basic size constraint such that the action space includes all the possible subsets with size $K$. Therefore, the optimization process to find the optimal subset $A_{t}$ is equal to selecting a list of $K$ items with the highest attractiveness factors. Of course, users are welcome to modify the **optimization** function to satisfy more complex constraints.\n",
    "\n",
    "## Key Steps\n",
    "For round $t = 1,2,\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$ with a Gaussian prior;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$;\n",
    "3. Update $\\tilde{\\boldsymbol{\\theta}}$ as $\\boldsymbol{s}_{i,t}^T \\tilde{\\boldsymbol{\\gamma}}$;\n",
    "5. Take the action $A_{t}$ w.r.t $\\tilde{\\boldsymbol{\\theta}}$ such that $A_t = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid \\tilde{\\boldsymbol{\\theta}})$;\n",
    "6. Receive reward $R_{t}$.\n",
    "\n",
    "*Notations can be found in either the inroduction of the chapter \"Structured Bandits\" or the introduction of the combinatorial Semi-Bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa0d14b",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "728a9324",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:\\\\GitHub\\\\CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-a4f395ec17bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetcwd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'D:\\GitHub\\CausalDM'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:\\\\GitHub\\\\CausalDM'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\GitHub\\CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dd94b4-81a0-4171-9a37-e62add6fd99d",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "074a9ca0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.Online.Structured_Bandits.Combinatorial_Semi import CombLinTS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef70e64c-c506-42bd-97e3-fe0950c6e928",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the Adult dataset. The length of horizon, $T$, is specified as $500$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b194b43-6741-481a-96c9-f73b81693c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Structured_Bandits.Combinatorial_Semi import _env_realComb as _env\n",
    "env = _env.CombSemi_env(T = 500, seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328596a2-5deb-47ac-ae63-5acfaa0e3960",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "- K: number of itmes to be recommended at each round\n",
    "- L: total number of candidate items\n",
    "- p: number of features (If the intercept is considerd, p includes the intercept as well.)\n",
    "- sigma: standard deviation of reward distribution (Note: we assume that the observed reward's random noise comes from the same distribution for all items.)\n",
    "- prior_gamma_mu: mean of the Gaussian prior of the $\\boldsymbol{\\gamma}$\n",
    "- prior_gamma_cov: the covariance matrix of the Gaussian prior of $\\boldsymbol{\\gamma}$\n",
    "- seed: random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14c2ed83",
   "metadata": {},
   "outputs": [],
   "source": [
    "L = env.L\n",
    "K = 10\n",
    "p = env.p\n",
    "sigma = 1\n",
    "prior_gamma_mu = np.zeros(p)\n",
    "prior_gamma_cov = np.identity(p)\n",
    "seed = 0\n",
    "LinTS_agent = CombLinTS.LinTS_Semi(sigma = sigma, prior_gamma_mu = prior_gamma_mu, \n",
    "                                   prior_gamma_cov = prior_gamma_cov,L = L, K = K, \n",
    "                                   p = p, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddb65c4-16c2-4eff-85c8-c503b95a6f40",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "We fisrt observe the feature information $X$ by\n",
    "<code> X = env.Phi </code>. (Note: if an intercept is considered, the X should include a column of ones).\n",
    "Starting from t = 0, for each step t, there are three steps:\n",
    "1. Recommend an action (a set of ordered restaturants)\n",
    "<code> A = LinTS_agent.take_action(X) </code>\n",
    "2. Get the reward of each item recommended from the environment\n",
    "<code> R, _, tot_R = env.get_reward(A, t) </code>\n",
    "3. Update the posterior distribution\n",
    "<code> LinTS_agent.receive_reward(t, A, R, X) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "acf95af3-dc5e-4969-a7b9-2602f5d81ce9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0,\n",
       " array([ 480, 1895, 1700, 2219, 2807, 1593, 2784,  172, 2831, 1523]),\n",
       " array([ 0.8214, -0.2055, -1.408 , -0.0487, -0.8551,  1.1778, -0.595 ,\n",
       "         0.9068,  0.6194,  1.9444]),\n",
       " 2.3574891974648375)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = env.Phi\n",
    "t = 0\n",
    "A = LinTS_agent.take_action(X)\n",
    "R, _, tot_R = env.get_reward(A, t)\n",
    "LinTS_agent.receive_reward(t, A, R, X)\n",
    "t, A, R, tot_R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cf743e",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the agent decides to send the advertisement to 10 potential customers (480, 1895, 1700, 2219, 2807, 1593, 2784,  172, 2831, 1523), and then receives a total reward of $2.36$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7723d78",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Wen, Z., Kveton, B., & Ashkan, A. (2015, June). Efficient learning in large-scale combinatorial semi-bandits. In International Conference on Machine Learning (pp. 1113-1122). PMLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}