{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# CascadeLinTS\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is scalable when the features are used. It outperforms algorithms based on other frameworks, such as UCB, in practice.\n",
    "- **Disadvantage**: It is susceptible to model misspecification.\n",
    "- **Application Situation**: Useful when presenting a ranked list of items, with only one selected at each interaction. The outcome is binary.\n",
    "\n",
    "## Main Idea\n",
    "\n",
    "Motivated by observations in most real-world applications, which have a large number of candidate items, Zong et al. (2016) proposed using feature information that is widely available to improve learning efficiency. Utilizing the feature information of each item $i$, **CascadeLinTS** [1] characterize $\\theta_{i}=E[W_t(i)]$ by assuming that\n",
    "\\begin{equation}\n",
    "\\theta_{i} = logistic(\\boldsymbol{s}_{i,t}^T \\boldsymbol{\\gamma}),\n",
    "\\end{equation}where $logistic(c) \\equiv 1 / (1 + exp^{-1}(c))$. \n",
    "\n",
    "Similar to the Thompson Sampling algorithm with generalized linear bandits [2], we approximate the posterior distribution of $\\boldsymbol{\\gamma}$ by its Laplace approximation. Specifically, we approximate the posterior of $\\boldsymbol{\\gamma}$ as:\n",
    "\\begin{equation}\n",
    "    \\begin{split}\n",
    "    \\tilde{\\boldsymbol{\\gamma}}^{t} &\\sim \\mathcal{N}\\Big(\\hat{\\boldsymbol{\\gamma}}_{t}, \\alpha^2 \\boldsymbol{H}_{t}^{-1}\\Big),\\\\\n",
    "    \\boldsymbol{H}_{t} &= \\sum_{t}\\mu'(\\boldsymbol{S}_{t}^{T}\\hat{\\boldsymbol{\\gamma}}^{t})\\boldsymbol{S}_{t}\\boldsymbol{S}_{t}^{T},\n",
    "    \\end{split}\n",
    "\\end{equation} where $\\alpha$ is a pre-specified constant to control the degree of exploration, and $\\mu'(\\cdot)$ is the derivative of the mean function. It should be noted that the posterior updating step differs for different pairs of the prior distribution of $\\boldsymbol{\\gamma}$ and the reward distribution, and the code can be easily modified to different prior/reward distribution specifications if necessary.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "For round $t = 1,2,\\cdots$:\n",
    "1. Approximate $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$ by the Laplace approximation;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$;\n",
    "3. Update $\\tilde{\\boldsymbol{\\theta}}$ as $logistic(\\boldsymbol{s}_{i,t}^T \\tilde{\\boldsymbol{\\gamma}})$;\n",
    "5. Take the action $A_{t}$ w.r.t $\\tilde{\\boldsymbol{\\theta}}$ such that $A_t = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid \\tilde{\\boldsymbol{\\theta}})$;\n",
    "6. Receive reward $R_{t}$.\n",
    "\n",
    "*Notations can be found in either the inroduction of the chapter \"Structured Bandits\" or the introduction of the cascading Bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\GitHub\\\\CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 3\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCausalDM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\GitHub\\\\CausalDM'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\GitHub\\CausalDM')"
   ]
  },
  {
=======
>>>>>>> Stashed changes
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.CPL4.Structured_Bandits.Cascade import CascadeLinTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the Yelp dataset. The number of items recommended at each round, $K$, is specified as $3$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\anaconda3\\\\lib\\\\site-packages\\\\causaldm\\\\learners\\\\CPL4\\\\Structured_Bandits\\\\Cascade\\\\data\\\\Cascading_realdata_d_10_X_transform_standardize_with_intercept_1'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCPL4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mStructured_Bandits\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCascade\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _env_realCascade \u001b[38;5;28;01mas\u001b[39;00m _env\n\u001b[1;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCascading_env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mK\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\_util_online.py:223\u001b[0m, in \u001b[0;36mautoargs.<locals>._autoargs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sieve(attr):\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, val)\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\Structured_Bandits\\Cascade\\_env_realCascade.py:14\u001b[0m, in \u001b[0;36mCascading_env.__init__\u001b[1;34m(self, K, seed)\u001b[0m\n\u001b[0;32m     12\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     13\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m---> 14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW_test, X, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mp, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwith_intercept, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mL \u001b[38;5;241m=\u001b[39m \u001b[43mget_Yelp\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m=\u001b[39m K\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mPhi \u001b[38;5;241m=\u001b[39m X\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\Structured_Bandits\\Cascade\\_env_realCascade.py:35\u001b[0m, in \u001b[0;36mget_Yelp\u001b[1;34m()\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_Yelp\u001b[39m():\n\u001b[0;32m     34\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/Cascading_realdata_d_10_X_transform_standardize_with_intercept_1\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 35\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m     36\u001b[0m         Yelp \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fp) \n\u001b[0;32m     38\u001b[0m     dataset_path \u001b[38;5;241m=\u001b[39m pkg_resources\u001b[38;5;241m.\u001b[39mresource_filename(\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/W_test.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\anaconda3\\\\lib\\\\site-packages\\\\causaldm\\\\learners\\\\CPL4\\\\Structured_Bandits\\\\Cascade\\\\data\\\\Cascading_realdata_d_10_X_transform_standardize_with_intercept_1'"
     ]
    }
   ],
   "source": [
    "from causaldm.learners.CPL4.Structured_Bandits.Cascade import _env_realCascade as _env\n",
    "env = _env.Cascading_env(K = 3, seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "- K: number of itmes to be recommended at each round\n",
    "- L: total number of candidate items\n",
    "- p: number of features (If the intercept is considerd, p includes the intercept as well.)\n",
    "- alpha: degree of exploration (default = 1)\n",
    "- retrain_freq: frequency to train the generalized linear model (i.e., update every retrain_freq steps)\n",
    "- seed: random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = env.K\n",
    "L = env.L\n",
    "p = env.p\n",
    "alpha = 1\n",
    "retrain_freq = 1\n",
    "seed = 0\n",
    "LinTS_agent = CascadeLinTS.CascadeLinTS(K = K, L = L, p = p, alpha = alpha, \n",
    "                                        retrain_freq = retrain_freq, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "We fisrt observe the feature information $\\boldsymbol{S}$ by\n",
    "<code> S = env.Phi </code>. (Note: if an intercept is considered, the S should include a column of ones). Starting from t = 0, for each step t, there are three steps:\n",
    "1. Recommend an action (a set of ordered restaturants)\n",
    "<code> A = LinTS_agent.take_action(S) </code>\n",
    "2. Get the reward from the environment (i.e., $W$, $E$, and $R$)\n",
    "<code> W,E,R = env.get_reward(A) </code>\n",
    "3. Update the posterior distribution\n",
    "<code> LinTS_agent.receive_reward(A,W,E,t,S) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1301, 2087, 1123]), array([0., 0., 0.]), array([1., 1., 1.]), 0.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "S = env.Phi\n",
    "A = LinTS_agent.take_action(S)\n",
    "W,E,R = env.get_reward(A)\n",
    "LinTS_agent.receive_reward(A,W,E,t,S)\n",
    "A, W, E, R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the agent decides to display three top restaurants, the first of which is restaurant 1301, the second is restaurant 2087, and the third is restaurant 1123. Unfortunately, the customer does not show any interest in any of the recommended restaurants. As a result, the agent receives a zero reward at round $0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Zong, S., Ni, H., Sung, K., Ke, N. R., Wen, Z., & Kveton, B. (2016). Cascading bandits for large-scale recommendation problems. arXiv preprint arXiv:1603.05359.\n",
    "\n",
    "[2] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., & Boutilier, C. (2020, June). Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics (pp. 2066-2076). PMLR."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.11.3"
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}