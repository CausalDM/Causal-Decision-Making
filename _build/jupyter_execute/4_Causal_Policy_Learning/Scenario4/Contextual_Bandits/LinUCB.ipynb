{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3ca32e0",
   "metadata": {},
   "source": [
    "# LinUCB\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is more scalable and efficient than **UCB** by utilizing features.\n",
    "- **Disadvantage**:  \n",
    "- **Application Situation**: discrete action space, Gaussian reward space\n",
    "\n",
    "## Main Idea\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. **LinUCB**[1] uses feature information to guide exploration by assuming a linear model between the expected potential reward and the features. Specifcially, for the Gaussain bandits, we assume that \n",
    "\\begin{align}\n",
    "E(R_{t}(a)) = \\theta_a = \\boldsymbol{s}_a^T \\boldsymbol{\\gamma}.\n",
    "\\end{align} Solving a linear gression model, at each round $t$, the corresponding estimated upper confidence interval of the mean potential reward is then updated as\n",
    "\\begin{align}\n",
    "U_a^t = \\boldsymbol{s}_a^T \\hat{\\boldsymbol{\\gamma}} + \\alpha\\sqrt{\\boldsymbol{s}_a^T \\boldsymbol{V}^{-1}  \\boldsymbol{s}_a},\n",
    "\\end{align} where $\\alpha$ is a tuning parameter that controls the rate of exploration, $\\boldsymbol{V}^{-1} = \\sum_{j=0}^{t-1}\\boldsymbol{s}_{a_j}\\boldsymbol{s}_{a_j}^T$, and $\\hat{\\boldsymbol{\\gamma}} = \\boldsymbol{V}^{-1}\\sum_{j=0}^{t-1}\\boldsymbol{s}_{a_j}R_j$.\n",
    "\n",
    "As for the Bernoulli bandits, we assume that \n",
    "\\begin{align}\n",
    "\\theta_{a} = logistic(\\boldsymbol{s}_a^T \\boldsymbol{\\gamma}),\n",
    "\\end{align}where $logistic(c) \\equiv 1 / (1 + exp^{-1}(c))$. At each round $t$, by fitting a generalized linear model to all historical observations, we obtain the maximum likelihood estimator of $\\boldsymbol{\\gamma}$. The corresponding estimated confidence upper bound is then calculated in the same way as for Gaussian bandits, such that\n",
    "\\begin{align}\n",
    "U_a^t = \\boldsymbol{s}_a^T \\hat{\\boldsymbol{\\gamma}} + \\alpha\\sqrt{\\boldsymbol{s}_a^T \\boldsymbol{V}^{-1}  \\boldsymbol{s}_a},\n",
    "\\end{align}where $\\alpha$ and $\\boldsymbol{V}$ are defined in the same way as before. \n",
    "\n",
    "Finally, using the estimated upper confidence bounds, $A_t = \\arg \\max_{a \\in \\mathcal{A}} U_a^t$ would be selected.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Initializing $\\hat{\\boldsymbol{\\gamma}}=\\boldsymbol{0}$ and $\\boldsymbol{V} = I$, and specifying $\\alpha$;\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "    - Calculate the upper confidence bound $U_a^t$;\n",
    "    - Select action $A_t$ as the arm with the maximum $U_a^t$;\n",
    "    - Receive the reward R, and update $\\hat{\\boldsymbol{\\gamma}}$, $V$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea1be59",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
<<<<<<< Updated upstream
   "cell_type": "code",
   "execution_count": 1,
   "id": "f3b02f48",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\GitHub\\\\CausalDM'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      2\u001b[0m os\u001b[38;5;241m.\u001b[39mgetcwd()\n\u001b[0;32m----> 3\u001b[0m os\u001b[38;5;241m.\u001b[39mchdir(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mD:\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mGitHub\u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mCausalDM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\GitHub\\\\CausalDM'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\GitHub\\CausalDM')"
   ]
  },
  {
=======
>>>>>>> Stashed changes
   "cell_type": "markdown",
   "id": "bbd7dbaf-2ee9-443b-94a2-aa1ee984bd4a",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "291f88a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.CPL4.CMAB import LinUCB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb597375-41f0-4ed6-b20b-4589ce044178",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the MovieLens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9c17056",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'D:\\\\anaconda3\\\\lib\\\\site-packages\\\\data/MovieLens_MTTS_1M.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCPL4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCMAB\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _env_realCMAB \u001b[38;5;28;01mas\u001b[39;00m _env\n\u001b[1;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSingle_Contextual_Env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mBinary\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\_util_online.py:223\u001b[0m, in \u001b[0;36mautoargs.<locals>._autoargs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sieve(attr):\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, val)\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\CMAB\\_env_realCMAB.py:13\u001b[0m, in \u001b[0;36mSingle_Contextual_Env.__init__\u001b[1;34m(self, seed, Binary, leave_drama)\u001b[0m\n\u001b[0;32m     11\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[0;32m     12\u001b[0m random\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m---> 13\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdat \u001b[38;5;241m=\u001b[39m \u001b[43mget_movielens\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mN \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdat[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mXs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleave_drame \u001b[38;5;241m=\u001b[39m leave_drama\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\CMAB\\_env_realCMAB.py:67\u001b[0m, in \u001b[0;36mget_movielens\u001b[1;34m()\u001b[0m\n\u001b[0;32m     65\u001b[0m main_folder_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mnormpath(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(current_script_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m..\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     66\u001b[0m data_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(main_folder_dir, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/MovieLens_MTTS_1M.txt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m     68\u001b[0m     MovieLens \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fp) \n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MovieLens\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'D:\\\\anaconda3\\\\lib\\\\site-packages\\\\data/MovieLens_MTTS_1M.txt'"
     ]
    }
   ],
   "source": [
    "from causaldm.learners.CPL4.CMAB import _env_realCMAB as _env\n",
    "env = _env.Single_Contextual_Env(seed = 0, Binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054854bf-79c3-4e11-a729-3f84d9c5fc96",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "\n",
    "- K: number of arms\n",
    "- p: number of features per arm\n",
    "- alpha: rate of exploration\n",
    "- seed: random seed\n",
    "- exploration_T: number of rounds to do random exploration at the beginning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc79fede",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = .1\n",
    "K = env.K\n",
    "p = env.p\n",
    "seed = 42\n",
    "exploration_T = 10\n",
    "LinUCB_Gaussian_agent = LinUCB.LinUCB_Gaussian(alpha = .1, K = K, p = p, seed = seed, exploration_T = exploration_T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0435f837-3189-455e-b430-2b081b59c6e0",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "\n",
    "Starting from t = 0, for each step t, there are three steps:\n",
    "1. Observe the feature information\n",
    "<code> X = env.get_Phi(t) </code>\n",
    "2. Recommend an action \n",
    "<code> A = LinUCB_Gaussian_agent.take_action(X) </code>\n",
    "3. Get the reward from the environment \n",
    "<code> R = env.get_reward(t,A) </code>\n",
    "4. Update the posterior distribution\n",
    "<code> LinUCB_Gaussian_agent.receive_reward(t,A,R,X) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c8a8d30-eeaf-4990-bc61-0e4e6a253d03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 4, 4.0, (25.0, 1.0, 'college/grad student'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "X, feature_info = env.get_Phi(t)\n",
    "A = LinUCB_Gaussian_agent.take_action(X)\n",
    "R = env.get_reward(t,A)\n",
    "LinUCB_Gaussian_agent.receive_reward(t,A,R,X)\n",
    "t,A,R,feature_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3aefb9c5-1fac-456b-84b5-160d804a1534",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the TS agent encounter a male user who is a 25-year-old college/grad student. Given the information, the agent recommend a Sci-Fi (arm 4), and receive a rate of 4 from the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "056fa1ee-35f0-4b84-bbf8-cc9931b94ba9",
   "metadata": {},
   "source": [
    "### Demo Code for Bernoulli Bandit\n",
    "The steps are similar to those previously performed with a Gaussian Bandit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f4c039a8-b99a-493d-9eb1-aaab7edd929c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 0, (25.0, 1.0, 'college/grad student'))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = _env.Single_Contextual_Env(seed = 0, Binary = True)\n",
    "K = env.K\n",
    "p = env.p\n",
    "seed = 42\n",
    "alpha = 1 # exploration rate\n",
    "retrain_freq = 1 #frequency to train the GLM model\n",
    "exploration_T = 10\n",
    "LinUCB_GLM_agent = LinUCB.LinUCB_GLM(K = K, p = p , alpha = alpha, retrain_freq = retrain_freq, \n",
    "                                     seed = seed, exploration_T = exploration_T)\n",
    "t = 0\n",
    "X, feature_info = env.get_Phi(t)\n",
    "A = LinUCB_GLM_agent.take_action(X)\n",
    "R = env.get_reward(t,A)\n",
    "LinUCB_GLM_agent.receive_reward(t,A,R,X)\n",
    "t,A,R,feature_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5486b3d6-29af-4c4f-8ae7-d943d8d812e4",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the TS agent encounter a male user who is a 25-year-old college/grad student. Given the information, the agent recommend a Thriller (arm 3), and receive a rate of 0 from the user."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ee7a2a",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Chu, W., Li, L., Reyzin, L., & Schapire, R. (2011, June). Contextual bandits with linear payoff functions. In Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics (pp. 208-214). JMLR Workshop and Conference Proceedings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ec41cc-5469-4c5b-95dc-f44e1ed5b14d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< Updated upstream
   "version": "3.11.3"
=======
   "version": "3.9.12"
>>>>>>> Stashed changes
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}