{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "154bbf56",
   "metadata": {},
   "source": [
    "# $\\epsilon$-Greedy\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: Simple and easy to understand. Compared to random policy, it makes better use of observations. \n",
    "- **Disadvantage**:  It is difficult to determine an ideal $\\epsilon$: if $\\epsilon$ is large, exploration will dominate; otherwise, eploitation will dominate. To address this issue, we offer a more adaptive versionâ€”$\\epsilon_t$-greedy, where $\\epsilon_t$ decreases as $t$ increases.\n",
    "- **Application Situation**: discrete action space, binary/Gaussian reward space\n",
    "\n",
    "## Main Idea\n",
    "$\\epsilon$-Greedy is an intuitive algorithm to incorporate the exploration and exploitation. It is simple and widely used [1]. Specifically, at each round $t$, we will select a random action with probability $\\epsilon$, and select an action with the highest estimated mean potential reward, $\\theta_a$, for each arm $a$ based on the history so far with probability $1-\\epsilon$. Specifically,\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_a = \\hat{E}(R_t(a)|\\{A_t, R_t\\})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "For example, in movie recommendation, the agent would either recommend a random genere of movies to the user or recommend the genere that the user watched the most in the past. Here the parameter $\\epsilon$ is pre-specified. A more adaptive variant is $\\epsilon_{t}$-greedy, where the probability of taking a random action is defined as a decreasing function of $t$. Auer et al. [2] showed that $\\epsilon_{t}$-greedy performs well in practice with $\\epsilon_{t}$ decreases to 0 at a rate of $\\frac{1}{t}$. Note that, the reward can be either binary or continuous.\n",
    "\n",
    "\n",
    "## Algorithms Details\n",
    "Supposed there are $K$ options, and the action space is $\\mathcal{A} = \\{0,1,\\cdots, K-1\\}$. The $\\epsilon$-greedy algorithm start with initializing the estimated values $\\theta_a^0$ and the count of being pulled $C_a^0$ for each action $a$ as 0. At each round $t$, we either take an action with the maximum estimated value $\\theta_a$ with probability $1-\\epsilon_{t}$ or randomly select an action with probability $\\epsilon_t$. After observing the rewards corresponding to the selected action $A_t$, we updated the total number of being pulled for $A_t$, and estimated the $\\theta_{A_{t}}$ by with the sample average for $A_t$.\n",
    "\n",
    "Remark that both the time-adaptive and the time-fixed version of $\\epsilon$-greedy algorithm are provided. By setting **decrease_eps=True**, the $\\epsilon_{t}$ in round $t$ is calculated as $\\frac{K}{T}$. Otherwise, $\\epsilon_{t}$ is a fixed value specfied by users.\n",
    "\n",
    "## Key Steps\n",
    "\n",
    "1. Initializing the $\\boldsymbol{\\theta}^0$ and $\\boldsymbol{C}^0$ for $K$ items as 0\n",
    "2. For t = $0, 1,\\cdots, T$:\n",
    "\n",
    "    2.1. select action $A_t$ as the arm with the maximum $\\theta_a^t$ with probability $1-\\epsilon_t$, or randomly select an action $A_t$ with probability $\\epsilon_t$\n",
    "    \n",
    "    2.2. Received the reward $R_t$, and update $C$ and $Q$ with\n",
    "    \\begin{align}\n",
    "    C_{A_{t}}^{t+1} &= C_{A_{t}}^{t} + 1 \\\\\n",
    "    \\theta_{A_{t}}^{t+1} &=\\theta_{A_{t}}^{t} + \\frac{1}{C_{A_{t+1}}^{t+1}}*(R_t-\\theta_{A_{t}}^{t})\n",
    "    \\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "755d09cc",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d56b53-7d0c-41cb-9f01-c187f0588e40",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "be69a99f",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/vb5nb4rd5bx0gr1q5ytx9q600000gn/T/ipykernel_51350/43005494.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mcausaldm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlearners\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCPL4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMAB\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEpsilon_Greedy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/learners/CPL4/MAB/Epsilon_Greedy.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_base_online_learner\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBaseOnlineLearner\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_util_online\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mEpsilon_Greedy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBaseOnlineLearner\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     '''\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/learners/CPL4/_base_online_learner.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_util_online\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mBaseOnlineLearner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mautoargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/causaldm/learners/CPL4/_util_online.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataFrame\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhist\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/api.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdatasets\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdistributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0miolib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregression\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrobust\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtools\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m from .discrete.count_model import (\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/regression/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlinear_model\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myule_walker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtools\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_testing\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPytestTester\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0m__all__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'yule_walker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/regression/linear_model.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrapper\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mwrap\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memplike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0melregress\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_ELRegOpts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;31m# need import in module instead of lazily to copy `__doc__`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregression\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prediction\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPredictionResults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/statsmodels/emplike/elregress.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \"\"\"\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mstatsmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0memplike\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdescriptive\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_OptFuncts\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.9/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36m_compile_bytecode\u001b[0;34m(data, name, bytecode_path, source_path)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.CPL4.MAB import Epsilon_Greedy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09f430d5-555a-40ef-b523-03721ff124d6",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the MovieLens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa35b307-55b2-4d7c-8712-d0010ee6367e",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './causaldm/learners/Online/MovieLens_MTTS_1M.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcausaldm\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlearners\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mCPL4\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMAB\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _env_realMAB \u001b[38;5;28;01mas\u001b[39;00m _env\n\u001b[1;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43m_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSingle_Gaussian_Env\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\_util_online.py:223\u001b[0m, in \u001b[0;36mautoargs.<locals>._autoargs.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sieve(attr):\n\u001b[0;32m    222\u001b[0m             \u001b[38;5;28msetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, attr, val)\n\u001b[1;32m--> 223\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\MAB\\_env_realMAB.py:34\u001b[0m, in \u001b[0;36mSingle_Gaussian_Env.__init__\u001b[1;34m(self, seed)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed \u001b[38;5;241m=\u001b[39m seed\n\u001b[0;32m     33\u001b[0m np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mseed(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed)\n\u001b[1;32m---> 34\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_r\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\MAB\\_env_realMAB.py:37\u001b[0m, in \u001b[0;36mSingle_Gaussian_Env.get_r\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_r\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m---> 37\u001b[0m     true_r \u001b[38;5;241m=\u001b[39m \u001b[43mget_movielens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinary\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     38\u001b[0m     arm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(true_r\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[0;32m     39\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mK \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(arm)\n",
      "File \u001b[1;32mD:\\anaconda3\\lib\\site-packages\\causaldm\\learners\\CPL4\\MAB\\_env_realMAB.py:49\u001b[0m, in \u001b[0;36mget_movielens\u001b[1;34m(binary)\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_movielens\u001b[39m(binary\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m./causaldm/learners/Online/MovieLens_MTTS_1M.txt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[0;32m     50\u001b[0m         MovieLens \u001b[38;5;241m=\u001b[39m pickle\u001b[38;5;241m.\u001b[39mload(fp) \n\u001b[0;32m     51\u001b[0m     arm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(MovieLens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndividual\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;28mlist\u001b[39m(MovieLens[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIndividual\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39mkeys())[\u001b[38;5;241m1\u001b[39m:]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './causaldm/learners/Online/MovieLens_MTTS_1M.txt'"
     ]
    }
   ],
   "source": [
    "from causaldm.learners.CPL4.MAB import _env_realMAB as _env\n",
    "env = _env.Single_Gaussian_Env(seed = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "578c0ed6-cf1b-4371-835d-91ef1740b759",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "\n",
    "- K: # of arms\n",
    "- epsilon: fixed $\\epsilon$ for time-fixed version of $\\epsilon$-greedy algorithm\n",
    "- decrease_eps: indicate if a time-adaptive $\\epsilon_t = min(1,\\frac{K}{t})$ employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f6af03c-8b0f-4a99-9655-a6a8ec51f51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = env.K\n",
    "greedy_agent = Epsilon_Greedy.Epsilon_Greedy(K, epsilon = None, decrease_eps = True, seed = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1913901-18ff-47ef-94db-88ebe2d4ed07",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "\n",
    "Starting from t = 0, for each step t, there are three steps:\n",
    "1. Recommend an action \n",
    "<code> A = greedy_agent.take_action() </code>\n",
    "2. Get the reward from the environment \n",
    "<code> R = env.get_reward(t,A) </code>\n",
    "3. Update the posterior distribution\n",
    "<code> greedy_agent.receive_reward(t,A,R) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0a25499-4a62-42c9-b877-3c1f22a39c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 3, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = 0\n",
    "A = greedy_agent.take_action()\n",
    "R = env.get_reward(A)\n",
    "greedy_agent.receive_reward(t,A,R)\n",
    "t, A, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5367247d-4e7c-4aa0-b187-8fc149023d7f",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the $\\epsilon-$greedy agent recommend a Thriller (arm 3), and received a rate of 4 from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debb450e-4dd2-4c38-88d9-35c41502b95d",
   "metadata": {},
   "source": [
    "### Demo Code for Bernoulli Bandit\n",
    "The steps are similar to those previously performed with a Gaussian Bandit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5bf81158-9ea9-4355-991b-acbf5eb7eb54",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = _env.Single_Bernoulli_Env(seed=42)\n",
    "\n",
    "K = env.K\n",
    "greedy_agent = Epsilon_Greedy.Epsilon_Greedy(K, epsilon = None, decrease_eps = True, seed = 42)\n",
    "\n",
    "t = 0\n",
    "A = greedy_agent.take_action()\n",
    "R = env.get_reward(A)\n",
    "greedy_agent.receive_reward(t,A,R)\n",
    "t, A, R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd892f4-c030-4ad8-a87c-94f000193ea1",
   "metadata": {},
   "source": [
    "**Interpretation**: For step 0, the $\\epsilon-$greedy agent recommend a Comedy (arm 0), and received a reward of 1 from the environment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9304145e",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.\n",
    "\n",
    "[2] Auer, P., Cesa-Bianchi, N., & Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2), 235-256."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}