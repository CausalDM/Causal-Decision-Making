{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fef41c9",
   "metadata": {},
   "source": [
    "(section:online_RL)=\n",
    "# Ooline Policy Learning and Evaluation in Markovian Environments \n",
    "\n",
    "This chapter focuses on the online policy learning and evaluation problem in an Markov Decision Process (MDP), which is the most well-known and typically default setup of Reinforcement Learning (RL). \n",
    "From the causal perspective, the data dependency structure is the same with that in [offline RL](section:OPE_OPO_preliminary), with the major difference in that the data collection policy is now data-dependent and the objective is sometimes shifted from finding the optimal policy to maximizing the cumulative rewards. \n",
    "As this is a vast area with a huge literature, we do not aim to repeat the disucssions hear. Instead, we will focus on connecting online RL to the other parts of this paper. \n",
    "We refer interested readers to {cite:t}`sutton2018reinforcement` for more materials.\n",
    "\n",
    "## Model\n",
    "\n",
    "We first recap the infinite-horizon discounted MDP model that we introduced in [offline RL](section:OPE_OPO_preliminary). \n",
    "For any $t\\ge 0$, let $\\bar{a}_t=(a_0,a_1,\\cdots,a_t)^\\top\\in \\mathcal{A}^{t+1}$ denote a treatment history vector up to time $t$. \n",
    "Let $\\mathbb{S} \\subset \\mathbb{R}^d$ denote the support of state variables and $S_0$ denote the initial state variable. \n",
    "For any $(\\bar{a}_{t-1},\\bar{a}_{t})$, let $S_{t}^*(\\bar{a}_{t-1})$ and $Y_t^*(\\bar{a}_t)$ be the counterfactual state and counterfactual outcome, respectively,  that would occur at time $t$ had the agent followed the treatment history $\\bar{a}_{t}$. \n",
    "The set of potential outcomes up to time $t$ is given by\n",
    "\\begin{eqnarray*}\n",
    "\tW_t^*(\\bar{a}_t)=\\{S_0,Y_0^*(a_0),S_1^*(a_0),\\cdots,S_{t}^*(\\bar{a}_{t-1}),Y_t^*(\\bar{a}_t)\\}.\n",
    "\\end{eqnarray*}\n",
    "Let $W^*=\\cup_{t\\ge 0,\\bar{a}_t\\in \\{0,1\\}^{t+1}} W_t^*(\\bar{a}_t)$ be the set of all potential outcomes.\n",
    "\n",
    "The goodness of  a policy $\\pi$ is measured by its value functions, \n",
    "\\begin{eqnarray*}\n",
    "    V^{\\pi}(s)=\\sum_{t\\ge 0} \\gamma^t \\mathbb{E} \\{Y_t^*(\\pi)|S_0=s\\}, \\;\\; \tQ^{\\pi}(a,s)=\\sum_{t\\ge 0} \\gamma^t \\mathbb{E} \\{Y_t^*(\\pi)|S_0=s, A_0 = a\\}. \n",
    "\\end{eqnarray*}\n",
    "\n",
    "We need two critical assumptions for the MDP model. \n",
    "\n",
    "**(MA) Markov assumption**:  there exists a Markov transition kernel $\\mathcal{P}$ such that  for any $t\\ge 0$, $\\bar{a}_{t}\\in \\{0,1\\}^{t+1}$ and $\\mathcal{S}\\subseteq \\mathbb{R}^d$, we have \n",
    "$\\mathbb{P}\\{S_{t+1}^*(\\bar{a}_{t})\\in \\mathcal{S}|W_t^*(\\bar{a}_t)\\}=\\mathcal{P}(\\mathcal{S};a_t,S_t^*(\\bar{a}_{t-1})).$\n",
    "\n",
    "**(CMIA) Conditional mean independence assumption**: there exists a function $r$ such that  for any $t\\ge 0, \\bar{a}_{t}\\in \\{0,1\\}^{t+1}$, we have \n",
    "$\\mathbb{E} \\{Y_t^*(\\bar{a}_t)|S_t^*(\\bar{a}_{t-1}),W_{t-1}^*(\\bar{a}_{t-1})\\}=r(a_t,S_t^*(\\bar{a}_{t-1}))$.\n",
    "\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "1. MC\n",
    "\n",
    "2. TD\n",
    "\n",
    "3. SAVE\n",
    "\n",
    "## Policy Optimization\n",
    "\n",
    "1. DQN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4871ef2-de01-4755-bd15-b8353e8f0dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}