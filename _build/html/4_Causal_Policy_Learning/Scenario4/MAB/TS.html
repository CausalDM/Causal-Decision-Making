
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>TS &#8212; Causal Decision Making</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Contextual Bandits" href="../Contextual_Bandits/Contextual_Bandits.html" />
    <link rel="prev" title="UCB" href="UCB.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Causal Decision Making</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../Overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../Motivation_Examples.html">
   <em>
    Motivating Examples
   </em>
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../1_Preliminary/Preliminary.html">
   Preliminary
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../1_Preliminary/Causal%20Inference%20Preliminary.html">
     Average Treatment Effect
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../1_Preliminary/Heterogenous%20Treatement%20Effect.html">
     Heterogenous Treatment Effect
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Structure Learning (CSL)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Causal%20Discovery%20Preliminary.html">
   Causal Discovery Preliminary
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Testing-based%20Learner%20-%20PC%20Algorithm.html">
   Testing-based Learner - PC Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Functional-based%20Learner%20-%20LiNGAM%20Algorithm.html">
   Functional-based Learner - LiNGAM Algorithm
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Score-based%20Learner%20-%20NOTEARS%20Algorithm.html">
   Score-based Learner - NOTEARS Algorithm
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Effect Learning (CEL)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html">
   Single Stage
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/ATE.html">
     ATE Estimation
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/HTE.html">
     HTE Estimation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/S-learner.html">
       <strong>
        1. S-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/T-learner.html">
       <strong>
        2. T-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/X-learner.html">
       <strong>
        3. X-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/R-Learner.html">
       <strong>
        4. R learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/DR-Learner.html">
       <strong>
        5. DR-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/Lp-R-Learner.html">
       <strong>
        6. Lp-R-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/GRF.html">
       <strong>
        7. Generalized Random Forest
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/Dragonnet.html">
       <strong>
        8. Dragon Net
       </strong>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%202/underMDP.html">
   Markov Decision Processes
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%202/ATE.html">
     ATE
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%202/HTE.html">
     HTE
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/Multiple%20Stage.html">
   Multiple Stage–Finite Horizon
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%204/Miscellaneous.html">
   Miscellaneous
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Scenario 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario1/Single%20Stage.html">
   Single Stage (DTR)
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario1/Discrete.html">
   Discrete Action Space
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
  <label for="toctree-checkbox-5">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Q-learning_Single.html">
     Q-Learning (Single Stage)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/A-learning_Single.html">
     A-Learning (Single Stage)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../Scenario1/Classification.html">
     Reduction to Classification Problems
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../Scenario1/Classification/O-Learning.html">
       Outcome Weighted Learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../Scenario1/Classification/E-learning.html">
       Entropy learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Quantile/QuantileOTR_test.html">
     <strong>
      Quantile Optimal Treatment Regime
     </strong>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario1/Continuous.html">
   Continuous Action Space
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Continuous/Deep%20Jump%20Learner.html">
     Deep Jump Learner for Continuous Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Continuous/Kernel-Based%20Learner.html">
     Kernel-Based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Continuous/Outcome%20Learning.html">
     Outcome Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario1/PlanToDo.html">
   Plan To Do
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Scenario 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario2/preliminary_MDP-potential-outcome.html">
   Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario2/Evaluation.html">
   Policy Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/FQE.html">
     Fitted-Q Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/IPW_Infinite.html">
     Importance Sampling for Policy Evaluation (Infinite Horizon)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/DR_Infinite.html">
     Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/Deeply_Debiased.html">
     Deeply-Debiased Off-Policy Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario2/Optimization.html">
   Policy Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/FQI.html">
     Fitted-Q Iteration
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Scenario 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/Multi%20Stage.html">
   Multiple Stages (DTR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/Q-learning_Multiple.html">
   Q-Learning (Multiple Stages)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/A-learning_Multiple.html">
   A-Learning (Multiple Stages)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Scenario 4
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Bandits.html">
   Overview: Bandits ALgorithm
  </a>
 </li>
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="MAB.html">
   Multi-Armed Bandits (MAB)
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2">
    <a class="reference internal" href="Epsilon_Greedy.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Greedy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="UCB.html">
     UCB
    </a>
   </li>
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     TS
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Contextual_Bandits/Contextual_Bandits.html">
   Contextual Bandits
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Contextual_Bandits/LinUCB.html">
     LinUCB
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Contextual_Bandits/LinTS.html">
     LinTS
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Meta_Bandits/Meta_Bandits.html">
   Meta Bandits
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Meta_Bandits/Meta_TS.html">
     Meta Thompson Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Meta_Bandits/MTTS.html">
     Multi-Task Thompson Sampling (MTTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Structured_Bandits/Structured_Bandit.html">
   Structured Bandit (Slate Recommendation)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Structured_Bandits/Cascade/Learning%20to%20rank.html">
     Online Learning to Rank (Cascading Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
    <label for="toctree-checkbox-15">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Cascade/TS_Cascade.html">
       TS_Cascade
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Cascade/CascadeLinTS.html">
       CascadeLinTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Cascade/MTSS_Cascade.html">
       MTSS_Cascade
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/Combinatorial%20Optimization.html">
     Online Combinatorial Optimization (Combinatorial Semi-Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
    <label for="toctree-checkbox-16">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/CombTS.html">
       CombTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/CombLinTS.html">
       CombLinTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/MTSS_Comb.html">
       MTSS_Comb
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Structured_Bandits/MNL/Assortment%20Optimization.html">
     Dynamic Assortment Optimization (Multinomial Logit Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
    <label for="toctree-checkbox-17">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/MNL/TS_MNL_Beta.html">
       TS_MNL
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/MNL/TS_Contextual_MNL.html">
       TS_Contextual_MNL
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/MNL/MTSS_MNL.html">
       MTSS_MNL
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../OnlineEval/Online%20Policy%20Evaluation.html">
   Online Policy Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../OnlineEval/Direct%20Online%20Policy%20Evaluator.html">
     Direct Online Policy Evaluator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../OnlineEval/Inverse%20Probability%20Weighted%20Online%20Policy%20Evaluator.html">
     Inverse Probability Weighted Online Policy Evaluator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html">
     Doubly Robust Online Policy Evaluator
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Scenario 5
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario5/OnlineRL.html">
   Online RL
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Case Studies
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../5_Case_Study/Case_Study_1.html">
   MIMIC III
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../5_Case_Study/Case_Study_2.html">
   MovieLens
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/4_Causal_Policy_Learning/Scenario4/MAB/TS.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F4_Causal_Policy_Learning/Scenario4/MAB/TS.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/4_Causal_Policy_Learning/Scenario4/MAB/TS.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main-idea">
   Main Idea
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorithms-details">
   Algorithms Details
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-steps">
   Key Steps
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demo-code">
   Demo Code
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-the-learner">
     Import the learner.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-the-environment">
     Generate the Environment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specify-hyperparameters">
     Specify Hyperparameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recommendation-and-interaction">
     Recommendation and Interaction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#demo-code-for-bernoulli-bandit">
     Demo Code for Bernoulli Bandit
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>TS</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#overview">
   Overview
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#main-idea">
   Main Idea
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#algorithms-details">
   Algorithms Details
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-steps">
   Key Steps
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#demo-code">
   Demo Code
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#import-the-learner">
     Import the learner.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#generate-the-environment">
     Generate the Environment
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#specify-hyperparameters">
     Specify Hyperparameters
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#recommendation-and-interaction">
     Recommendation and Interaction
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#demo-code-for-bernoulli-bandit">
     Demo Code for Bernoulli Bandit
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="ts">
<h1>TS<a class="headerlink" href="#ts" title="Permalink to this headline">#</a></h1>
<section id="overview">
<h2>Overview<a class="headerlink" href="#overview" title="Permalink to this headline">#</a></h2>
<ul class="simple">
<li><p><strong>Advantage</strong>: Be able to incorporate prior knowledge about reward distribution, which is especially useful when the prior knowledge is informative. Taking uncertainty into account by updating the posterior distribution of the expectation of potential reward to achieve a good balance between exploration and exploitation.</p></li>
<li><p><strong>Disadvantage</strong>: Inefficient if there is a large number of action items.</p></li>
<li><p><strong>Application Situation</strong>: discrete action space, binary/Gaussian reward space</p></li>
</ul>
</section>
<section id="main-idea">
<h2>Main Idea<a class="headerlink" href="#main-idea" title="Permalink to this headline">#</a></h2>
<p>Thompson Sampling (TS), also known as posterior sampling, solves the exploration-exploitation dilemma by selecting an action according to its posterior distribution [1].  At each round <span class="math notranslate nohighlight">\(t\)</span>, the agent sample the rewards from the corresponding posterior distributions of the expectation of the potential reward (i.e., <span class="math notranslate nohighlight">\(E[R_t(a)]\)</span>) and then select the action with the highest sampled reward greedily. It has been shown that, when the true reward distribution is known, a TS algorithm with the true reward distribution as the prior is nearly optimal [2]. However, such a distribution is always unknown in practice. Therefore, one of the major objectives of TS-based algorithms is to find an informative prior to guide the exploration. Note that the algorithm here supports bandit problem with either binary reward or continuous reward.</p>
</section>
<section id="algorithms-details">
<h2>Algorithms Details<a class="headerlink" href="#algorithms-details" title="Permalink to this headline">#</a></h2>
<p>Supposed there are <span class="math notranslate nohighlight">\(K\)</span> options, and the action space is <span class="math notranslate nohighlight">\(\mathcal{A} = \{0,1,\cdots, K-1\}\)</span>. The TS algorithm starts with specifying a prior distribution of the potential reward, based on the domian knowledge. At each round <span class="math notranslate nohighlight">\(t\)</span>, the agent will samples a vector of <span class="math notranslate nohighlight">\(\theta^{t}\)</span> from the posterior distribution of the potential rewards. The action <span class="math notranslate nohighlight">\(a\)</span> with the greatest <span class="math notranslate nohighlight">\(\theta_{a}^{t}\)</span> is then selected. Finally, the posterior distribution would be updated after receiving the realized reward at the end of each round. Note that the posterior updating step differs for different pairs of prior distribution of the mean reward and reward distribution. Here, we consider two classical examples of the TS algorithm, including</p>
<ul class="simple">
<li><p>Gaussian Bandits
\begin{aligned}
\boldsymbol{\theta} &amp;\sim Q(\boldsymbol{\theta}),\
R_t(a) &amp;\sim \mathcal{N}(\theta_a,\sigma^2),
\end{aligned}</p></li>
<li><p>Bernoulli Bandits
\begin{aligned}
\boldsymbol{\theta} &amp;\sim Q(\boldsymbol{\theta}),\
R_t(a) &amp;\sim Bernoulli(\theta_a).
\end{aligned}</p></li>
</ul>
<p>Assuming a Gaussian prior for the Gaussian bandits and a Beta prior for the Bernoulli bandits, the posterior updating is straightforward with closed-form expression. In the Gaussian bandits, the variance of reward <span class="math notranslate nohighlight">\(\sigma^2\)</span> is assumed to be known, and need to be specified manually. Note that code can be easily modified to different specifications of the prior/potential reward distribution.</p>
</section>
<section id="key-steps">
<h2>Key Steps<a class="headerlink" href="#key-steps" title="Permalink to this headline">#</a></h2>
<ol class="simple">
<li><p>Specifying a prior distirbution of <span class="math notranslate nohighlight">\(E[R_0(a)]\)</span>, <span class="math notranslate nohighlight">\(a \in \mathcal{A}\)</span>, and the variance of the reward distribution.</p></li>
<li><p>For t = <span class="math notranslate nohighlight">\(0, 1,\cdots, T\)</span>:</p>
<ul class="simple">
<li><p>sample a <span class="math notranslate nohighlight">\(\theta^{t}\)</span> from the posterior distribution of <span class="math notranslate nohighlight">\(E[R_t(a)]\)</span> or prior distribution if in round <span class="math notranslate nohighlight">\(0\)</span></p></li>
<li><p>select action <span class="math notranslate nohighlight">\(A_t\)</span> which has the greatest <span class="math notranslate nohighlight">\(\theta^{t}_{a}\)</span>, i.e. <span class="math notranslate nohighlight">\(A_t = \arg\max_{a \in \mathcal{A}} \theta_{a}^{t}\)</span></p></li>
<li><p>receive the rewad <span class="math notranslate nohighlight">\(R_t\)</span>, and update the posterior distirbution accordingly.</p></li>
</ul>
</li>
</ol>
</section>
<section id="demo-code">
<h2>Demo Code<a class="headerlink" href="#demo-code" title="Permalink to this headline">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">os</span>
<span class="n">os</span><span class="o">.</span><span class="n">getcwd</span><span class="p">()</span>
<span class="n">os</span><span class="o">.</span><span class="n">chdir</span><span class="p">(</span><span class="s1">&#39;D:\GitHub\CausalDM&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="import-the-learner">
<h3>Import the learner.<a class="headerlink" href="#import-the-learner" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">causaldm.learners.Online.MAB</span> <span class="kn">import</span> <span class="n">TS</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>WARNING (theano.tensor.blas): Using NumPy C-API based implementation for BLAS functions.
</pre></div>
</div>
</div>
</div>
</section>
<section id="generate-the-environment">
<h3>Generate the Environment<a class="headerlink" href="#generate-the-environment" title="Permalink to this headline">#</a></h3>
<p>Here, we imitate an environment based on the MovieLens data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">causaldm.learners.Online.MAB</span> <span class="kn">import</span> <span class="n">_env_realMAB</span> <span class="k">as</span> <span class="n">_env</span>
<span class="n">env</span> <span class="o">=</span> <span class="n">_env</span><span class="o">.</span><span class="n">Single_Gaussian_Env</span><span class="p">(</span><span class="n">seed</span> <span class="o">=</span> <span class="mi">42</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="specify-hyperparameters">
<h3>Specify Hyperparameters<a class="headerlink" href="#specify-hyperparameters" title="Permalink to this headline">#</a></h3>
<ul class="simple">
<li><p>Reward_Type: the type of the MAB, i.e., “Gaussian”/”Bernoulli”</p></li>
<li><p>sigma: the standard deviation of the reward distributions</p></li>
<li><p>u_prior_mean: mean of the prior distribution of the mean reward</p></li>
<li><p>u_prior_cov: Covaraince matrix of the prior distribution of the mean reward</p></li>
<li><p>seed: random seed</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">Reward_Type</span> <span class="o">=</span> <span class="s2">&quot;Gaussian&quot;</span>
<span class="n">sigma</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">K</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">K</span>
<span class="n">u_prior_mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="n">u_prior_cov</span> <span class="o">=</span> <span class="mi">10000</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="n">seed</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">TS_Gaussian_agent</span> <span class="o">=</span> <span class="n">TS</span><span class="o">.</span><span class="n">TS</span><span class="p">(</span><span class="n">Reward_Type</span> <span class="o">=</span> <span class="n">Reward_Type</span><span class="p">,</span> <span class="n">sigma</span> <span class="o">=</span> <span class="n">sigma</span><span class="p">,</span> 
                          <span class="n">u_prior_mean</span> <span class="o">=</span> <span class="n">u_prior_mean</span><span class="p">,</span> <span class="n">u_prior_cov</span> <span class="o">=</span> <span class="n">u_prior_cov</span><span class="p">,</span> 
                          <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="recommendation-and-interaction">
<h3>Recommendation and Interaction<a class="headerlink" href="#recommendation-and-interaction" title="Permalink to this headline">#</a></h3>
<p>Starting from t = 0, for each step t, there are three steps:</p>
<ol class="simple">
<li><p>Recommend an action
<code> A = TS_Gaussian_agent.take_action() </code></p></li>
<li><p>Get the reward from the environment
<code> R = env.get_reward(t,A) </code></p></li>
<li><p>Update the posterior distribution
<code> TS_Gaussian_agent.receive_reward(t,A,R) </code></p></li>
</ol>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">TS_Gaussian_agent</span><span class="o">.</span><span class="n">take_action</span><span class="p">()</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_reward</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">TS_Gaussian_agent</span><span class="o">.</span><span class="n">receive_reward</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">R</span><span class="p">)</span>
<span class="n">t</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">R</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0, 3, 2)
</pre></div>
</div>
</div>
</div>
<p><strong>Interpretation</strong>: For step 0, the TS agent recommend a Thriller (arm 3), and received a rate of 2 from the environment.</p>
</section>
<section id="demo-code-for-bernoulli-bandit">
<h3>Demo Code for Bernoulli Bandit<a class="headerlink" href="#demo-code-for-bernoulli-bandit" title="Permalink to this headline">#</a></h3>
<p>The steps are similar to those previously performed with a Gaussian Bandit. Note that, when specifying the prior distribution of the expected reward, the mean-precision form of the Beta distribution is used here, i.e., Beta(<span class="math notranslate nohighlight">\(\mu\)</span>, <span class="math notranslate nohighlight">\(\phi\)</span>), where <span class="math notranslate nohighlight">\(\mu\)</span> is the mean reward of each arm and <span class="math notranslate nohighlight">\(\phi\)</span> is the precision of the Beta distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">env</span> <span class="o">=</span> <span class="n">_env</span><span class="o">.</span><span class="n">Single_Bernoulli_Env</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">42</span><span class="p">)</span>

<span class="n">K</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">K</span>
<span class="n">Reward_Type</span> <span class="o">=</span> <span class="s2">&quot;Bernoulli&quot;</span>
<span class="c1">## specify the mean of the prior beta distribution</span>
<span class="n">u_prior_mean</span> <span class="o">=</span> <span class="mf">.5</span><span class="o">*</span><span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">K</span><span class="p">)</span>
<span class="c1">## specify the precision of the prior beta distribution</span>
<span class="n">prior_phi_beta</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">TS_Bernoulli_agent</span> <span class="o">=</span> <span class="n">TS</span><span class="o">.</span><span class="n">TS</span><span class="p">(</span><span class="n">Reward_Type</span> <span class="o">=</span> <span class="n">Reward_Type</span><span class="p">,</span>
                           <span class="n">u_prior_mean</span> <span class="o">=</span> <span class="n">u_prior_mean</span><span class="p">,</span>
                           <span class="n">prior_phi_beta</span> <span class="o">=</span> <span class="n">prior_phi_beta</span><span class="p">,</span>
                           <span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span><span class="p">)</span>
<span class="n">t</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">A</span> <span class="o">=</span> <span class="n">TS_Bernoulli_agent</span><span class="o">.</span><span class="n">take_action</span><span class="p">()</span>
<span class="n">R</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">get_reward</span><span class="p">(</span><span class="n">A</span><span class="p">)</span>
<span class="n">TS_Bernoulli_agent</span><span class="o">.</span><span class="n">receive_reward</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="n">A</span><span class="p">,</span><span class="n">R</span><span class="p">)</span>
<span class="n">t</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">R</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(0, 4, 0)
</pre></div>
</div>
</div>
</div>
<p><strong>Interpretation</strong>: For step 0, the TS agent recommend a Sci-Fi (arm 4), and received a reward of 0 from the environment.</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<p>[1] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. (2017). A tutorial on thompson sampling. arXiv preprint arXiv:1707.0203</p>
<p>[2] Lattimore, T. and Szepesv´ari, C. (2020). Bandit algorithms. Cambridge University Press.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4_Causal_Policy_Learning\Scenario4\MAB"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="UCB.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">UCB</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Contextual_Bandits/Contextual_Bandits.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Contextual Bandits</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Causal Decision Making Team<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>