
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Single-Item Recommendation &#8212; Causal Decision Making</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Causal Decision Making</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../../Overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Motivating Examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../0_Motivating_Examples/CSL.html">
   <em>
    Causal Structure Learning (CSL)
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../0_Motivating_Examples/CEL.html">
   <em>
    Causal Effect Learning (CEL)
   </em>
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../0_Motivating_Examples/CPL.html">
   <em>
    Causal Policy Learning (CPL)
   </em>
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Preliminary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../1_Preliminary/Causal%20Inference%20Preliminary.html">
   Causal Inference Preliminary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Structure Learning (CSL)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Preliminaries%20of%20Causal%20Graphs.html">
   Preliminaries of Causal Graphs
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Causal%20Discovery.html">
   Causal Discovery
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../2_Causal_Structure_Learning/Testing-based%20Learner.html">
     Testing-based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../2_Causal_Structure_Learning/Functional-based%20Learner.html">
     Functional-based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../2_Causal_Structure_Learning/Score-based%20Learner.html">
     Score-based Learner
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../2_Causal_Structure_Learning/Causal%20Mediation%20Analysis.html">
   Causal Mediation Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Effect Learning (CEL)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html">
   <strong>
    Single Stage
   </strong>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/ATE.html">
     ATE Estimation
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/HTE.html">
     HTE Estimation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/S-learner.html">
       <strong>
        1. S-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/T-learner.html">
       <strong>
        2. T-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/X-learner.html">
       <strong>
        3. X-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/R-Learner.html">
       <strong>
        4. R learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/DR-Learner.html">
       <strong>
        5. DR-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/Lp-R-Learner.html">
       <strong>
        6. Lp-R-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/GRF.html">
       <strong>
        7. Generalized Random Forest
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%201/Dragonnet.html">
       <strong>
        8. Dragon Net
       </strong>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%202/underMDP.html">
   Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/Panel%20Data.html">
   Panel Data
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/ATE.html">
     ATE
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20Control.html">
       <strong>
        Synthetic Control
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20Learner.html">
       <strong>
        Synthetic Learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20DiD.html">
       <strong>
        Synthetic DiD
       </strong>
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/HTE.html">
     HTE
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
    <label for="toctree-checkbox-6">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/DiD.html">
       <strong>
        Difference in Difference
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/R-DiD.html">
       <strong>
        R-DiD
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20X-Learner.html">
       <strong>
        Synthetic X-Learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../../3_Causal_Effect_Learning/Scenario%203/H1SL_H2SL.html">
       <strong>
        H1SL and H2SL
       </strong>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario1/Single%20Stage.html">
   Single Stage
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario1/Discrete.html">
   Discrete Action Space
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Q-learning_Single.html">
     Q-Learning (Single Stage)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/A-learning_Single.html">
     A-Learning (Single Stage)
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../Scenario1/Classification.html">
     Reduction to Classification Problems
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
    <label for="toctree-checkbox-8">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../Scenario1/Classification/O-Learning.html">
       Outcome Weighted Learning
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../Scenario1/Classification/E-learning.html">
       Entropy learning
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Quantile/QuantileOTR_test.html">
     <strong>
      Quantile Optimal Treatment Regime
     </strong>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario1/Continuous.html">
   Continuous Action Space
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Continuous/Deep%20Jump%20Learner.html">
     Deep Jump Learner for Continuous Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Continuous/Kernel-Based%20Learner.html">
     Kernel-Based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario1/Continuous/Outcome%20Learning.html">
     Outcome Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario1/PlanToDo.html">
   Plan To Do
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="simple">
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario2/preliminary_MDP-potential-outcome.html">
   Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario2/Evaluation.html">
   Policy Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/FQE.html">
     Fitted-Q Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/IPW_Infinite.html">
     Importance Sampling for Policy Evaluation (Infinite Horizon)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/DR_Infinite.html">
     Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/Deeply_Debiased.html">
     Deeply-Debiased Off-Policy Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../Scenario2/Optimization.html">
   Policy Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../Scenario2/FQI.html">
     Fitted-Q Iteration
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/Multi%20Stage.html">
   Multiple Stages (DTR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/Q-learning_Multiple.html">
   Q-Learning (Multiple Stages)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/A-learning_Multiple.html">
   A-Learning (Multiple Stages)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario3/MediatedQ-learning_Multiple.html">
   MediatedQ-Learning (Multiple Stages)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 4
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Bandits.html">
   Overview: Bandits ALgorithm
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../MAB/MAB.html">
   Multi-Armed Bandits (MAB)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../MAB/Epsilon_Greedy.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Greedy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MAB/UCB.html">
     UCB
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../MAB/TS.html">
     TS
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Contextual_Bandits/Contextual_Bandits.html">
   Contextual Bandits
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Contextual_Bandits/LinUCB.html">
     LinUCB
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Contextual_Bandits/LinTS.html">
     LinTS
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Meta_Bandits/Meta_Bandits.html">
   Meta Bandits
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
  <label for="toctree-checkbox-15">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Meta_Bandits/Meta_TS.html">
     Meta Thompson Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Meta_Bandits/MTTS.html">
     Multi-Task Thompson Sampling (MTTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Structured_Bandits/Structured_Bandit.html">
   Structured Bandit (Slate Recommendation)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
  <label for="toctree-checkbox-16">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Structured_Bandits/Cascade/Learning%20to%20rank.html">
     Online Learning to Rank (Cascading Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
    <label for="toctree-checkbox-17">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Cascade/TS_Cascade.html">
       TS_Cascade
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Cascade/CascadeLinTS.html">
       CascadeLinTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Cascade/MTSS_Cascade.html">
       MTSS_Cascade
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/Combinatorial%20Optimization.html">
     Online Combinatorial Optimization (Combinatorial Semi-Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
    <label for="toctree-checkbox-18">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/CombTS.html">
       CombTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/CombLinTS.html">
       CombLinTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/Combinatorial-Semi/MTSS_Comb.html">
       MTSS_Comb
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Structured_Bandits/MNL/Assortment%20Optimization.html">
     Dynamic Assortment Optimization (Multinomial Logit Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
    <label for="toctree-checkbox-19">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/MNL/TS_MNL_Beta.html">
       TS_MNL
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/MNL/TS_Contextual_MNL.html">
       TS_Contextual_MNL
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Structured_Bandits/MNL/MTSS_MNL.html">
       MTSS_MNL
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../OnlineEval/Online%20Policy%20Evaluation.html">
   Online Policy Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-20" name="toctree-checkbox-20" type="checkbox"/>
  <label for="toctree-checkbox-20">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../OnlineEval/Direct%20Online%20Policy%20Evaluator.html">
     Direct Online Policy Evaluator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../OnlineEval/Inverse%20Probability%20Weighted%20Online%20Policy%20Evaluator.html">
     Inverse Probability Weighted Online Policy Evaluator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html">
     Doubly Robust Online Policy Evaluator
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 5
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario5/OnlineRL_Markov.html">
   Ooline Policy Learning and Evaluation in Markovian Environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 6
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../Scenario6/OnlineRL_non_Markov.html">
   Ooline Policy Learning in Non-Markovian Environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Case Studies
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../5_Case_Study/MIMIC3/MIMIC3_intro.html">
   Mimic3 Demo
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../../5_Case_Study/MovieLens/Case_Study_2.html">
   MovieLens
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/4_Causal_Policy_Learning/Scenario4/_old_docs(to delete)/Single-Item Recommendation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F4_Causal_Policy_Learning/Scenario4/_old_docs(to delete)/Single-Item Recommendation.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/4_Causal_Policy_Learning/Scenario4/_old_docs(to delete)/Single-Item Recommendation.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-setting">
   Problem Setting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#claasical-methods">
   Claasical Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epsilon-greedy">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Greedy
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supported-algorithms">
       Supported Algorithms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thompson-sampling">
     Thompson Sampling
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Supported Algorithms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#upper-confidence-bounds">
     Upper Confidence Bounds
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Supported Algorithms
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-data">
   Real Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulation-1-gaussian-bandit">
   Simulation 1: Gaussian Bandit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulation-1-bernoulli-bandit">
   Simulation 1: Bernoulli Bandit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Single-Item Recommendation</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#problem-setting">
   Problem Setting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#claasical-methods">
   Claasical Methods
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#epsilon-greedy">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Greedy
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#supported-algorithms">
       Supported Algorithms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#thompson-sampling">
     Thompson Sampling
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id1">
       Supported Algorithms
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#upper-confidence-bounds">
     Upper Confidence Bounds
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#id2">
       Supported Algorithms
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#real-data">
   Real Data
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulation-1-gaussian-bandit">
   Simulation 1: Gaussian Bandit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#simulation-1-bernoulli-bandit">
   Simulation 1: Bernoulli Bandit
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#reference">
   Reference
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="single-item-recommendation">
<h1>Single-Item Recommendation<a class="headerlink" href="#single-item-recommendation" title="Permalink to this headline">#</a></h1>
<p>The bandit problems have received increasing attention recently and have been widely applied to areas such as clinical trials [1], finance [2], and recommendation systems [3], among others. The most classical version of it is the multi-armed bandit (MAB) [4], where an agent will sequentially select an item (arm) from a few and then receive a random reward for the item selected. Since the reward distributions are unknown in most real applications, the central task of a MAB algorithm is to learn the distributions from feedback received and find the optimal item to maximize the cumulative rewards or, equivalently, to minimize the cumulative regret. This chapter focuses on the MAB problems by illustrating a group of classical algorithms to tackle the well-known exploration-exploitation trade-off.</p>
<section id="problem-setting">
<h2>Problem Setting<a class="headerlink" href="#problem-setting" title="Permalink to this headline">#</a></h2>
<p>Let <span class="math notranslate nohighlight">\(T\)</span> be the total number of rounds, and <span class="math notranslate nohighlight">\(K\)</span> be the number of arms (actions to be selected). The agent would choose one arm at each round <span class="math notranslate nohighlight">\(t = 1, \dots, T\)</span>. Then the agent will receive the corresponding stochastic reward <span class="math notranslate nohighlight">\(R_t\)</span> from the environment. Denote the expected reward for each arm <span class="math notranslate nohighlight">\(i\)</span> as <span class="math notranslate nohighlight">\(r_{i}\)</span>. Since, in most real applications, such a reward distribution is always unknown, the agent needs to learn the reward distribution from feedback received. Overall, the objective is to find a bandit algorithm to maximize the cumulative Reward <span class="math notranslate nohighlight">\(\sum_{t=1}^{T}R_{t}\)</span>.</p>
<p>MAB has been extensively studied and widely applied to different areas, including healthcare, recommender system, and finance, to name a few. See [4] for a detailed review of MAB and [5] for a survey of practical applications. Among them, the ultimate goal of a learning algorithm is always to strike a good balance between exploration (try an unfamiliar action to learn more information) and exploitation (take the action that has the highest estimated reward so far) so as to maximize the cumulative reward. In the following, we will briefly illustrate three popular and classical categories of algorithms to handle the exploration-exploitation trade-off: i) <span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy, ii) Upper Confidence Bound (UCB), and iii) Thompson Sampling (TS).</p>
</section>
<section id="claasical-methods">
<h2>Claasical Methods<a class="headerlink" href="#claasical-methods" title="Permalink to this headline">#</a></h2>
<section id="epsilon-greedy">
<h3><span class="math notranslate nohighlight">\(\epsilon\)</span>-Greedy<a class="headerlink" href="#epsilon-greedy" title="Permalink to this headline">#</a></h3>
<p>An intuitive algorithm to incorporate the exploration and exploitation is <span class="math notranslate nohighlight">\(\epsilon\)</span>-Greedy, which is simple and widely used [6]. Specifically, at each round <span class="math notranslate nohighlight">\(t\)</span>, we will select a random action with probability <span class="math notranslate nohighlight">\(\epsilon\)</span>, and select an action with the highest estimated mean reward based on the history so far with probability <span class="math notranslate nohighlight">\(1-\epsilon\)</span>. Here the parameter <span class="math notranslate nohighlight">\(\epsilon\)</span> is pre-specified. A more adaptive variant is <span class="math notranslate nohighlight">\(\epsilon_{t}\)</span>-greedy, where the probability of taking a random action is defined as a decreasing function of <span class="math notranslate nohighlight">\(t\)</span>. Auer et al. [7] showed that <span class="math notranslate nohighlight">\(\epsilon_{t}\)</span>-greedy performs well in practice with <span class="math notranslate nohighlight">\(\epsilon_{t}\)</span> decreases to 0 at a rate of <span class="math notranslate nohighlight">\(\frac{1}{t}\)</span>.</p>
<section id="supported-algorithms">
<h4>Supported Algorithms<a class="headerlink" href="#supported-algorithms" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>algorithm</p></th>
<th class="text-align:center head"><p>Reward</p></th>
<th class="text-align:center head"><p>with features?</p></th>
<th class="text-align:center head"><p>Advantage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><span class="xref myst"><span class="math notranslate nohighlight">\(\epsilon\)</span>-greedy</span></p></td>
<td class="text-align:center"><p>Binary/Gaussian</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p>Simple</p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="thompson-sampling">
<h3>Thompson Sampling<a class="headerlink" href="#thompson-sampling" title="Permalink to this headline">#</a></h3>
<p>Thompson Sampling, also known as posterior sampling, solves the exploration-exploitation dilemma by selecting an action according to its posterior distribution [8].  At each round <span class="math notranslate nohighlight">\(t\)</span>, the agent sample the rewards from the corresponding posterior distributions and then select the action with the highest sampled reward greedily. It has been shown that, when the true reward distribution is known, a TS algorithm with the true reward distribution as the prior is nearly optimal [9]. However, such a distribution is always unknown in practice. Therefore, one of the major objectives of TS-based algorithms is to find an informative prior to guide the exploration.</p>
<section id="id1">
<h4>Supported Algorithms<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>algorithm</p></th>
<th class="text-align:center head"><p>Reward</p></th>
<th class="text-align:center head"><p>with features?</p></th>
<th class="text-align:center head"><p>Advantage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><a class="reference external" href="https://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/sampling/TS_Tutorial.pdf">TS [8]</a></p></td>
<td class="text-align:center"><p>Binary/Guaasian</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference external" href="http://proceedings.mlr.press/v28/agrawal13.pdf">LinTS [13]</a></p></td>
<td class="text-align:center"><p>Gaussian</p></td>
<td class="text-align:center"><p>✅</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><a class="reference external" href="http://proceedings.mlr.press/v108/kveton20a/kveton20a.pdf">GLMTS [12]</a></p></td>
<td class="text-align:center"><p>GLM</p></td>
<td class="text-align:center"><p>✅</p></td>
<td class="text-align:center"><p></p></td>
</tr>
</tbody>
</table>
</section>
</section>
<section id="upper-confidence-bounds">
<h3>Upper Confidence Bounds<a class="headerlink" href="#upper-confidence-bounds" title="Permalink to this headline">#</a></h3>
<p>As the name suggested, the UCB algorithm estimates the upper confidence bound <span class="math notranslate nohighlight">\(U_{i}^{t}\)</span> of the mean rewards based on the observations and then choose the action has the highest estimates. The class of UCB-based algorithms is firstly introduced by Auer et al. [7]. Generally, at each round <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(U_{i}^{t}\)</span> is calculated as the sum of the estimated reward (exploitation) and the estimated confidence radius (exploration) of item <span class="math notranslate nohighlight">\(i\)</span> based on <span class="math notranslate nohighlight">\(\mathcal{H}_{t}\)</span>. Then, <span class="math notranslate nohighlight">\(A_{t}\)</span> is selected as</p>
<div class="amsmath math notranslate nohighlight" id="equation-0cebed74-c3b6-4006-b35b-cdb2bcdf2bc7">
<span class="eqno">()<a class="headerlink" href="#equation-0cebed74-c3b6-4006-b35b-cdb2bcdf2bc7" title="Permalink to this equation">#</a></span>\[\begin{equation}
    A_t = argmax_{a \in \mathcal{A}} E(R_t \mid a,\{ U_{i}^{t}\}_{i=1}^{N}).
\end{equation}\]</div>
<section id="id2">
<h4>Supported Algorithms<a class="headerlink" href="#id2" title="Permalink to this headline">#</a></h4>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="text-align:left head"><p>algorithm</p></th>
<th class="text-align:center head"><p>Reward</p></th>
<th class="text-align:center head"><p>with features?</p></th>
<th class="text-align:center head"><p>Advantage</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td class="text-align:left"><p><a class="reference external" href="https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf">UCB1 [7]</a></p></td>
<td class="text-align:center"><p>Binary/Gaussian</p></td>
<td class="text-align:center"><p></p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-odd"><td class="text-align:left"><p><a class="reference external" href="https://dl.acm.org/doi/pdf/10.1145/1772690.1772758?casa_token=CJjeIziLmjEAAAAA:CkRvgHQNqpy10rzcUP5kx31NWJmgSldd6zx8wZxskZYCoCc8v7EDIw3t3Gk1_6mfurqQTqRZ7fVA">LinUCB [10]</a></p></td>
<td class="text-align:center"><p>Guassian</p></td>
<td class="text-align:center"><p>✅</p></td>
<td class="text-align:center"><p></p></td>
</tr>
<tr class="row-even"><td class="text-align:left"><p><a class="reference external" href="http://proceedings.mlr.press/v70/li17c/li17c.pdf">UCB-GLM [11]</a></p></td>
<td class="text-align:center"><p>GLM</p></td>
<td class="text-align:center"><p>✅</p></td>
<td class="text-align:center"><p></p></td>
</tr>
</tbody>
</table>
</section>
</section>
</section>
<section id="real-data">
<h2>Real Data<a class="headerlink" href="#real-data" title="Permalink to this headline">#</a></h2>
<p><strong>1. MovieLens</strong></p>
<p>Movie Lens is a website that helps users find the movies they like and where they will rate the recommended movies. <a class="reference external" href="https://grouplens.org/datasets/movielens/1m/">MovieLens 1M dataset</a> is a dataset including the observations collected in an online movie recommendation experiment and is widely used to generate data for online bandit simulation studies. The goal of the simulation studies below is to learn the reward distribution of different movie genres and hence to recommend the optimal movie genres to the users to optimize the cumulative user satisfaction. In other words, every time a user visits the website, the agent will recommend a movie genre (<span class="math notranslate nohighlight">\(A_t\)</span>) to the user, and then the user will give a rating (<span class="math notranslate nohighlight">\(R_t\)</span>) to the genre recommended. We assume that users’ satisfaction is fully reflected through the ratings. Therefore, the ultimate goal of the bandit algorithms is to optimize the cumulative ratings received by finding and recommending the optimal movie genre that will receive the highest rating. In this chapter, we mainly focus on the top 5 Genres, including</p>
<ul class="simple">
<li><p><strong>Comedy</strong>: <span class="math notranslate nohighlight">\(a=0\)</span>,</p></li>
<li><p><strong>Drama</strong>: <span class="math notranslate nohighlight">\(a=1\)</span>,</p></li>
<li><p><strong>Action</strong>: <span class="math notranslate nohighlight">\(a=2\)</span>,</p></li>
<li><p><strong>Thriller</strong>: <span class="math notranslate nohighlight">\(a=3\)</span>,</p></li>
<li><p><strong>Sci-Fi</strong>: <span class="math notranslate nohighlight">\(a=4\)</span>.</p></li>
</ul>
<p>Therefore, <span class="math notranslate nohighlight">\(K=5\)</span>. For each user, feature information, including age, gender and occupation, are available:</p>
<ul class="simple">
<li><p><strong>age</strong>: numerical, from 18 to 56,</p></li>
<li><p><strong>gender</strong>: binary, =1 if male,</p></li>
<li><p><strong>college/grad student</strong>: binary, =1 if a college/grad student,</p></li>
<li><p><strong>executive/managerial</strong>: binary, =1 if a executive/managerial,</p></li>
<li><p><strong>academic/educator</strong>: binary, =1 if an academic/educator,</p></li>
<li><p><strong>technician/engineer</strong>: binary, =1 if a technician/engineer,</p></li>
<li><p><strong>writer</strong>: if a writer, then all the previous occupation-related variables = 0 (baseline).</p></li>
</ul>
<p>Furthermore, there are two different types of the reward <span class="math notranslate nohighlight">\(R_t\)</span>:</p>
<ul class="simple">
<li><p><strong>Gaussian Bandit</strong>: <span class="math notranslate nohighlight">\(R_t\)</span> is a numerical variable, taking the value of <span class="math notranslate nohighlight">\(\{1,2,3,4,5\}\)</span>, where 1 is the least satisfied and 5 is the most satisfied.</p></li>
<li><p><strong>Bernoulli Bandit</strong>: <span class="math notranslate nohighlight">\(R_t\)</span> is a binary variable, =1 if the rating is higher than 3.</p></li>
</ul>
<p>In the following, we evaluated the empirical performance of the supported algorithms on the MovieLens dataset under either the Gaussian bandit or Bernoulli bandit settings.</p>
</section>
<section id="simulation-1-gaussian-bandit">
<h2>Simulation 1: Gaussian Bandit<a class="headerlink" href="#simulation-1-gaussian-bandit" title="Permalink to this headline">#</a></h2>
<p>We repeat the experiment over <strong>100</strong> random seeds, with <span class="math notranslate nohighlight">\(T = 100\)</span>.</p>
</section>
<section id="simulation-1-bernoulli-bandit">
<h2>Simulation 1: Bernoulli Bandit<a class="headerlink" href="#simulation-1-bernoulli-bandit" title="Permalink to this headline">#</a></h2>
<p>We repeat the experiment over <strong>100</strong> random seeds, with <span class="math notranslate nohighlight">\(T = 100\)</span>.</p>
</section>
<section id="reference">
<h2>Reference<a class="headerlink" href="#reference" title="Permalink to this headline">#</a></h2>
<p>[1] Durand, A., Achilleos, C., Iacovides, D., Strati, K., Mitsis, G. D., and Pineau, J. (2018). Contextual bandits for adapting treatment in a mouse model of de novo carcinogenesis. In Machine learning for healthcare conference, pages 67–82. PMLR.</p>
<p>[2] Shen, W., Wang, J., Jiang, Y.-G., and Zha, H. (2015). Portfolio choices with orthogonal bandit learning. In Twenty-fourth international joint conference on artificial intelligence.</p>
<p>[3] Zhou, Q., Zhang, X., Xu, J., and Liang, B. (2017). Large-scale bandit approaches for recommender systems. In International Conference on Neural Information Processing, pages 811–821. Springer.</p>
<p>[4] Slivkins, A. (2019). Introduction to multi-armed bandits. arXiv preprint arXiv:1904.07272.</p>
<p>[5] Bouneffouf, D. and Rish, I. (2019). A survey on practical applications of multi-armed and contextual bandits. arXiv preprint arXiv:1904.10040.</p>
<p>[6] Sutton, R. S. and Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press</p>
<p>[7] Auer, P., Cesa-Bianchi, N., and Fischer, P. (2002). Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235–256.</p>
<p>[8] Russo, D., Van Roy, B., Kazerouni, A., Osband, I., and Wen, Z. (2017). A tutorial on thompson sampling. arXiv preprint arXiv:1707.0203</p>
<p>[9] Lattimore, T. and Szepesv´ari, C. (2020). Bandit algorithms. Cambridge University Press.</p>
<p>[10] Li, L., Chu, W., Langford, J., and Schapire, R. E. (2010). A contextual-bandit approach to personalized news article recommendation. In Proceedings of the 19th international conference on World wide web, pages 661–670</p>
<p>[11] Li, L., Lu, Y., and Zhou, D. (2017). Provably optimal algorithms for generalized linear contextual bandits. In International Conference on Machine Learning, pages 2071–2080. PMLR.</p>
<p>[12] Kveton, B., Zaheer, M., Szepesvari, C., Li, L., Ghavamzadeh, M., and Boutilier, C. (2020). Randomized exploration in generalized linear bandits. In International Conference on Artificial Intelligence and Statistics, pages 2066–2076. PMLR.</p>
<p>[13] Agrawal, S. and Goyal, N. (2013). Thompson sampling for contextual bandits with linear payoffs. In International conference on machine learning, pages 127–135. PMLR.</p>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4_Causal_Policy_Learning\Scenario4\_old_docs(to delete)"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Causal Decision Making Team<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>