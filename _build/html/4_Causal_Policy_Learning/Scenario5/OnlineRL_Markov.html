
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Ooline Policy Learning and Evaluation in Markovian Environments &#8212; Causal Decision Making</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=62ba249389abaaa9ffc34bf36a076bdc1d65ee18" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=f31d14ad54b65d19161ba51d4ffff3a77ae00456"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Ooline Policy Learning in Non-Markovian Environments" href="../Scenario6/OnlineRL_non_Markov.html" />
    <link rel="prev" title="Doubly Robust Online Policy Evaluator" href="../Scenario4/OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Causal Decision Making</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Overview.html">
                    Overview
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Motivating Examples
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../0_Motivating_Examples/CSL.html">
   Causal Structure Learning (CSL)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../0_Motivating_Examples/CEL.html">
   Causal Effect Learning (CEL)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../0_Motivating_Examples/CPL.html">
   Causal Policy Learning (CPL)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Preliminary
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../1_Preliminary/Causal%20Inference%20Preliminary.html">
   Causal Inference Preliminary
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Structure Learning (CSL)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../2_Causal_Structure_Learning/Preliminaries%20of%20Causal%20Graphs.html">
   Preliminaries of Causal Graphs
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../2_Causal_Structure_Learning/Causal%20Discovery.html">
   Causal Discovery
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../2_Causal_Structure_Learning/Testing-based%20Learner.html">
     Testing-based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../2_Causal_Structure_Learning/Functional-based%20Learner.html">
     Functional-based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../2_Causal_Structure_Learning/Score-based%20Learner.html">
     Score-based Learner
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../2_Causal_Structure_Learning/Causal%20Mediation%20Analysis.html">
   Causal Mediation Analysis
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Effect Learning (CEL)
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html">
   <strong>
    Single Stage – Paradigm 1
   </strong>
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/>
  <label for="toctree-checkbox-2">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/ATE.html">
     ATE Estimation
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/HTE.html">
     HTE Estimation
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/>
    <label for="toctree-checkbox-3">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/S-learner.html">
       <strong>
        1. S-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/T-learner.html">
       <strong>
        2. T-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/X-learner.html">
       <strong>
        3. X-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/R-Learner.html">
       <strong>
        4. R learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/DR-Learner.html">
       <strong>
        5. DR-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Lp-R-Learner.html">
       <strong>
        6. Lp-R-learner
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/GRF.html">
       <strong>
        7. Generalized Random Forest
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Dragonnet.html">
       <strong>
        8. Dragon Net
       </strong>
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Mediation%20Analysis.html">
     Mediation Analysis
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%202/underMDP.html">
   Markov Decision Processes – Paradigm 2
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Panel%20Data.html">
   Panel Data  – Paradigm 3
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-4" name="toctree-checkbox-4" type="checkbox"/>
  <label for="toctree-checkbox-4">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/DiD.html">
     <strong>
      Difference in Difference
     </strong>
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20Control.html">
     <strong>
      Synthetic Control
     </strong>
    </a>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Extensions.html">
     Extensions
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-5" name="toctree-checkbox-5" type="checkbox"/>
    <label for="toctree-checkbox-5">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Matrix%20Completion.html">
       <strong>
        Matrix Completion
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20DiD.html">
       <strong>
        Synthetic DiD
       </strong>
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/H1SL_H2SL.html">
       <strong>
        H1SL and H2SL
       </strong>
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 1
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario1/Single%20Stage.html">
   Single Stage
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario1/Discrete.html">
   Discrete Action Space
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-6" name="toctree-checkbox-6" type="checkbox"/>
  <label for="toctree-checkbox-6">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Q-learning_Single.html">
     Q-Learning (Single Stage)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/A-learning_Single.html">
     A-Learning (Single Stage)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Classification/O-Learning.html">
     Outcome Weighted Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Quantile/QuantileOTR_test.html">
     <strong>
      Quantile Optimal Treatment Regime
     </strong>
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario1/Continuous.html">
   Continuous Action Space
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-7" name="toctree-checkbox-7" type="checkbox"/>
  <label for="toctree-checkbox-7">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Continuous/Deep%20Jump%20Learner.html">
     Deep Jump Learner for Continuous Actions
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Continuous/Kernel-Based%20Learner.html">
     Kernel-Based Learner
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Continuous/Outcome%20Learning.html">
     Outcome Learning
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario1/PlanToDo.html">
   Plan To Do
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-8" name="toctree-checkbox-8" type="checkbox"/>
  <label for="toctree-checkbox-8">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario1/Classification/E-learning.html">
     Entropy learning
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 2
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario2/preliminary_MDP-potential-outcome.html">
   Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario2/Evaluation.html">
   Policy Evaluation–Value Estimation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-9" name="toctree-checkbox-9" type="checkbox"/>
  <label for="toctree-checkbox-9">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario2/FQE.html">
     Fitted-Q Evaluation
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario2/IPW_Infinite.html">
     Importance Sampling for Policy Evaluation (Infinite Horizon)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario2/DR_Infinite.html">
     Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario2/Deeply_Debiased.html">
     Deeply-Debiased Off-Policy Evaluation
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario2/MediationRL.html">
   Policy Evaluation--Mediation Analysis
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario2/Optimization.html">
   Policy Optimization
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-10" name="toctree-checkbox-10" type="checkbox"/>
  <label for="toctree-checkbox-10">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario2/FQI.html">
     Fitted-Q Iteration
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 3
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario3/Multi%20Stage.html">
   Multiple Stages (DTR)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario3/Q-learning_Multiple.html">
   Q-Learning (Multiple Stages)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario3/A-learning_Multiple.html">
   A-Learning (Multiple Stages)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 4
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario4/Bandits.html">
   Overview: Bandits ALgorithm
  </a>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario4/MAB/MAB.html">
   Multi-Armed Bandits (MAB)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-11" name="toctree-checkbox-11" type="checkbox"/>
  <label for="toctree-checkbox-11">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/MAB/Epsilon_Greedy.html">
     <span class="math notranslate nohighlight">
      \(\epsilon\)
     </span>
     -Greedy
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/MAB/UCB.html">
     UCB
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/MAB/TS.html">
     TS
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario4/Contextual_Bandits/Contextual_Bandits.html">
   Contextual Bandits
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-12" name="toctree-checkbox-12" type="checkbox"/>
  <label for="toctree-checkbox-12">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/Contextual_Bandits/LinUCB.html">
     LinUCB
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/Contextual_Bandits/LinTS.html">
     LinTS
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario4/Meta_Bandits/Meta_Bandits.html">
   Meta Bandits
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-13" name="toctree-checkbox-13" type="checkbox"/>
  <label for="toctree-checkbox-13">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/Meta_Bandits/Meta_TS.html">
     Meta Thompson Sampling
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/Meta_Bandits/MTTS.html">
     Multi-Task Thompson Sampling (MTTS)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario4/Structured_Bandits/Structured_Bandit.html">
   Structured Bandit (Slate Recommendation)
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-14" name="toctree-checkbox-14" type="checkbox"/>
  <label for="toctree-checkbox-14">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/Learning%20to%20rank.html">
     Online Learning to Rank (Cascading Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-15" name="toctree-checkbox-15" type="checkbox"/>
    <label for="toctree-checkbox-15">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/TS_Cascade.html">
       TS_Cascade
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/CascadeLinTS.html">
       CascadeLinTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/MTSS_Cascade.html">
       MTSS_Cascade
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/Combinatorial%20Optimization.html">
     Online Combinatorial Optimization (Combinatorial Semi-Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-16" name="toctree-checkbox-16" type="checkbox"/>
    <label for="toctree-checkbox-16">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/CombTS.html">
       CombTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/CombLinTS.html">
       CombLinTS
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/MTSS_Comb.html">
       MTSS_Comb
      </a>
     </li>
    </ul>
   </li>
   <li class="toctree-l2 has-children">
    <a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/Assortment%20Optimization.html">
     Dynamic Assortment Optimization (Multinomial Logit Bandit)
    </a>
    <input class="toctree-checkbox" id="toctree-checkbox-17" name="toctree-checkbox-17" type="checkbox"/>
    <label for="toctree-checkbox-17">
     <i class="fas fa-chevron-down">
     </i>
    </label>
    <ul>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/TS_MNL_Beta.html">
       TS_MNL
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/TS_Contextual_MNL.html">
       TS_Contextual_MNL
      </a>
     </li>
     <li class="toctree-l3">
      <a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/MTSS_MNL.html">
       MTSS_MNL
      </a>
     </li>
    </ul>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../Scenario4/OnlineEval/Online%20Policy%20Evaluation.html">
   Online Policy Evaluation
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-18" name="toctree-checkbox-18" type="checkbox"/>
  <label for="toctree-checkbox-18">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/OnlineEval/Direct%20Online%20Policy%20Evaluator.html">
     Direct Online Policy Evaluator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/OnlineEval/Inverse%20Probability%20Weighted%20Online%20Policy%20Evaluator.html">
     Inverse Probability Weighted Online Policy Evaluator
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../Scenario4/OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html">
     Doubly Robust Online Policy Evaluator
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 5
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Ooline Policy Learning and Evaluation in Markovian Environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Causal Policy Learning (CPL)--Paradigm 6
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../Scenario6/OnlineRL_non_Markov.html">
   Ooline Policy Learning in Non-Markovian Environments
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Case Studies
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1 has-children">
  <a class="reference internal" href="../../5_Case_Study/MIMIC3/MIMIC3_intro.html">
   Mimic3
  </a>
  <input class="toctree-checkbox" id="toctree-checkbox-19" name="toctree-checkbox-19" type="checkbox"/>
  <label for="toctree-checkbox-19">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul>
   <li class="toctree-l2">
    <a class="reference internal" href="../../5_Case_Study/MIMIC3/MIMIC3-Demo-Ver2.html">
     Mimic3 Demo-Ver2
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../5_Case_Study/MIMIC3/Single_Stage.html">
     MIMIC III (Single-Stage)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../5_Case_Study/MIMIC3/Longitudinal.html">
     MIMIC III (3-Stages)
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../../5_Case_Study/MIMIC3/Infinite_Horizon.html">
     MIMIC III (Infinite Horizon)
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../5_Case_Study/MovieLens/MovieLens.html">
   MovieLens
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<div class="menu-dropdown menu-dropdown-launch-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Launch interactive content">
      <i class="fas fa-rocket"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://mybinder.org/v2/gh/executablebooks/jupyter-book/master?urlpath=tree/docs/4_Causal_Policy_Learning/Scenario5/OnlineRL_Markov.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Launch on Binder"
>
  

<span class="headerbtn__icon-container">
  
    <img src="../../_static/images/logo_binder.svg">
  </span>
<span class="headerbtn__text-container">Binder</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/executablebooks/jupyter-book"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2F4_Causal_Policy_Learning/Scenario5/OnlineRL_Markov.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../_sources/4_Causal_Policy_Learning/Scenario5/OnlineRL_Markov.ipynb"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.ipynb</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model">
   Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-evaluation">
   Policy Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-optimization">
   Policy Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-gradient">
     Policy gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-based-approximate-dp">
     Value-based (Approximate DP)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#actor-critic">
     Actor Critic
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Ooline Policy Learning and Evaluation in Markovian Environments</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model">
   Model
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-evaluation">
   Policy Evaluation
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-optimization">
   Policy Optimization
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#policy-gradient">
     Policy gradient
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#value-based-approximate-dp">
     Value-based (Approximate DP)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#actor-critic">
     Actor Critic
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#references">
   References
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="ooline-policy-learning-and-evaluation-in-markovian-environments">
<span id="section-online-rl"></span><h1>Ooline Policy Learning and Evaluation in Markovian Environments<a class="headerlink" href="#ooline-policy-learning-and-evaluation-in-markovian-environments" title="Permalink to this headline">#</a></h1>
<p>This chapter focuses on the online policy learning and evaluation problem in an Markov Decision Process (MDP), which is the most well-known and typically default setup of Reinforcement Learning (RL).
From the causal perspective, the data dependency structure is the same with that in <a class="reference internal" href="../Scenario2/preliminary_MDP-potential-outcome.html#section-ope-opo-preliminary"><span class="std std-ref">offline RL</span></a>, with the major difference in that the data collection policy is now data-dependent and the objective is sometimes shifted from finding the optimal policy to maximizing the cumulative rewards.
As this is a vast area with a huge literature, we do not aim to repeat the disucssions hear. Instead, we will focus on connecting online RL to the other parts of this paper.
We refer interested readers to <span id="id1">Sutton and Barto [<a class="reference internal" href="#id27" title="Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction. MIT press, 2018.">SB18</a>]</span> for more materials.</p>
<section id="model">
<h2>Model<a class="headerlink" href="#model" title="Permalink to this headline">#</a></h2>
<p>We first recap the infinite-horizon discounted MDP model that we introduced in <a class="reference internal" href="../Scenario2/preliminary_MDP-potential-outcome.html#section-ope-opo-preliminary"><span class="std std-ref">offline RL</span></a>.
For any <span class="math notranslate nohighlight">\(t\ge 0\)</span>, let <span class="math notranslate nohighlight">\(\bar{a}_t=(a_0,a_1,\cdots,a_t)^\top\in \mathcal{A}^{t+1}\)</span> denote a treatment history vector up to time <span class="math notranslate nohighlight">\(t\)</span>.
Let <span class="math notranslate nohighlight">\(\mathbb{S} \subset \mathbb{R}^d\)</span> denote the support of state variables and <span class="math notranslate nohighlight">\(S_0\)</span> denote the initial state variable.
For any <span class="math notranslate nohighlight">\((\bar{a}_{t-1},\bar{a}_{t})\)</span>, let <span class="math notranslate nohighlight">\(S_{t}^*(\bar{a}_{t-1})\)</span> and <span class="math notranslate nohighlight">\(Y_t^*(\bar{a}_t)\)</span> be the counterfactual state and counterfactual outcome, respectively,  that would occur at time <span class="math notranslate nohighlight">\(t\)</span> had the agent followed the treatment history <span class="math notranslate nohighlight">\(\bar{a}_{t}\)</span>.
The set of potential outcomes up to time <span class="math notranslate nohighlight">\(t\)</span> is given by</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{eqnarray*}
	W_t^*(\bar{a}_t)=\{S_0,Y_0^*(a_0),S_1^*(a_0),\cdots,S_{t}^*(\bar{a}_{t-1}),Y_t^*(\bar{a}_t)\}.
\end{eqnarray*}\]</div>
<p>Let <span class="math notranslate nohighlight">\(W^*=\cup_{t\ge 0,\bar{a}_t\in \{0,1\}^{t+1}} W_t^*(\bar{a}_t)\)</span> be the set of all potential outcomes.</p>
<p>The goodness of  a policy <span class="math notranslate nohighlight">\(\pi\)</span> is measured by its value functions,</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{eqnarray*}
    V^{\pi}(s)=\sum_{t\ge 0} \gamma^t \mathbb{E} \{Y_t^*(\pi)|S_0=s\}, \;\; 	Q^{\pi}(a,s)=\sum_{t\ge 0} \gamma^t \mathbb{E} \{Y_t^*(\pi)|S_0=s, A_0 = a\}. 
\end{eqnarray*}\]</div>
<p>We need two critical assumptions for the MDP model.</p>
<p><strong>(MA) Markov assumption</strong>:  there exists a Markov transition kernel <span class="math notranslate nohighlight">\(\mathcal{P}\)</span> such that  for any <span class="math notranslate nohighlight">\(t\ge 0\)</span>, <span class="math notranslate nohighlight">\(\bar{a}_{t}\in \{0,1\}^{t+1}\)</span> and <span class="math notranslate nohighlight">\(\mathcal{S}\subseteq \mathbb{R}^d\)</span>, we have
<span class="math notranslate nohighlight">\(\mathbb{P}\{S_{t+1}^*(\bar{a}_{t})\in \mathcal{S}|W_t^*(\bar{a}_t)\}=\mathcal{P}(\mathcal{S};a_t,S_t^*(\bar{a}_{t-1})).\)</span></p>
<p><strong>(CMIA) Conditional mean independence assumption</strong>: there exists a function <span class="math notranslate nohighlight">\(r\)</span> such that  for any <span class="math notranslate nohighlight">\(t\ge 0, \bar{a}_{t}\in \{0,1\}^{t+1}\)</span>, we have
<span class="math notranslate nohighlight">\(\mathbb{E} \{Y_t^*(\bar{a}_t)|S_t^*(\bar{a}_{t-1}),W_{t-1}^*(\bar{a}_{t-1})\}=r(a_t,S_t^*(\bar{a}_{t-1}))\)</span>.</p>
</section>
<section id="policy-evaluation">
<h2>Policy Evaluation<a class="headerlink" href="#policy-evaluation" title="Permalink to this headline">#</a></h2>
<p>To either purely evaluate a policy or improve over it, we need to understand its performance (ideally at every state-action tuple), which corresponds to the policy value function estimation and evaluation problem.
We introduce two main appraoches in the section.</p>
<p><strong>Monte Carlo (MC).</strong> In an online environment, the most straightforward approach is to just sample trajectories and use the average observed cumulative reward from sub-trajectories that satisfy our conditions as the estimator.
Due to the sampling nature, this approach is typically referred to as Monte Carlo <span id="id2">[<a class="reference internal" href="#id39" title="Satinder P Singh and Richard S Sutton. Reinforcement learning with replacing eligibility traces. Machine learning, 22(1-3):123–158, 1996.">SS96</a>]</span>.
For example, to estimate the value <span class="math notranslate nohighlight">\(V^{\pi}(s)\)</span> for a given state <span class="math notranslate nohighlight">\(s\)</span>, we can sample <span class="math notranslate nohighlight">\(N\)</span> trajectories following <span class="math notranslate nohighlight">\(\pi\)</span>, then find time points where we visit state <span class="math notranslate nohighlight">\(s\)</span>, and finally use the returns from then on to construct an average as our value estimate.</p>
<p><strong>Temporal-Difference (TD) Learning.</strong>
One limitation of MC is that one has to wait until the end of a trajectory to collect a data point, which makes it less online and incremental.
An alternative is to leverage the Bellman equation and the dynamic optimization structure, as we have utilized in <a class="reference internal" href="../Scenario2/FQE.html#section-fqe"><span class="std std-ref">Paradigm 2</span></a>.
The is known as the Temporal-Difference (TD) Learning <span id="id3">[<a class="reference internal" href="#id38" title="Richard S Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:9–44, 1988.">Sut88</a>]</span>.
The name is from the fact that it involves the estimate at time point <span class="math notranslate nohighlight">\(t\)</span> and $t+1).
We first recall the Bellman equation:</p>
<div class="amsmath math notranslate nohighlight" id="equation-c0d43a2d-4f49-4407-b4b4-403f7b5d9400">
<span class="eqno">(157)<a class="headerlink" href="#equation-c0d43a2d-4f49-4407-b4b4-403f7b5d9400" title="Permalink to this equation">#</a></span>\[\begin{equation}\label{eqn:bellman_Q}
    Q^\pi(a, s) = \mathbb{E}^\pi \Big(R_t + \gamma Q^\pi(A_{t + 1}, S_{t+1})  | A_t = a, S_t = s \Big). 
\end{equation}\]</div>
<p>Therefore, suppose we currently have an Q-function estimate <span class="math notranslate nohighlight">\(\hat{Q}^{\pi}\)</span>.
Then, after collecting a trasition tuple <span class="math notranslate nohighlight">\((s, a, r, s')\)</span>, we can then update the estimate of <span class="math notranslate nohighlight">\(\hat{Q}^{\pi}(s, a)\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-d60b9965-2719-43aa-b870-7aea9fa7c159">
<span class="eqno">(158)<a class="headerlink" href="#equation-d60b9965-2719-43aa-b870-7aea9fa7c159" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{Q}^\pi(a, s) + \alpha \Big[r + \gamma \hat{Q}^\pi(\pi(s'), s')  - \hat{Q}^\pi(a, s) \Big], 
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha\)</span> is a learning rate.</p>
<p><strong>Statistical inference.</strong> As discussed in [Paradigm 4](section:Direct Online Policy Evaluator), statistical inference with adaptively collected data is challenging.
To address that issue, <span id="id4">Shi <em>et al.</em> [<a class="reference internal" href="#id33" title="Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Song. Statistical inference of the value function for reinforcement learning in infinite horizon settings. arXiv preprint arXiv:2001.04515, 2020.">SZLS20</a>]</span> leverages a carefully designed data splitting schema to provide valid asymptotic distribution (and hence the confidence interval).</p>
</section>
<section id="policy-optimization">
<h2>Policy Optimization<a class="headerlink" href="#policy-optimization" title="Permalink to this headline">#</a></h2>
<p>The online policy optimization problem with MDP is the most well-known RL problem.
There are three major approaches: policy-based (policy gradient), value-based (approximate DP), and actor critic.
We will focus on illustrate their main idea and connection to other topics in the book.</p>
<section id="policy-gradient">
<h3>Policy gradient<a class="headerlink" href="#policy-gradient" title="Permalink to this headline">#</a></h3>
<p>The policy-based algorithms (e.g., REINFORCE <span id="id5"></span>, TRPO <span id="id6">Schulman <em>et al.</em> [<a class="reference internal" href="#id42" title="John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learning, 1889–1897. PMLR, 2015.">SLA+15</a>]</span>, PPO <span id="id7">Schulman <em>et al.</em> [<a class="reference internal" href="#id43" title="John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017.">SWD+17</a>]</span>) directly learn a policy function <span class="math notranslate nohighlight">\(\pi\)</span> by applying gradient descent to optimize its value.
In its simplist form, the value estimation is obtained via MC, i.e., sampling trajectories following a policy.
However, the gradient descient is not straightforward, as it requires the value estimation of any policies around the current one to compute the gradient, which is not feasible.
Fortunately, we have the following <em>policy gradient theorem</em>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-882fc5bf-db52-449d-b6f2-9e9b87b8d2e7">
<span class="eqno">(159)<a class="headerlink" href="#equation-882fc5bf-db52-449d-b6f2-9e9b87b8d2e7" title="Permalink to this equation">#</a></span>\[\begin{align}
\bigtriangledown_{\theta}\, J(\theta)
&amp;= 
\mathbb{E}_{\tau \sim \pi(\cdot;\theta)}
\big\{
G(\tau) \times
\big[
\sum_{t=0}^T 
\bigtriangledown_{\theta}\, 
\log \pi(A_t| S_t; \theta)
\big]
\big\}, 
\label{eqn:REINFORCE}
\end{align}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tau\)</span> represents a trajectory and <span class="math notranslate nohighlight">\(\theta\)</span> parameterizes the policy.</p>
</section>
<section id="value-based-approximate-dp">
<h3>Value-based (Approximate DP)<a class="headerlink" href="#value-based-approximate-dp" title="Permalink to this headline">#</a></h3>
<p>The second appraoch is closely related to the Q-function-based appraoch discussed in Paradigm 1 and 2, and in particular, the <a class="reference internal" href="../Scenario2/FQI.html#section-fqi"><span class="std std-ref">FQI</span></a> algorithm.</p>
<p>Recall the Bellman optimality equations: <span class="math notranslate nohighlight">\(Q^*\)</span> is the unique solution of</p>
<div class="amsmath math notranslate nohighlight" id="equation-f30647ca-de29-46a0-9b84-7d7c3e1360ac">
<span class="eqno">(160)<a class="headerlink" href="#equation-f30647ca-de29-46a0-9b84-7d7c3e1360ac" title="Permalink to this equation">#</a></span>\[\begin{equation}
    Q(a, s) = \mathbb{E} \Big(R_t + \gamma \arg \max_{a'} Q(a, S_{t+1})  | A_t = a, S_t = s \Big).  \;\;\;\;\; \text{(1)}. 
\end{equation}\]</div>
<p>Since the right-hand side of (1) is a contraction mapping on <span class="math notranslate nohighlight">\(Q\)</span> and its fixed point is <span class="math notranslate nohighlight">\(Q^*\)</span>, we can iteratively solve the following problem to update the estimation of <span class="math notranslate nohighlight">\(Q^*\)</span>:</p>
<div class="amsmath math notranslate nohighlight" id="equation-27b2778c-8edd-432e-a5a8-6f13519d24c2">
<span class="eqno">(161)<a class="headerlink" href="#equation-27b2778c-8edd-432e-a5a8-6f13519d24c2" title="Permalink to this equation">#</a></span>\[\begin{eqnarray}
	\widehat{Q}^{{\ell}}=\arg \min_{Q} 
	\sum_{(s,a,r,s') \sim D}
	\Big\{
	\gamma \max_{a'} \widehat{Q}^{\ell-1}(a', s') 
    +r- Q(a, s)
\Big\}^2.  \;\;\;\;\; \text{(2)}. 
\end{eqnarray}\]</div>
<p>One major difference lies in how the optimization above is done.
In the offline setting (e.g., <a class="reference internal" href="../Scenario2/FQI.html#section-fqi"><span class="std std-ref">FQI</span></a>), <span class="math notranslate nohighlight">\(D\)</span> is the fixed batch dataset and the optimization is solved fully.
In the original online Q-learning <span id="id8">[]</span> algorithm, we only run one gradient descent step in the optimization.
There are many variants to improve this idea in different practical ways.
Besides, a unique feature of the online setting is the ability to collect new data (i.e., exploration), and how to efficiently explore is another new problem compared with the offline setup.
For example, DQN <span id="id9">[<a class="reference internal" href="#id40" title="Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, and others. Human-level control through deep reinforcement learning. Nature, 518(7540):529–533, 2015.">MKS+15</a>]</span> maintains a replay buffer and apply epsilon-greedy for exploration, and Double DQN <span id="id10">[<a class="reference internal" href="#id41" title="Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In Proceedings of the AAAI conference on artificial intelligence, volume 30. 2016.">VHGS16</a>]</span>) uses two different Q-estimators in the RHS of (2) to solve the over estimation issue.</p>
</section>
<section id="actor-critic">
<h3>Actor Critic<a class="headerlink" href="#actor-critic" title="Permalink to this headline">#</a></h3>
<p>One limitation of the policy gradient approach is the efficiency, since it is heavy to sample new trajectories from scratch to evaluate the current policy and hence has high variance.
A natural idea is:
if we have a value estimator, then it can be used for policy evaluation as well.
Such a motivation is well grounded by the following relationship (there are more extensions):</p>
<div class="amsmath math notranslate nohighlight" id="equation-d09a6ab0-3acb-488a-9710-dded270f9780">
<span class="eqno">(162)<a class="headerlink" href="#equation-d09a6ab0-3acb-488a-9710-dded270f9780" title="Permalink to this equation">#</a></span>\[\begin{align}
\bigtriangledown_{\theta}\, J(\theta)
&amp;= 
\mathbb{E}_{\tau \sim \pi(\cdot;\theta)}
\big\{
G(\tau) \times
\big[
\sum_{t=0}^T 
\bigtriangledown_{\theta}\, 
\log \pi(A_t| S_t; \theta)
\big]
\big\}\\
&amp;= 
\mathbb{E}_{\tau \sim \pi(\cdot;\theta)}
\big\{
\sum_{t=0}^T 
Q_t^{\theta}(S_t, A_t)
\bigtriangledown_{\theta}\,
\log \pi(A_t| S_t; \theta)
% Q_0^{\theta}(s,a) \bigtriangledown_{\theta}\, \pi_\theta(s)
\big\}. 
\end{align}\]</div>
<p>Therefore, an actor-critic maintains both a policy function estimator (the actor) to select actions, and an value function estimator (the critic) that evaluates the current policy and guides the direction to apply gradient descent.
It hences combines the idea of the first two approaches in this section to improve effiicency.
From the other direction, it shares similar ideas with the direct policy search appraoch that we disucssed in Paradigm 1.
Popular actor-critic algorithms include A2C <span id="id11">Mnih <em>et al.</em> [<a class="reference internal" href="#id44" title="Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, 1928–1937. PMLR, 2016.">MBM+16</a>]</span>, SAC <span id="id12">Haarnoja <em>et al.</em> [<a class="reference internal" href="#id45" title="Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning, 1861–1870. PMLR, 2018.">HZAL18</a>]</span>, A3C <span id="id13">Mnih <em>et al.</em> [<a class="reference internal" href="#id44" title="Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In International conference on machine learning, 1928–1937. PMLR, 2016.">MBM+16</a>]</span>)</p>
</section>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<div class="docutils container" id="id14">
<dl class="citation">
<dt class="label" id="id45"><span class="brackets"><a class="fn-backref" href="#id12">HZAL18</a></span></dt>
<dd><p>Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: off-policy maximum entropy deep reinforcement learning with a stochastic actor. In <em>International conference on machine learning</em>, 1861–1870. PMLR, 2018.</p>
</dd>
<dt class="label" id="id44"><span class="brackets">MBM+16</span><span class="fn-backref">(<a href="#id11">1</a>,<a href="#id13">2</a>)</span></dt>
<dd><p>Volodymyr Mnih, Adria Puigdomenech Badia, Mehdi Mirza, Alex Graves, Timothy Lillicrap, Tim Harley, David Silver, and Koray Kavukcuoglu. Asynchronous methods for deep reinforcement learning. In <em>International conference on machine learning</em>, 1928–1937. PMLR, 2016.</p>
</dd>
<dt class="label" id="id40"><span class="brackets"><a class="fn-backref" href="#id9">MKS+15</a></span></dt>
<dd><p>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, and others. Human-level control through deep reinforcement learning. <em>Nature</em>, 518(7540):529–533, 2015.</p>
</dd>
<dt class="label" id="id42"><span class="brackets"><a class="fn-backref" href="#id6">SLA+15</a></span></dt>
<dd><p>John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In <em>International conference on machine learning</em>, 1889–1897. PMLR, 2015.</p>
</dd>
<dt class="label" id="id43"><span class="brackets"><a class="fn-backref" href="#id7">SWD+17</a></span></dt>
<dd><p>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. <em>arXiv preprint arXiv:1707.06347</em>, 2017.</p>
</dd>
<dt class="label" id="id33"><span class="brackets"><a class="fn-backref" href="#id4">SZLS20</a></span></dt>
<dd><p>Chengchun Shi, Sheng Zhang, Wenbin Lu, and Rui Song. Statistical inference of the value function for reinforcement learning in infinite horizon settings. <em>arXiv preprint arXiv:2001.04515</em>, 2020.</p>
</dd>
<dt class="label" id="id39"><span class="brackets"><a class="fn-backref" href="#id2">SS96</a></span></dt>
<dd><p>Satinder P Singh and Richard S Sutton. Reinforcement learning with replacing eligibility traces. <em>Machine learning</em>, 22(1-3):123–158, 1996.</p>
</dd>
<dt class="label" id="id38"><span class="brackets"><a class="fn-backref" href="#id3">Sut88</a></span></dt>
<dd><p>Richard S Sutton. Learning to predict by the methods of temporal differences. <em>Machine learning</em>, 3:9–44, 1988.</p>
</dd>
<dt class="label" id="id27"><span class="brackets"><a class="fn-backref" href="#id1">SB18</a></span></dt>
<dd><p>Richard S Sutton and Andrew G Barto. <em>Reinforcement learning: An introduction</em>. MIT press, 2018.</p>
</dd>
<dt class="label" id="id41"><span class="brackets"><a class="fn-backref" href="#id10">VHGS16</a></span></dt>
<dd><p>Hado Van Hasselt, Arthur Guez, and David Silver. Deep reinforcement learning with double q-learning. In <em>Proceedings of the AAAI conference on artificial intelligence</em>, volume 30. 2016.</p>
</dd>
</dl>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./4_Causal_Policy_Learning\Scenario5"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../Scenario4/OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Doubly Robust Online Policy Evaluator</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../Scenario6/OnlineRL_non_Markov.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Ooline Policy Learning in Non-Markovian Environments</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Causal Decision Making Team<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>