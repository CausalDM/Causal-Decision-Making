{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fef41c9",
   "metadata": {},
   "source": [
    "# Ooline Policy Learning in Non-Markovian Environments \n",
    "\n",
    "An important extension of the [Markov assumption-based RL](section:online_RL) is to the non-Markovian environment. \n",
    "\n",
    "\n",
    "The extension is valuable when either (i) the system dynamic depends on multiple or infinite lagged time points and hence it is infeasible to summarize historical information in a fixed-dimensional vector (the DTR problem that we studied in Paradigm 2). \n",
    "\n",
    "## Model\n",
    "\n",
    "No that assumption. \n",
    "\n",
    "for DTR\n",
    "\n",
    "\n",
    "```{image} POMDP_comparison.png\n",
    ":alt: d2ope\n",
    ":width: 800px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "No unmeasured confounder issue. \n",
    "\n",
    "shi2022off\n",
    "\n",
    "for POMDP\n",
    "\n",
    "```{image} POMDP.png\n",
    ":alt: d2ope\n",
    ":width: 500px\n",
    ":align: center\n",
    "```\n",
    "\n",
    "## Policy learning\n",
    "\n",
    "It is challenging to learn the optimal policy in a POMDP due to the model complexity. Below, we mainly introduce three classes of approaches. \n",
    "\n",
    "1. When the horizon is short (e.g., 2 or 3), it is still feasible to learn a time-dependent policy that directly utilizes the vector of all historical information for decision making. This is the online version of the DTR problem and was recently studied in {cite:t}`hu2020dtr`. However, when the horiozn is longer, such an approach quickly becomes computationally infeasible and certain dimension reduction method is then needed. \n",
    "2. In POMDP, a classic approach is to infer the underlying state via the history information, and then use the inferred state distribution as a *belief state* for decision making {cite:p}`spaan2012partially`. \n",
    "3. In recent years, there is also a growing literature on applying memory-based NN architecture for directly learning the policy from a sequence of history transition tuples {cite:p}`zhu2017improving, meng2021memory`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4871ef2-de01-4755-bd15-b8353e8f0dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
