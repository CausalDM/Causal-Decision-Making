{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b57443d1-88b9-4b9b-ae21-34fd1f42d250",
   "metadata": {},
   "source": [
    "# Overview\n",
    "## Introduction\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "## What to expect?\n",
    "---\n",
    "The diagram below depicts the overall structure of this book, which is comprised of three primary components: **Causal Structure Learning**, **Causal Policy Learning**, and **Causal Effect Learning**. Specifically, in the chapter [**Causal Structure Learning (CSL)**](#SL), we present state-of-the-art techniques for learning the skeleton of causal relationships among input variables. When a causal structure is known, the second chapter of [**Causal Effect Learning (CEL)**](#ML) introduces approaches making causal inference. Finally, the [**Causal Policy Learning (CPL)**](#PL) chapter introduces diverse policy learners to learn optimal policies and evaluate various policies of interest.\n",
    "\n",
    "![Overall.png](Overall.png)\n",
    "\n",
    "Following is a brief summary of the contents of each chapter.\n",
    "\n",
    "## <a name=\"SL\"></a> Causal Structure Learning (CSL)\n",
    "---\n",
    "This chapter discusses three classical techniques for learning causal graphs, each with its own merits and downsides.\n",
    "\n",
    "| Learners      Type    | Supported Model  | Noise Required for Training |   Complexity     | Scale-Free? | Learners Example |\n",
    "|-----------------------|------------------|-----------------------------|------------------|-------------|------------------|\n",
    "|    Testing based      |      Models 1    |          Gaussian           |     $O(p^q)$     |     Yes     |         PC       |\n",
    "|    Functional based   |   Models 1 & 2   |        non-Gaussian         |     $O(p^3)$     |     Yes     |       LiNGAM     |\n",
    "|    Score based        |   Models 1 & 3   |    Gaussian/non-Gaussian    |     $O(p^3)$     |     No      |       NOTEARS    |\n",
    "\n",
    "*$p$ is the number of nodes in $\\mathcal{G}$, and $q$ is the max number of nodes adjacent to any nodes in $\\mathcal{G}$.*\n",
    "\n",
    "## <a name=\"ML\"></a> Causal Effect Learning (CEL)\n",
    "---\n",
    "\n",
    "## <a name=\"PL\"></a> Causal Policy Learning (CPL)\n",
    "---\n",
    "This chapter focuses on four common data dependence structures in decision making, including [**I.I.D.**](#Case1), [**Adaptive Decision Making with Independent States (ADMIS)**](#Case2), [**Adaptive Decision Making with State Transition (ADMST)**](#Case3), and [**All Others**](#Case4). The similarities and differences between four scenarios are summarized as follows.\n",
    "\n",
    "![Causal_DM_Causal_Structure_Table.png](Causal_DM_Causal_Structure_Table.png)\n",
    "\n",
    "### <a name=\"Case1\"></a> Scenario 1: I.I.D\n",
    "As the figure illustrated, observations in Scenario 1 are i.i.d. sampled. For each observation, there are three components, $S_i$ is the context information if there is any, $A_i$ is the action taken, and $R_i$ is the reward received. When there is contextual information, the action would be affected by the contextual information, while the final reward would be affected by both the contextual information and the action. There are two types of problems are widely studied in this context: the **Single-Stage Dynamic Treatment Regime (DTR)** and the **Offline Bandits**[1]. In this book, we mainly focus on the Single-Stage DTR by providing methods for policy evaluation and policy optimization, with a detailed map in [Appendix A](#SingleDTR)\n",
    "\n",
    "### <a name=\"Case2\"></a> Scenario 2: Adaptive Decision Making with Independent States (ADMIS)\n",
    "The Scenario 2 setting is widely examined in the online decision making literature, especially the bandits, where the treatment policy is time-adaptive. Specifically, $H_{t-1}$ includes all the previous observations up to time $t-1$ (include observations at time $t-1$) and is used to update the action policy at time $t$, and therefore affect the action $A_t$. While $S_t$ is i.i.d sampled from the correponding distribution, $R_t$ is influenced by both $A_t$ and $S_t$. Finally, the new observation $(S_t,A_t,R_t)$, in conjunction with all previous observations, would then be formulated as $H_{t+1}$ and affect $A_{t+1}$ only. A structure that lacks contextual information $S_t$ is also very common. In this book, a list of bandits algorithms would be introduced, with a detailed map in [Appendix B](#Bandits).\n",
    "\n",
    "### <a name=\"Case3\"></a> Scenario 3: Adaptive Decision Making with State Transition (ADMST)\n",
    "The Scenario 3 includes two data dependence structures with a Markovian state transition structure. In particular, the offline version is the well-known Markov Decision Process (MDP). While $A_t$ is only affected by $S_t$, both $R_t$ and $S_{t+1}$ would be affected by $(S_t,A_t)$. Given $S_{t}, A_t$, a standard assumption of MDP problems is that $R_t$ and $S_{t+1}$ are independent of previous observations. Build upon the MDP structure, when an adaptive policy is applied, the online version of the strucuture clearly depicts the data generating process, in which $A_t$ would be affected by all previous observations $H_{t-1}$. List of related learning methods would be introduced, with a map in [Appendix C](#MDP).\n",
    "\n",
    "### <a name=\"Case4\"></a> Scenario 4: All Others\n",
    "The Scenario 4 also includes two versions (i.e., offline and online). Taking all the possible causal relationship into account, this scenario considers all the data dependence struacture that are not included in the previous three classical scenarios, such as **Multi-Stage DTR**, **Partially Observable MDP (POMDP)**,$\\cdots$\n",
    "\n",
    "\n",
    "## Appendix\n",
    "---\n",
    "### <a name=\"SingelDTR\"></a> A. Scenario 1\n",
    "![Scenario1.png](Scenario1.png)\n",
    "\n",
    "### <a name=\"Bandits\"></a> B. Scenario 2\n",
    "![Online.png](Online.png)\n",
    "\n",
    "### <a name=\"MDP\"></a> C. Scenario 3\n",
    "![Scenario3.png](Scenario3.png)\n",
    "\n",
    "### D. Scenario 4\n",
    "\n",
    "| Algorithm | Treatment Type | Outcome Type | Evaluation? | Optimization? | C.I.? | Advantages |\n",
    "|:-|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| [Q-Learning](https://www.jmlr.org/papers/volume6/murphy05a/murphy05a.pdf) | Discrete | Continuous (Mean) |✅|✅| ||\n",
    "| [A-Learning](https://www.researchgate.net/profile/Eric-Laber/publication/221665211_Q-_and_A-Learning_Methods_for_Estimating_Optimal_Dynamic_Treatment_Regimes/links/58825d074585150dde402268/Q-and-A-Learning-Methods-for-Estimating-Optimal-Dynamic-Treatment-Regimes.pdf) | Discrete | Continuous (Mean)  |✅|✅| ||\n",
    "\n",
    "\n",
    "## Reference\n",
    "[1] Dudík, M., Langford, J., & Li, L. (2011). [Doubly robust policy evaluation and learning](https://arxiv.org/pdf/1103.4601.pdf). arXiv preprint arXiv:1103.4601.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Supported Offline Algorithms\n",
    "---\n",
    "| Algorithm | Treatment Type | Outcome Type | Single Stage? | Multiple Stages? | Infinite Horizon? | Evaluation? | Optimization? | C.I.? | Advantages |\n",
    "|:-|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|:-:|\n",
    "| [Q-Learning](https://www.jmlr.org/papers/volume6/murphy05a/murphy05a.pdf) | Discrete | Continuous (Mean) |✅|✅| |✅|✅| ||\n",
    "| [A-Learning](https://www.researchgate.net/profile/Eric-Laber/publication/221665211_Q-_and_A-Learning_Methods_for_Estimating_Optimal_Dynamic_Treatment_Regimes/links/58825d074585150dde402268/Q-and-A-Learning-Methods-for-Estimating-Optimal-Dynamic-Treatment-Regimes.pdf) | Discrete | Continuous (Mean) |✅|✅|  |✅|✅| ||\n",
    "| [OWL](https://www.tandfonline.com/doi/pdf/10.1080/01621459.2012.695674?casa_token=bwkVvffpyFcAAAAA:hlN58Fbz59blLj5npZFjEQD-HkPeMevEN5pWWLu_vuIVxPWl5aYShgCVHUVeODAfj6Pr8DpzGFlPZ1E) | Discrete | Continuous (Mean) |✅|❗BOWL| |✅|❗TODO| ||\n",
    "| [Quatile-OTR](https://doi.org/10.1080/01621459.2017.1330204) | Discrete | Continuous (Quantiles) |✅✏️|  |  |✅✏️|✅✏️| ||\n",
    "| [Deep Jump Learner](https://proceedings.neurips.cc/paper/2021/file/816b112c6105b3ebd537828a39af4818-Paper.pdf) | Continuous | Continuous/Discrete |✅|  |  |✅| ✅| | Flexible to implement & Fast to Converge|\n",
    "|**TODO** | | ||  |  ||| ||\n",
    "| Policy Search| | ||  |  ||| ||\n",
    "| Concordance-assisted learning| | ||  |  ||| ||\n",
    "| Entropy learning | | ||  |  ||| ||\n",
    "| **Continuous Action Space (Main Page)** | | ||  |  ||| ||\n",
    "| Kernel-Based Learner | | ||  |  ||| ||\n",
    "| Outcome Learning | | ||  |  ||| ||\n",
    "| **Miscellaneous (Main Page)** | | ||  |  ||| ||\n",
    "| Time-to-Event Data | | ||  |  ||| ||\n",
    "| Adaptively Collected Data | | ||  |  ||| ||\n",
    "\n",
    "\n",
    "## Supported Online Algorithms\n",
    "| algorithm | Reward | with features? | Advantage | Progress|\n",
    "|:-|:-:|:-:|:-:|:-:|\n",
    "| **Single-Item Recommendation** || | |95% (proofreading)|\n",
    "| [$\\epsilon$-greedy]() | Binary/Gaussian | |Simple| 100%|\n",
    "| [TS](https://www.ccs.neu.edu/home/vip/teach/DMcourse/5_topicmodel_summ/notes_slides/sampling/TS_Tutorial.pdf) | Binary/Guaasian | | |100%|\n",
    "| [LinTS](http://proceedings.mlr.press/v28/agrawal13.pdf) | Gaussian | ✅ | | 90% notebook|\n",
    "| [GLMTS](http://proceedings.mlr.press/v108/kveton20a/kveton20a.pdf) | GLM | ✅ | | 0% |\n",
    "| [UCB1](https://link.springer.com/content/pdf/10.1023/A:1013689704352.pdf) | Binary/Gaussian | | |  100%|\n",
    "| [LinUCB](https://dl.acm.org/doi/pdf/10.1145/1772690.1772758?casa_token=CJjeIziLmjEAAAAA:CkRvgHQNqpy10rzcUP5kx31NWJmgSldd6zx8wZxskZYCoCc8v7EDIw3t3Gk1_6mfurqQTqRZ7fVA) | Guassian | ✅ | | 50% (code ready)|\n",
    "| [UCB-GLM](http://proceedings.mlr.press/v70/li17c/li17c.pdf) | GLM | ✅ | | 0%|\n",
    "|*Multi-Task*|| | |0% (main page)|\n",
    "|Meta-TS|| | |50% (code almost ready)|\n",
    "|MTSS|| | |50% (code almost ready)|\n",
    "| **Slate Recommendation** || | |95% (proofreading)|\n",
    "| *Online Learning to Rank* || | |95% (proofreading)|\n",
    "| [TS-Cascade](http://proceedings.mlr.press/v89/cheung19a/cheung19a.pdf) | Binary | | | 40% code+notebook|\n",
    "| [CascadeLinTS](https://arxiv.org/pdf/1603.05359.pdf) | Binary | ✅ | |  40% code+notebook|\n",
    "| [MTSS-Cascade](https://arxiv.org/pdf/2202.13227.pdf) | Binary | ✅ | Scalable, Robust, accounts for inter-item heterogeneity | 95% (proofreading)|\n",
    "| *Online Combinatorial Optimization* || | |95% (proofreading)|\n",
    "| [CombTS](http://proceedings.mlr.press/v80/wang18a/wang18a.pdf) | | | | 50% code ready|\n",
    "| [CombLinTS](http://proceedings.mlr.press/v37/wen15.pdf) | | ✅ | | 50% code ready\n",
    "| [MTSS-Comb](https://arxiv.org/pdf/2202.13227.pdf) | Continuous | ✅ | Scalable, Robust, accounts for inter-item heterogeneity | 95% (proofreading)|\n",
    "| *Dynamic Assortment Optimization* || | |95% (proofreading)|\n",
    "| [MNL-Thompson-Beta](http://proceedings.mlr.press/v65/agrawal17a/agrawal17a.pdf) | Binary | | |  40% code+notebook|\n",
    "| [TS-Contextual-MNL](https://proceedings.neurips.cc/paper/2019/file/36d7534290610d9b7e9abed244dd2f28-Paper.pdf) | Binary | ✅ | |  40% code+notebook|\n",
    "| [MTSS-MNL](https://arxiv.org/pdf/2202.13227.pdf) | Binary | ✅ | Scalable, Robust, accounts for inter-item heterogeneity |95% (proofreading)|\n",
    "| [UCB-MNL [6]](https://pubsonline.informs.org/doi/pdf/10.1287/opre.2018.1832?casa_token=6aWDZ292SSsAAAAA:KAG0_j813jxeL6PVNI1dcdLv_CHD7oQ6SKinqxcoq0pC2mX5Q2qGgyYvE8esMSXZPlqOanCPOQ) | Binary | | |80% (notebook)| \n",
    "| [LUMB [7]](https://arxiv.org/pdf/1805.02971.pdf) | Binary | ✅ | | 50% code ready|\n",
    "| **TODO** || | ||\n",
    "| environment with real data || | |50% MAB is ready|\n",
    "| overall simulation || | ||"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6aeca1-d3ea-4ea1-8f36-f58d7839de0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
