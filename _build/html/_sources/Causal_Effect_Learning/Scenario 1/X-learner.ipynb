{"cells":[{"cell_type":"markdown","source":["### **3. X-learner**\n","Next, let's introduce the X-learner. As a combination of S-learner and T-learner, the X-learner can use information from the control(treatment) group to derive better estimators for the treatment(control) group, which is provably more efficient than the above two.\n","\n","The basic\n","\n","\n","**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods (same as T-learner);\n","\n","\n","**Step 2:**  Obtain the imputed treatment effects for individuals\n","\\begin{equation*}\n","\\tilde{\\Delta}_i^1:=R_i^1-\\hat\\mu_0(S_i^1), \\quad \\tilde{\\Delta}_i^0:=\\hat\\mu_1(S_i^0)-R_i^0.\n","\\end{equation*}\n","\n","**Step 3:**  Fit the imputed treatment effects to obtain $\\hat\\tau_1(s):=\\mathbb{E}[\\tilde{\\Delta}_i^1|S=s]$ and $\\hat\\tau_0(s):=\\mathbb{E}[\\tilde{\\Delta}_i^0|S=s]$;\n","\n","**Step 4:**  The final HTE estimator is given by\n","\\begin{equation*}\n","\\hat{\\tau}_{\\text{X-learner}}(s)=g(s)\\hat\\tau_0(s)+(1-g(s))\\hat\\tau_1(s),\n","\\end{equation*}\n","\n","where $g(s)$ is a weight function between $[0,1]$. A possible way is to use the propensity score model as an estimate of $g(s)$."],"metadata":{"id":"8lwheJQ8RxAw"},"id":"8lwheJQ8RxAw"},{"cell_type":"code","source":["import sys\n","!{sys.executable} -m pip install scikit-uplift"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676133719672,"user_tz":300,"elapsed":5036,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"2f3f929c-fbba-40a3-d9bf-b74ea77f3f7f","id":"epBAzoVzVqTK"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-uplift\n","  Downloading scikit_uplift-0.5.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.21.6)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (4.64.1)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.0.2)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (3.2.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.3.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (2.25.1)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.2.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.7.3)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (3.1.0)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->scikit-uplift) (2022.7.1)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (2.10)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (1.24.3)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (2022.12.7)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-uplift) (1.15.0)\n","Installing collected packages: scikit-uplift\n","Successfully installed scikit-uplift-0.5.1\n"]}],"id":"epBAzoVzVqTK"},{"cell_type":"code","source":["# import related packages\n","from matplotlib import pyplot as plt;\n","from lightgbm import LGBMRegressor;\n","from sklearn.linear_model import LinearRegression\n","from causaldm._util_causaldm import *;"],"metadata":{"id":"kDtKXxNtVqTM"},"execution_count":null,"outputs":[],"id":"kDtKXxNtVqTM"},{"cell_type":"code","source":["n = 10**3  # sample size in observed data\n","n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n","seed=223"],"metadata":{"id":"dMktSxiPVqTN"},"execution_count":null,"outputs":[],"id":"dMktSxiPVqTN"},{"cell_type":"code","source":["# Get data\n","data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n","#data_target = get_data_simulation(n0, seed, policy=\"target\")\n","\n","# The true expected heterogeneous treatment effect\n","HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n","\n"],"metadata":{"id":"4rhP8UeSVqTN"},"execution_count":null,"outputs":[],"id":"4rhP8UeSVqTN"},{"cell_type":"code","source":["# Step 1: Fit two models under treatment and control separately, same as T-learner\n","\n","import numpy as np\n","mu0 = LGBMRegressor(max_depth=3)\n","mu1 = LGBMRegressor(max_depth=3)\n","\n","S_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2]\n","S_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2]\n","R_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] \n","R_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] \n","\n","mu0.fit(S_T0, R_T0)\n","mu1.fit(S_T1, R_T1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfb-mplOP9HJ","executionInfo":{"status":"ok","timestamp":1676133738275,"user_tz":300,"elapsed":449,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"8b2be802-5918-414b-de0f-8d26dd0930cf"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMRegressor(max_depth=3)"]},"metadata":{},"execution_count":6}],"id":"sfb-mplOP9HJ"},{"cell_type":"code","source":["# Step 2: impute the potential outcomes that are unobserved in original data\n","\n","n_T0 = len(R_T0)\n","n_T1 = len(R_T1)\n","\n","Delta0 = mu1.predict(S_T0) - R_T0\n","Delta1 = R_T1 - mu0.predict(S_T1) "],"metadata":{"id":"zb42ZMw3pkqm"},"execution_count":null,"outputs":[],"id":"zb42ZMw3pkqm"},{"cell_type":"code","source":["# Step 3: Fit tau_1(s) and tau_0(s)\n","\n","tau0 = LGBMRegressor(max_depth=2)\n","tau1 = LGBMRegressor(max_depth=2)\n","\n","tau0.fit(S_T0, Delta0)\n","tau1.fit(S_T1, Delta1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxYLjE0Ar2_5","executionInfo":{"status":"ok","timestamp":1676133740928,"user_tz":300,"elapsed":194,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"539a9b7e-21ad-4c03-8a48-da65ec5d3a2e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMRegressor(max_depth=2)"]},"metadata":{},"execution_count":8}],"id":"pxYLjE0Ar2_5"},{"cell_type":"code","source":["# Step 4: fit the propensity score model $\\hat{g}(s)$ and obtain the final HTE estimator by taking weighted average of tau0 and tau1\n","from sklearn.linear_model import LogisticRegression \n","\n","g = LogisticRegression()\n","g.fit(data_behavior.iloc[:,0:2],data_behavior['A'])\n","\n","HTE_X_learner = g.predict_proba(data_behavior.iloc[:,0:2])[:,0]*tau0.predict(data_behavior.iloc[:,0:2]) + g.predict_proba(data_behavior.iloc[:,0:2])[:,1]*tau1.predict(data_behavior.iloc[:,0:2])\n","\n","\n"],"metadata":{"id":"LRvEZ4uluT-U"},"execution_count":null,"outputs":[],"id":"LRvEZ4uluT-U"},{"cell_type":"code","source":["print(\"X-learner:  \",HTE_X_learner[0:8])\n","print(\"true value: \",HTE_true[0:8].to_numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676133743287,"user_tz":300,"elapsed":172,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"a45b9739-0f4e-4934-dacd-e17420d86055","id":"Mz8I_J0h4N6B"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["X-learner:   [ 1.9341  1.9235  0.2944  0.2013 -0.4147 -0.5626 -2.214  -1.5443]\n","true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"]}],"id":"Mz8I_J0h4N6B"},{"cell_type":"markdown","source":["From the result above we can see that X-learner can roughly catch the trend of treatment effect w.r.t. the change of baseline information $S$. In this synthetic example, X-learner also performs slightly better than T-learner."],"metadata":{"id":"XyxjzDBy4N6k"},"id":"XyxjzDBy4N6k"},{"cell_type":"code","source":["Bias_X_learner = np.sum(HTE_X_learner-HTE_true)/n\n","Variance_X_learner = np.sum((HTE_X_learner-HTE_true)**2)/n\n","print(\"The overall estimation bias of X-learner is :     \", Bias_X_learner, \", \\n\", \"The overall estimation variance of X-learner is :\",Variance_X_learner,\". \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676133743982,"user_tz":300,"elapsed":12,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"b99bc160-95ce-4eee-8718-376a69ce0164","id":"rlbacETd4N6l"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The overall estimation bias of X-learner is :      0.2827518068171628 , \n"," The overall estimation variance of X-learner is : 1.7686646616779012 . \n","\n"]}],"id":"rlbacETd4N6l"},{"cell_type":"markdown","source":["**Conclusion:** In this toy example, the overall estimation variance of X-learner is the smallest, followed by T-learner, and the worst is given by S-learner.\n","\n"],"metadata":{"id":"EnxQ_Tkg4o_q"},"id":"EnxQ_Tkg4o_q"},{"cell_type":"markdown","source":["**Note**: For more details about the meta learners, please refer to [1]."],"metadata":{"id":"zxpRscObJmbX"},"id":"zxpRscObJmbX"},{"cell_type":"markdown","source":["## References\n","1. Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 4156–4165.\n"],"metadata":{"id":"nyirbjS5JdGh"},"id":"nyirbjS5JdGh"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}