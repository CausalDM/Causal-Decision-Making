{"cells":[{"cell_type":"markdown","source":["### **1. S-learner**\n","\n","\n","The first estimator we would like to introduce is the S-learner, also known as a ``single learner\". This is one of the most foundamental learners in HTE esitmation, and is very easy to implement.\n","\n","Under three common assumptions in causal inference, i.e. (1) consistency, (2) no unmeasured confounders (NUC), (3) positivity assumption, the heterogeneous treatment effect can be identified by the observed data, where\n","\\begin{equation*}\n","\\tau(s)=\\mathbb{E}[R|S,A=1]-\\mathbb{E}[R|S,A=0].\n","\\end{equation*}\n","\n","The basic idea of S-learner is to fit a model for $\\mathbb{E}[R|S,A]$, and then construct a plug-in estimator based on the expression above. Specifically, the algorithm can be summarized as below:\n","\n","**Step 1:**  Estimate the combined response function $\\mu(s,a):=\\mathbb{E}[R|S=s,A=a]$ with any regression algorithm or supervised machine learning methods;\n","\n","**Step 2:**  Estimate HTE by \n","\\begin{equation*}\n","\\hat{\\tau}_{\\text{S-learner}}(s)=\\hat\\mu(s,1)-\\hat\\mu(s,0).\n","\\end{equation*}\n","\n","\n"],"metadata":{"id":"yfiI-lSZRuOP"},"id":"yfiI-lSZRuOP"},{"cell_type":"code","source":["import sys\n","!{sys.executable} -m pip install scikit-uplift"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zxo4NKRBicrm","executionInfo":{"status":"ok","timestamp":1675480517517,"user_tz":300,"elapsed":7939,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"b822a214-e15e-495e-b1c5-e56630be5b8b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-uplift\n","  Downloading scikit_uplift-0.5.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m839.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (2.25.1)\n","Requirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.21.6)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (3.2.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (4.64.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.2.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.7.3)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (2.8.2)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (1.4.4)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (0.11.0)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (3.0.9)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->scikit-uplift) (2022.7.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (2.10)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-uplift) (1.15.0)\n","Installing collected packages: scikit-uplift\n","Successfully installed scikit-uplift-0.5.1\n"]}],"id":"zxo4NKRBicrm"},{"cell_type":"code","source":["# import related packages\n","from matplotlib import pyplot as plt;\n","from lightgbm import LGBMRegressor;\n","from sklearn.linear_model import LinearRegression\n","from drive.MyDrive.CausalDM.causaldm._util_causaldm import *;"],"metadata":{"id":"eRpP5k9MBtzO","executionInfo":{"status":"ok","timestamp":1675480604892,"user_tz":300,"elapsed":4593,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":5,"outputs":[],"id":"eRpP5k9MBtzO"},{"cell_type":"code","source":["n = 10**3  # sample size in observed data\n","n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n","seed=223"],"metadata":{"id":"lovM_twTxuOj","executionInfo":{"status":"ok","timestamp":1675480604893,"user_tz":300,"elapsed":31,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":6,"outputs":[],"id":"lovM_twTxuOj"},{"cell_type":"code","source":["# Get data\n","data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n","#data_target = get_data_simulation(n0, seed, policy=\"target\")\n","\n","# The true expected heterogeneous treatment effect\n","HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n","\n"],"metadata":{"id":"JhfJntzcVVy2","executionInfo":{"status":"ok","timestamp":1675480604894,"user_tz":300,"elapsed":31,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":7,"outputs":[],"id":"JhfJntzcVVy2"},{"cell_type":"code","source":["data_behavior"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":424},"id":"F4yvhP_yJ9DH","executionInfo":{"status":"ok","timestamp":1675480604895,"user_tz":300,"elapsed":31,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"3a84de92-f342-44ba-fa92-b4094f9382b1"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["           S1        S2  A         R\n","0    0.034775  2.453145  1  7.167637\n","1    0.084880 -1.234459  0 -1.553798\n","2   -0.144626  2.040543  1  5.956732\n","3    0.148426 -0.021139  1  1.095578\n","4   -0.120852  1.377594  1  4.323133\n","..        ...       ... ..       ...\n","995 -2.022440  1.887551  0  6.797542\n","996  0.411179 -1.655833  0 -2.722846\n","997  0.155706 -0.992197  0 -1.140100\n","998 -1.510241  0.828438  0  4.167118\n","999 -1.744187  0.857147  0  4.458481\n","\n","[1000 rows x 4 columns]"],"text/html":["\n","  <div id=\"df-c61c9893-973f-4fb6-a9fc-be419e504f49\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>S1</th>\n","      <th>S2</th>\n","      <th>A</th>\n","      <th>R</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.034775</td>\n","      <td>2.453145</td>\n","      <td>1</td>\n","      <td>7.167637</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0.084880</td>\n","      <td>-1.234459</td>\n","      <td>0</td>\n","      <td>-1.553798</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.144626</td>\n","      <td>2.040543</td>\n","      <td>1</td>\n","      <td>5.956732</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0.148426</td>\n","      <td>-0.021139</td>\n","      <td>1</td>\n","      <td>1.095578</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>-0.120852</td>\n","      <td>1.377594</td>\n","      <td>1</td>\n","      <td>4.323133</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>995</th>\n","      <td>-2.022440</td>\n","      <td>1.887551</td>\n","      <td>0</td>\n","      <td>6.797542</td>\n","    </tr>\n","    <tr>\n","      <th>996</th>\n","      <td>0.411179</td>\n","      <td>-1.655833</td>\n","      <td>0</td>\n","      <td>-2.722846</td>\n","    </tr>\n","    <tr>\n","      <th>997</th>\n","      <td>0.155706</td>\n","      <td>-0.992197</td>\n","      <td>0</td>\n","      <td>-1.140100</td>\n","    </tr>\n","    <tr>\n","      <th>998</th>\n","      <td>-1.510241</td>\n","      <td>0.828438</td>\n","      <td>0</td>\n","      <td>4.167118</td>\n","    </tr>\n","    <tr>\n","      <th>999</th>\n","      <td>-1.744187</td>\n","      <td>0.857147</td>\n","      <td>0</td>\n","      <td>4.458481</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1000 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c61c9893-973f-4fb6-a9fc-be419e504f49')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c61c9893-973f-4fb6-a9fc-be419e504f49 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c61c9893-973f-4fb6-a9fc-be419e504f49');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"id":"F4yvhP_yJ9DH"},{"cell_type":"code","source":["SandA = data_behavior.iloc[:,0:3]"],"metadata":{"id":"Nxf-mXJmFrEl","executionInfo":{"status":"ok","timestamp":1675480604896,"user_tz":300,"elapsed":30,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":9,"outputs":[],"id":"Nxf-mXJmFrEl"},{"cell_type":"code","source":["# S-learner\n","S_learner = LGBMRegressor(max_depth=5)\n","#S_learner = LinearRegression()\n","#SandA = np.hstack((S.to_numpy(),A.to_numpy().reshape(-1,1)))\n","S_learner.fit(SandA, data_behavior['R'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"h5G8dAwM-PGO","executionInfo":{"status":"ok","timestamp":1675480604897,"user_tz":300,"elapsed":29,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"0ad1d5ef-f445-4246-a321-36f8ee9867ab"},"execution_count":10,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMRegressor(max_depth=5)"]},"metadata":{},"execution_count":10}],"id":"h5G8dAwM-PGO"},{"cell_type":"code","source":["HTE_S_learner = S_learner.predict(np.hstack(( data_behavior.iloc[:,0:2].to_numpy(),np.ones(n).reshape(-1,1)))) - S_learner.predict(np.hstack(( data_behavior.iloc[:,0:2].to_numpy(),np.zeros(n).reshape(-1,1))))\n"],"metadata":{"id":"Vqsb5wLTaR0q","executionInfo":{"status":"ok","timestamp":1675480604898,"user_tz":300,"elapsed":24,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":11,"outputs":[],"id":"Vqsb5wLTaR0q"},{"cell_type":"markdown","source":["To evaluate how well S-learner is in estimating heterogeneous treatment effect, we compare its estimates with the true value for the first 10 subjects:"],"metadata":{"id":"FA-F8Jc_T5Lz"},"id":"FA-F8Jc_T5Lz"},{"cell_type":"code","source":["print(\"S-learner:  \",HTE_S_learner[0:8])\n","print(\"true value: \",HTE_true[0:8].to_numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675480604898,"user_tz":300,"elapsed":23,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"834ffc4a-bf31-429e-e030-888052a4b36e","id":"GvHnTOxmT5Lz"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["S-learner:   [-0.1492  0.1687 -0.589  -0.0319 -0.8354 -0.5843 -0.4577 -2.0791]\n","true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"]}],"id":"GvHnTOxmT5Lz"},{"cell_type":"code","source":["Bias_S_learner = np.sum(HTE_S_learner-HTE_true)/n\n","Variance_S_learner = np.sum((HTE_S_learner-HTE_true)**2)/n\n","print(\"The overall estimation bias of S-learner is :     \", Bias_S_learner, \", \\n\", \"The overall estimation variance of S-learner is :\",Variance_S_learner,\". \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g9NgyjQ7PqRf","executionInfo":{"status":"ok","timestamp":1675480605638,"user_tz":300,"elapsed":346,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"ee4a1419-12a6-4a2a-f27c-db3f9d6b8487"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["The overall estimation bias of S-learner is :      0.2857192464627009 , \n"," The overall estimation variance of S-learner is : 4.079505077680185 . \n","\n"]}],"id":"g9NgyjQ7PqRf"},{"cell_type":"markdown","source":["**Conclusion:** The performance of S-learner, at least in this toy example, is not very attractive. Although it is the easiest approach to implement, the over-simplicity tends to cover some information that can be better explored with some advanced approaches."],"metadata":{"id":"mVAZTZYTUKJ6"},"id":"mVAZTZYTUKJ6"},{"cell_type":"markdown","source":["\n","### **2. T-learner**\n","The second learner is called T-learner, which denotes ``two learners\". Instead of fitting a single model to estimate the potential outcomes under both treatment and control groups, T-learner aims to learn different models for $\\mathbb{E}[R(1)|S]$ and $\\mathbb{E}[R(0)|S]$ separately, and finally combines them to obtain a final HTE estimator.\n","\n","Define the control response function as $\\mu_0(s)=\\mathbb{E}[R(0)|S=s]$, and the treatment response function as $\\mu_1(s)=\\mathbb{E}[R(1)|S=s]$. The algorithm of T-learner is summarized below:\n","\n","**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods;\n","\n","**Step 2:**  Estimate HTE by \n","\\begin{equation*}\n","\\hat{\\tau}_{\\text{T-learner}}(s)=\\hat\\mu_1(s)-\\hat\\mu_0(s).\n","\\end{equation*}\n","\n"],"metadata":{"id":"cMny8Ri7RvqC"},"id":"cMny8Ri7RvqC"},{"cell_type":"code","source":["mu0 = LGBMRegressor(max_depth=3)\n","mu1 = LGBMRegressor(max_depth=3)\n","\n","mu0.fit(data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2],data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] )\n","mu1.fit(data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2],data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] )\n","\n","\n","# estimate the HTE by T-learner\n","HTE_T_learner = mu1.predict(data_behavior.iloc[:,0:2]) - mu0.predict(data_behavior.iloc[:,0:2])\n"],"metadata":{"id":"X1VmlNjstdsN","executionInfo":{"status":"ok","timestamp":1675480606791,"user_tz":300,"elapsed":325,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":14,"outputs":[],"id":"X1VmlNjstdsN"},{"cell_type":"markdown","source":["Now let's take a glance at the performance of T-learner by comparing it with the true value for the first 10 subjects:"],"metadata":{"id":"CUv_0SuBTi3e"},"id":"CUv_0SuBTi3e"},{"cell_type":"code","source":["print(\"T-learner:  \",HTE_T_learner[0:8])\n","print(\"true value: \",HTE_true[0:8].to_numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5OHVneDpTgMp","executionInfo":{"status":"ok","timestamp":1675480607174,"user_tz":300,"elapsed":10,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"0f987dc7-c5ba-49eb-94c9-58e4ab346da9"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["T-learner:   [ 1.869   1.8733  0.6596  0.3087 -0.2298 -0.5598 -2.2745 -1.8211]\n","true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"]}],"id":"5OHVneDpTgMp"},{"cell_type":"markdown","source":["This is quite good! T-learner captures the overall trend of the treatment effect w.r.t. the heterogeneity of different subjects."],"metadata":{"id":"Ux89PwJagR5_"},"id":"Ux89PwJagR5_"},{"cell_type":"code","source":["Bias_T_learner = np.sum(HTE_T_learner-HTE_true)/n\n","Variance_T_learner = np.sum((HTE_T_learner-HTE_true)**2)/n\n","print(\"The overall estimation bias of T-learner is :     \", Bias_T_learner, \", \\n\", \"The overall estimation variance of T-learner is :\",Variance_T_learner,\". \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SW8ONIdFPpvM","executionInfo":{"status":"ok","timestamp":1675480608079,"user_tz":300,"elapsed":5,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"a9373651-5081-40ac-ef54-d713602f3976"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["The overall estimation bias of T-learner is :      0.29138198450323705 , \n"," The overall estimation variance of T-learner is : 1.810391408711312 . \n","\n"]}],"id":"SW8ONIdFPpvM"},{"cell_type":"markdown","source":["**Conclusion:** In this toy example, the overall estimation variance of T-learner is smaller than that of S-learner. In some cases when the treatment effect is relatively complex, it's likely to yield better performance by fitting two models separately. \n","\n","However, in an extreme case when both $\\mu_0(s)$ and $\\mu_1(s)$ are nonlinear complicated function of state $s$ while their difference is just a constant, T-learner will overfit each model very easily, yielding a nonlinear treatment effect estimator. In this case, other estimators are often preferred."],"metadata":{"id":"vOsw-rfxU415"},"id":"vOsw-rfxU415"},{"cell_type":"markdown","source":["### **3. X-learner**\n","Next, let's introduce the X-learner. As a combination of S-learner and T-learner, the X-learner can use information from the control(treatment) group to derive better estimators for the treatment(control) group, which is provably more efficient than the above two.\n","\n","The basic\n","\n","\n","**Step 1:**  Estimate $\\mu_0(s)$ and $\\mu_1(s)$ separately with any regression algorithms or supervised machine learning methods (same as T-learner);\n","\n","\n","**Step 2:**  Obtain the imputed treatment effects for individuals\n","\\begin{equation*}\n","\\tilde{\\Delta}_i^1:=R_i^1-\\hat\\mu_0(S_i^1), \\quad \\tilde{\\Delta}_i^0:=\\hat\\mu_1(S_i^0)-R_i^0.\n","\\end{equation*}\n","\n","**Step 3:**  Fit the imputed treatment effects to obtain $\\hat\\tau_1(s):=\\mathbb{E}[\\tilde{\\Delta}_i^1|S=s]$ and $\\hat\\tau_0(s):=\\mathbb{E}[\\tilde{\\Delta}_i^0|S=s]$;\n","\n","**Step 4:**  The final HTE estimator is given by\n","\\begin{equation*}\n","\\hat{\\tau}_{\\text{X-learner}}(s)=g(s)\\hat\\tau_0(s)+(1-g(s))\\hat\\tau_1(s),\n","\\end{equation*}\n","\n","where $g(s)$ is a weight function between $[0,1]$. A possible way is to use the propensity score model as an estimate of $g(s)$."],"metadata":{"id":"8lwheJQ8RxAw"},"id":"8lwheJQ8RxAw"},{"cell_type":"code","source":["# Step 1: Fit two models under treatment and control separately, same as T-learner\n","\n","import numpy as np\n","mu0 = LGBMRegressor(max_depth=3)\n","mu1 = LGBMRegressor(max_depth=3)\n","\n","S_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],0:2]\n","S_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],0:2]\n","R_T0 = data_behavior.iloc[np.where(data_behavior['A']==0)[0],3] \n","R_T1 = data_behavior.iloc[np.where(data_behavior['A']==1)[0],3] \n","\n","mu0.fit(S_T0, R_T0)\n","mu1.fit(S_T1, R_T1)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sfb-mplOP9HJ","executionInfo":{"status":"ok","timestamp":1675480609180,"user_tz":300,"elapsed":4,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"a225f7dc-6b60-4198-9db3-5cd4c73ce5e4"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMRegressor(max_depth=3)"]},"metadata":{},"execution_count":17}],"id":"sfb-mplOP9HJ"},{"cell_type":"code","source":["# Step 2: impute the potential outcomes that are unobserved in original data\n","\n","n_T0 = len(R_T0)\n","n_T1 = len(R_T1)\n","\n","Delta0 = mu1.predict(S_T0) - R_T0\n","Delta1 = R_T1 - mu0.predict(S_T1) "],"metadata":{"id":"zb42ZMw3pkqm","executionInfo":{"status":"ok","timestamp":1675480609493,"user_tz":300,"elapsed":5,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":18,"outputs":[],"id":"zb42ZMw3pkqm"},{"cell_type":"code","source":["# Step 3: Fit tau_1(s) and tau_0(s)\n","\n","tau0 = LGBMRegressor(max_depth=2)\n","tau1 = LGBMRegressor(max_depth=2)\n","\n","tau0.fit(S_T0, Delta0)\n","tau1.fit(S_T1, Delta1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pxYLjE0Ar2_5","executionInfo":{"status":"ok","timestamp":1675480610349,"user_tz":300,"elapsed":11,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"561b87a3-e53a-4293-dfd1-55ccc98cf050"},"execution_count":19,"outputs":[{"output_type":"execute_result","data":{"text/plain":["LGBMRegressor(max_depth=2)"]},"metadata":{},"execution_count":19}],"id":"pxYLjE0Ar2_5"},{"cell_type":"code","source":["# Step 4: fit the propensity score model $\\hat{g}(s)$ and obtain the final HTE estimator by taking weighted average of tau0 and tau1\n","from sklearn.linear_model import LogisticRegression \n","\n","g = LogisticRegression()\n","g.fit(data_behavior.iloc[:,0:2],data_behavior['A'])\n","\n","HTE_X_learner = g.predict_proba(data_behavior.iloc[:,0:2])[:,0]*tau0.predict(data_behavior.iloc[:,0:2]) + g.predict_proba(data_behavior.iloc[:,0:2])[:,1]*tau1.predict(data_behavior.iloc[:,0:2])\n","\n","\n"],"metadata":{"id":"LRvEZ4uluT-U","executionInfo":{"status":"ok","timestamp":1675480610350,"user_tz":300,"elapsed":6,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}}},"execution_count":20,"outputs":[],"id":"LRvEZ4uluT-U"},{"cell_type":"code","source":["print(\"X-learner:  \",HTE_X_learner[0:8])\n","print(\"true value: \",HTE_true[0:8].to_numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675480611104,"user_tz":300,"elapsed":3,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"721754de-27bb-41d5-99b1-cb8f6e758381","id":"Mz8I_J0h4N6B"},"execution_count":21,"outputs":[{"output_type":"stream","name":"stdout","text":["X-learner:   [ 1.9341  1.9235  0.2944  0.2013 -0.4147 -0.5626 -2.214  -1.5443]\n","true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"]}],"id":"Mz8I_J0h4N6B"},{"cell_type":"markdown","source":["From the result above we can see that X-learner can roughly catch the trend of treatment effect w.r.t. the change of baseline information $S$. In this synthetic example, X-learner also performs slightly better than T-learner."],"metadata":{"id":"XyxjzDBy4N6k"},"id":"XyxjzDBy4N6k"},{"cell_type":"code","source":["Bias_X_learner = np.sum(HTE_X_learner-HTE_true)/n\n","Variance_X_learner = np.sum((HTE_X_learner-HTE_true)**2)/n\n","print(\"The overall estimation bias of X-learner is :     \", Bias_X_learner, \", \\n\", \"The overall estimation variance of X-learner is :\",Variance_X_learner,\". \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675480612355,"user_tz":300,"elapsed":6,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"a432821d-9f34-4701-9d56-e404179795cd","id":"rlbacETd4N6l"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["The overall estimation bias of X-learner is :      0.2827518068171628 , \n"," The overall estimation variance of X-learner is : 1.7686646616779012 . \n","\n"]}],"id":"rlbacETd4N6l"},{"cell_type":"markdown","source":["**Conclusion:** In this toy example, the overall estimation variance of X-learner is the smallest, followed by T-learner, and the worst is given by S-learner.\n","\n"],"metadata":{"id":"EnxQ_Tkg4o_q"},"id":"EnxQ_Tkg4o_q"},{"cell_type":"markdown","source":["**Note**: For more details about the meta learners, please refer to [1]."],"metadata":{"id":"zxpRscObJmbX"},"id":"zxpRscObJmbX"},{"cell_type":"markdown","source":["## References\n","1. Kunzel, S. R., Sekhon, J. S., Bickel, P. J., and Yu, B. (2019). Metalearners for estimating heterogeneous treatment effects using machine learning. Proceedings of the national academy of sciences 116, 4156–4165.\n"],"metadata":{"id":"nyirbjS5JdGh"},"id":"nyirbjS5JdGh"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"collapsed_sections":["cMny8Ri7RvqC"]}},"nbformat":4,"nbformat_minor":5}