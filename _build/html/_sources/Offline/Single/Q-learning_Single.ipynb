{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning (Single Stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "\n",
    "Q-learning is a classic method of Reinforcement Learning. Early in 2000, it was adapted to decision-making problems[1] and kept evolving with various extensions, such as penalized Q-learning [2]. In the following, we would start from a simple case having only one decision point and then introduce the multistage case with multiple decision points. Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., A,B,C,D), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Decision Point\n",
    "\n",
    "- **Application Situation**: Suppose we have a dataset containing observations from $N$ individuals. For each individual $i$, we have $\\{\\mathbf{X}_{i},A_{i},Y_{i}\\}$, $i=1,\\cdots,N$. $\\mathbf{X}_{i}$ includes the feature information, $A_{i}$ is the action taken, and $Y_{i}$ is the observed reward received. The target of Q-learning is to find an optimal policy $\\pi$ that can maximize the expected reward received. In other words, by training a model with the observed dataset, we want to find an optimal policy that can help us determine the optimal action for each individual to optimize the reward.\n",
    "\n",
    "- **Basic Logic**: Q-learning with a single decision point is mainly a regression modeling problem, as the major component is to find the relationship between $Y$ and $\\{X,A\\}$. Let's first define a Q-function, such that\n",
    "\\begin{align}\n",
    "    Q(x,a) = E(Y|X=x, A=a).\n",
    "\\end{align} Then, to find the optimal policy is equivalent to solve\n",
    "\\begin{align}\n",
    "    \\text{arg max}_{\\pi}Q(x_{i},\\pi(x_{i})).\n",
    "\\end{align} \n",
    "\n",
    "- **Key Steps**:\n",
    "    1. Fitted a model $\\hat{Q}(x,a,\\hat{\\beta})$, which can be solved directly by existing approaches (i.e., OLS, .etc),\n",
    "    2. For each individual find the optimal action $a_{i}$ such that $a_{i} = \\text{arg max}_{a}\\hat{Q}(x_{i},a,\\hat{\\beta})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Optimal Decision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import QLearning\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklift.datasets import fetch_hillstrom"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### prepare the dataset (dataset from the DTR book)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['recency', 'history', 'mens', 'womens', 'newbie', 'zip_code_Surburban',\n",
       "       'zip_code_Urban', 'channel_Phone', 'channel_Web'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# continuous Y\n",
    "data, target, treatment = fetch_hillstrom(target_col='spend', return_X_y_t=True)\n",
    "# use pd.concat to join the new columns with your original dataframe\n",
    "data = pd.concat([data,pd.get_dummies(data['zip_code'], prefix='zip_code')],axis=1)\n",
    "data = pd.concat([data,pd.get_dummies(data['channel'], prefix='channel')],axis=1)\n",
    "# now drop the original 'country' column (you don't need it anymore)\n",
    "data.drop(['zip_code'],axis=1, inplace=True)\n",
    "data.drop(['channel'],axis=1, inplace=True)\n",
    "data.drop(['history_segment'],axis=1, inplace=True)\n",
    "data.drop(['zip_code_Rural'],axis=1, inplace=True) # Rural as the reference group\n",
    "data.drop(['channel_Multichannel'],axis=1, inplace=True) # Multichannel as the reference group \n",
    "\n",
    "Y = target\n",
    "Y.name = 'Y'\n",
    "X = data# add an intercept column\n",
    "A = treatment\n",
    "A.name = 'A'\n",
    "#get the subset which has Y>0 == n=578\n",
    "X = X[Y>0]\n",
    "A = A[Y>0]\n",
    "Y = Y[Y>0]\n",
    "X.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recency</th>\n",
       "      <th>history</th>\n",
       "      <th>mens</th>\n",
       "      <th>womens</th>\n",
       "      <th>newbie</th>\n",
       "      <th>zip_code_Surburban</th>\n",
       "      <th>zip_code_Urban</th>\n",
       "      <th>channel_Phone</th>\n",
       "      <th>channel_Web</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>10</td>\n",
       "      <td>88.37</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>4</td>\n",
       "      <td>297.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>10</td>\n",
       "      <td>29.99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>451</th>\n",
       "      <td>6</td>\n",
       "      <td>265.61</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>459</th>\n",
       "      <td>1</td>\n",
       "      <td>101.99</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63466</th>\n",
       "      <td>9</td>\n",
       "      <td>536.80</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63552</th>\n",
       "      <td>2</td>\n",
       "      <td>980.39</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63743</th>\n",
       "      <td>5</td>\n",
       "      <td>210.12</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63876</th>\n",
       "      <td>2</td>\n",
       "      <td>215.61</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63883</th>\n",
       "      <td>1</td>\n",
       "      <td>239.70</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>578 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       recency  history  mens  womens  newbie  zip_code_Surburban  \\\n",
       "217         10    88.37     0       1       0                   0   \n",
       "267          4   297.80     1       1       0                   0   \n",
       "332         10    29.99     0       1       0                   1   \n",
       "451          6   265.61     0       1       1                   1   \n",
       "459          1   101.99     0       1       0                   1   \n",
       "...        ...      ...   ...     ...     ...                 ...   \n",
       "63466        9   536.80     1       1       1                   0   \n",
       "63552        2   980.39     1       0       1                   0   \n",
       "63743        5   210.12     0       1       0                   1   \n",
       "63876        2   215.61     1       0       0                   0   \n",
       "63883        1   239.70     1       1       1                   0   \n",
       "\n",
       "       zip_code_Urban  channel_Phone  channel_Web  \n",
       "217                 1              1            0  \n",
       "267                 1              1            0  \n",
       "332                 0              0            1  \n",
       "451                 0              0            0  \n",
       "459                 0              0            1  \n",
       "...               ...            ...          ...  \n",
       "63466               0              0            1  \n",
       "63552               1              1            0  \n",
       "63743               0              1            0  \n",
       "63876               0              0            1  \n",
       "63883               1              0            1  \n",
       "\n",
       "[578 rows x 9 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Intercept                                   86.340116\n",
       "C(A)[T.No E-Mail]                          -72.636541\n",
       "C(A)[T.Womens E-Mail]                       80.298265\n",
       "recency                                     -0.535940\n",
       "C(A)[T.No E-Mail]:recency                    3.667566\n",
       "C(A)[T.Womens E-Mail]:recency                0.938664\n",
       "history                                     -0.005399\n",
       "C(A)[T.No E-Mail]:history                   -0.006813\n",
       "C(A)[T.Womens E-Mail]:history                0.027592\n",
       "mens                                        25.143276\n",
       "C(A)[T.No E-Mail]:mens                      33.137982\n",
       "C(A)[T.Womens E-Mail]:mens                 -56.033031\n",
       "womens                                       6.923481\n",
       "C(A)[T.No E-Mail]:womens                    21.021383\n",
       "C(A)[T.Womens E-Mail]:womens               -85.180182\n",
       "newbie                                       9.657639\n",
       "C(A)[T.No E-Mail]:newbie                   -33.554932\n",
       "C(A)[T.Womens E-Mail]:newbie               -10.708279\n",
       "zip_code_Surburban                           4.722924\n",
       "C(A)[T.No E-Mail]:zip_code_Surburban        48.520623\n",
       "C(A)[T.Womens E-Mail]:zip_code_Surburban     7.496915\n",
       "zip_code_Urban                              -7.171190\n",
       "C(A)[T.No E-Mail]:zip_code_Urban            32.114015\n",
       "C(A)[T.Womens E-Mail]:zip_code_Urban        41.838176\n",
       "channel_Phone                                6.212860\n",
       "C(A)[T.No E-Mail]:channel_Phone             12.531739\n",
       "C(A)[T.Womens E-Mail]:channel_Phone         -8.744767\n",
       "channel_Web                                 13.713458\n",
       "C(A)[T.No E-Mail]:channel_Web                7.319584\n",
       "C(A)[T.Womens E-Mail]:channel_Web          -20.474710\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize the learner\n",
    "QLearn = QLearning.QLearning()\n",
    "# specify the model you would like to use\n",
    "# If want to include all the variable in X and A with no specific model structure, then use \"Y~.\"\n",
    "# Otherwise, specify the model structure by hand\n",
    "# Note: if the action space is not binary, use C(A) in the model instead of A\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history+mens+womens+newbie+zip_code_Surburban+zip_code_Urban+channel_Phone+channel_Web)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':['Womens E-Mail', 'No E-Mail', 'Mens E-Mail']}}]\n",
    "# train the policy\n",
    "QLearn.train(X, A, Y, model_info, T=1)\n",
    "# Fitted Model\n",
    "QLearn.fitted_model[0].params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### get the optimal regime and the optimal value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "opt regime:                  A\n",
      "217  Womens E-Mail\n",
      "267      No E-Mail\n",
      "332      No E-Mail\n",
      "451  Womens E-Mail\n",
      "459      No E-Mail\n",
      "opt value: 140.06857615909965\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = QLearn.recommend().head()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = QLearn.estimate_value()\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 148.59510088839244 Value_std: 8.035500416430729\n"
     ]
    }
   ],
   "source": [
    "# Optional: we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history+mens+womens+newbie+zip_code_Surburban+zip_code_Urban+channel_Phone+channel_Web)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':['Womens E-Mail', 'No E-Mail', 'Mens E-Mail']}}]\n",
    "QLearn.train(X, A, Y, model_info, T=1, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.estimate_value_boots()\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a given policy, we utilze the boostrap resampling to get the estimated value of the regime and the corresponding estimated standard error. Basically, for each round of bootstrap, we resample a dataset of the same size as the original dataset with replacement, fitted the Q function based on the sampled dataset, and estimated the value of a given regime using the estimated Q function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#specify the fixed regime to be tested\n",
    "# For example, regime d = 'No E-Mail' for all subjects\n",
    "N=len(X)\n",
    "# !! IMPORTANTï¼š index shold be the same as that of the X\n",
    "regime = pd.DataFrame({'A':['No E-Mail']*N}).set_index(X.index)\n",
    "#evaluate the regime\n",
    "QLearn = QLearning.QLearning()\n",
    "model_info = [{\"model\": \"Y~C(A)*(recency+history+mens+womens+newbie+zip_code_Surburban+zip_code_Urban+channel_Phone+channel_Web)\", #default is add an intercept!!!\n",
    "              'action_space':{'A':['Womens E-Mail', 'No E-Mail', 'Mens E-Mail']}}]\n",
    "QLearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=QLearn.estimate_value_boots()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 112.21261479003178 Value_std: 9.989950778047078\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "113.1586653330154"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Otional: just estimate the value\n",
    "QLearn.train(X, A, Y, model_info, T=1, regime = regime, evaluate = True)\n",
    "QLearn.estimate_value()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO: \n",
    "    1. estimate the standard error for the binary case with sandwich formula;\n",
    "    2. inference for the estimated optimal regime: projected confidence interval? m-out-of-n CI?...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Murphy, S. A. (2005). A generalization error for Q-learning.\n",
    "2. Song, R., Wang, W., Zeng, D., & Kosorok, M. R. (2015). Penalized q-learning for dynamic treatment regimens. Statistica Sinica, 25(3), 901."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## !!Functions are already tested with the data and results provided in the DTR book"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
