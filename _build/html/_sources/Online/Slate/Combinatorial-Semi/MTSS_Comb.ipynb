{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dd4f7e27",
   "metadata": {},
   "source": [
    "# MTSS_Comb\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is both scalable and robust. Furthermore, it also accounts for the iter-item heterogeneity.\n",
    "- **Disadvantage**:\n",
    "- **Application Situation**: Useful when presenting a list of items, each of which will generate a partial outcome (reward). The outcome is continuous. Static feature information.\n",
    "\n",
    "## Main Idea\n",
    "MTSS_Comb is an example of the general Thompson Sampling(TS)-based framework, MTSS [1], to deal with online combinatorial optimization problems.\n",
    "\n",
    "**Review of MTSS:** MTSS[1] is a meta-learning framework designed for large-scale structured bandit problems [2]. Mainly, it is a TS-based algorithm that learns the information-sharing structure while minimizing the cumulative regrets. Adapting the TS framework to a problem-specific Bayesian hierarchical model, MTSS simultaneously enables information sharing among items via their features and models the inter-item heterogeneity. Specifically, it assumes that the item-specific parameter $\\theta_i = E[Y_{t}(i)]$ is sampled from a distribution $g(\\theta_i|\\boldsymbol{s}_i, \\boldsymbol{\\gamma})$ instead of being entirely determined by $\\boldsymbol{s}_i$ via a deterministic function. Here, $g$ is a model parameterized by an **unknown** vector $\\boldsymbol{\\gamma}$. The following is the general feature-based hierarchical model MTSS considered. \n",
    "\\begin{equation}\\label{eqn:general_hierachical}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(Prior)} \\quad\n",
    "\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\n",
    "\\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}),\\\\\n",
    "&\\text{(Generalization function)} \\;\n",
    "\\;    \\theta_i| \\boldsymbol{s}_i, \\boldsymbol{\\gamma}  &&\\sim g(\\theta_i|\\boldsymbol{s}_i, \\boldsymbol{\\gamma}), \\forall i \\in [N],\\\\ \n",
    "&\\text{(Observations)} \\quad\\quad\\quad\\quad\\quad\\quad\\;\n",
    "\\;    \\boldsymbol{Y}_t(a) &&\\sim f(\\boldsymbol{Y}_t(a)|\\boldsymbol{\\theta}),\\\\\n",
    "&\\text{(Reward)} \\quad\\quad\\quad\\quad\\quad\\quad\\quad\\quad\\;\n",
    "\\;   R_t(a) &&= f_r(\\boldsymbol{Y}_t(a) ; \\boldsymbol{\\eta}), \n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "where $Q(\\boldsymbol{\\gamma})$ is the prior distribution for $\\boldsymbol{\\gamma}$. \n",
    "Overall, MTTS is a **general** framework that subsumes a wide class of practical problems, **scalable** to large systems, and **robust** to the specification of the generalization model.\n",
    "\n",
    "**Review of MTSS_Comb:** In this tutorial, as an example, we focus on the combinatorial semi-bandits with Gaussian outcome, $Y_{i, t}$, and consider using a linear mixed model (LMM) as the generalization model to share information. Specifically, the full model is as follows: \n",
    "\\begin{equation}\\label{eqn:LMM}\n",
    "    \\begin{split}\n",
    "     \\theta_i &\\sim \\mathcal{N}(\\boldsymbol{s}_i^T \\boldsymbol{\\gamma}, \\sigma_1^2), \\forall i \\in [N],\\\\\n",
    "    Y_{i, t}(a) &\\sim \\mathcal{N}(\\theta_i, \\sigma_2^2), \\forall i \\in a,\\\\\n",
    "    R_t(a) &= \\sum_{i \\in a} Y_{i,t}(a), \n",
    "    \\end{split}\n",
    "\\end{equation}\n",
    "where it is typically assumed that $\\sigma_1$ and $\\sigma_2$ are known. We choose the prior $\\boldsymbol{\\gamma} \\sim \\mathcal{N}(\\boldsymbol{\\mu}_{\\boldsymbol{\\gamma}}, {\\boldsymbol{\\Sigma}}_{\\boldsymbol{\\gamma}})$ with parameters as known. It is worth noting that many other outcome distributions (e.g., Bernoulli) and model assumptions (e.g., Gaussian process) can be formulated similarly, depending on the applications. Users can directly modify the code for posterior updating to adapt different model assumptions. Further, for simplicity, we consider the most basic size constraint such that the action space includes all the possible subsets with size $K$. Therefore, the optimization process to find the optimal subset $A_{t}$ is equal to selecting a list of $K$ items with the highest attractiveness factors. Of course, users are welcome to modify the **optimization** function to satisfy more complex constraints.\n",
    "\n",
    "## Algorithm Details\n",
    "Under the assumption of a LMM, the posteriors can be derived explicitly, following the Bayes' theorem. At each round $t$, given the feedback $\\mathcal{H}_{t}$ received from previous rounds, there are two major steps including posterior sampling and combinatorial optimization. Specifically, the posterior sampling step is decomposed into four steps: 1. updating the posterior distribution of $\\boldsymbol{\\gamma}$, $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$; 2. sampling a $\\tilde{\\boldsymbol{\\gamma}}$ from $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$; 3. updating the posterior distribution of $\\boldsymbol{\\theta}$ conditional on $\\tilde{\\boldsymbol{\\gamma}}$, $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$; 4. sampling $\\tilde{\\boldsymbol{\\theta}}$ from $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$. Then, the action $A_{t}$ is selected greedily as $A_t = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid \\tilde{\\boldsymbol{\\theta}})$. Considering the simple size constraint, $A_{t}$ is the list of $K$ items with the highest $\\tilde{\\theta}_{i}$. Note that $\\tilde{\\boldsymbol{\\gamma}}$ can be sampled in a batch mode to further facilitate computationally efficient online deployment.\n",
    "\n",
    "## Key Steps\n",
    "For round $t = 1,2,\\cdots$:\n",
    "1. Update $P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H}_{t})$;\n",
    "3. Update $P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$;\n",
    "4. Sample $\\tilde{\\boldsymbol{\\theta}} \\sim P(\\boldsymbol{\\theta}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H}_{t})$;\n",
    "5. Take the action $A_{t}$ w.r.t $\\tilde{\\boldsymbol{\\theta}}$ such that $A_t = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid \\tilde{\\boldsymbol{\\theta}})$;\n",
    "6. Receive reward $R_{t}$.\n",
    "\n",
    "*Notations can be found in either the inroduction of the chapter \"Structured Bandits\" or the introduction of the combinatorial Semi-Bandit problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f94a940",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f8db32a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29527f70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Slate.Combinatorial_Semi import MTSS_Comb\n",
    "from causaldm.learners.Online.Slate.Combinatorial_Semi import _env_SemiBandit\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "06795008",
   "metadata": {},
   "outputs": [],
   "source": [
    "L, T, K, p = 300, 1000, 5, 3\n",
    "mu_gamma = np.zeros(p)\n",
    "sigma_gamma = np.identity(p)\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "with_intercept = True\n",
    "seed = 0\n",
    "sigma_1 = .5\n",
    "sigma_2 = 1\n",
    "\n",
    "env = _env_SemiBandit.Semi_env(L, K, T, p, sigma_1, sigma_2\n",
    "                               , mu_gamma, sigma_gamma, seed = seed\n",
    "                               , with_intercept = with_intercept\n",
    "                               , X_mu = X_mu, X_sigma = X_sigma)\n",
    "MTSS_agent = MTSS_Comb.MTSS_Semi(sigma_2 = 1, L=L, T = T\n",
    "                                 , gamma_prior_mean = np.zeros(p), gamma_prior_cov = np.identity(p)\n",
    "                                 , sigma_1 = sigma_1\n",
    "                                 , K = K\n",
    "                                 , Xs = env.Phi# [L, p]\n",
    "                                 , update_freq = 1)\n",
    "S = MTSS_agent.take_action(env.Phi)\n",
    "t = 1\n",
    "obs_R, exp_R, R = env.get_reward(S, t)\n",
    "MTSS_agent.receive_reward(t, S, obs_R, X = env.Phi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c6b8ae",
   "metadata": {},
   "source": [
    "**Interpretation:** A sentence to include the analysis result: the estimated optimal regime is..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0fa6ea",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Wan, R., Ge, L., & Song, R. (2022). Towards Scalable and Robust Structured Bandits: A Meta-Learning Framework. arXiv preprint arXiv:2202.13227.\n",
    "[2] Van Parys, B., & Golrezaei, N. (2020). Optimal learning for structured bandits. Available at SSRN 3651397."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
