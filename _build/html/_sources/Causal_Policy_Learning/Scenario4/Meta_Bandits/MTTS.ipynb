{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f17c8f1-4560-4d7a-90b3-86a9d9cc810d",
   "metadata": {},
   "source": [
    "# Multi-Task Thompson Sampling (MTTS)\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: It is both scalable and robust. Furthermore, it also accounts for the iter-task heterogeneity.\n",
    "- **Disadvantage**:\n",
    "- **Application Situation**: Useful when there are a large number of tasks to learn, especially when new tasks are introduced on a regular basis. The outcome can be either binary or continuous. Static baseline information.\n",
    "\n",
    "## Main Idea\n",
    "The **MTTS**[1] utilize baseline information to share information among different tasks efficiently, by constructing a Bayesian hierarchical model. Specifically, it assumes that\n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Inter-task)} \\quad\n",
    "\\;    \\boldsymbol{\\mu}_j | \\boldsymbol{s}_j, \\boldsymbol{\\gamma} &&\\sim g(\\boldsymbol{\\mu}_j | \\boldsymbol{s}_j, \\boldsymbol{\\gamma})=\\boldsymbol{s}_j^{T}\\boldsymbol{\\gamma} + \\boldsymbol{\\delta}_{j}, \\\\\n",
    "&\\text{(Intra-task)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&= \\mu_{j,a} + \\epsilon_{j,t}, \n",
    "      \\end{alignedat}\n",
    "\\end{equation} where $\\boldsymbol{\\delta}_{j} \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$, and $\\epsilon_{j,t} \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\boldsymbol{0}, \\sigma^{2})$. For simplicity, we assume a Normal prior, which resulted in a Normal posterior with explicit form. Note that, if we replace the inter-task layer to a deterministic model (i.e., $g(\\boldsymbol{\\mu}_j | \\boldsymbol{x}_j, \\boldsymbol{\\gamma})=\\boldsymbol{s}_j^{T}\\boldsymbol{\\gamma}$), **MTTS** is reduced to an algorithm similar to **AdaTS** with linear bandits and Gaussian rewards discussed in Section 3.2 [2]. In contrast to **MTSS**, the **AdaTS** fail to address the issue of heterogeneous tasks.\n",
    "\n",
    "Similarly, considering the Bernoulli bandit, it assumes that\n",
    "\\begin{equation}\\label{eqn:hierachical_model}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Inter-task)} \\quad\n",
    "\\;    \\boldsymbol{\\mu}_j | \\boldsymbol{x}_j, \\boldsymbol{\\gamma} &&\\sim g(\\boldsymbol{\\mu}_j | \\boldsymbol{x}_j, \\boldsymbol{\\gamma})=\\text{Beta}\\big(logistic(\\boldsymbol{x}_j^T \\boldsymbol{\\gamma}), \\psi \\big), \\\\\n",
    "&\\text{(Intra-task)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&\\sim  \\text{Bernoulli} \\big( \\mu_{j, a} \\big), \n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "where  $logistic(s) \\equiv 1 / (1 + exp^{-1}(s))$, $\\psi$ is a known parameter, and  $\\text{Beta}(\\mu, \\psi)$ denotes a Beta distribution with mean $\\mu$ and precision $\\psi$. Still, we assume a Normal prior of $\\boldsymbol{\\gamma}$. As there is no explicit form of the corresponding posterior, we update the posterior distribution by **Pymc3**.\n",
    "\n",
    "Under the TS framework, at each round $t$ with task $j$, the agent will sample a $\\tilde{\\boldsymbol{\\mu}}_{j}$ from its posterior distribution updated according to the hierarchical model, then the action $a$ with the maximum sampled $\\tilde{\\mu}_{j,a}$ will be pulled. Mathmetically,\n",
    "\\begin{equation}\n",
    "    A_{j,t} = argmax_{a \\in \\mathcal{A}} \\hat{E}(R_{j,t}(a)) = argmax_{a \\in \\mathcal{A}} \\tilde\\mu_{j,a}.\n",
    "\\end{equation}\n",
    "\n",
    "Essentially, **MTTS** assumes that the mean reward $\\boldsymbol{\\mu}_{j}$ is sampled from model $g$ parameterized by unknown parameter $\\boldsymbol{\\gamma}$ and conditional on task feature $\\boldsymbol{s}_{j}$. Instead of assuming that $\\boldsymbol{\\mu}_j$ is fully determined by its features through a deterministic function, **MTTS** adds an item-specific noise term to account for the inter-task heterogeneity. Simultaneously modeling heterogeneity and sharing information across tasks via $g$, **MTTS** is able to provide an informative prior distribution to guide the exploration. Appropriately addressing the heterogeneity between tasks, the MTTS has been shown to have a superior performance in practice[1].\n",
    "\n",
    "## Key Steps\n",
    "For $(j,t) = (0,0),(0,1),\\cdots$:\n",
    "1. Approximate meta-posterior $P(\\boldsymbol{\\gamma}|\\mathcal{H})$ either by implementing **Pymc3** or by calculating the explicit form of the posterior distribution;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H})$;\n",
    "3. Update $P(\\boldsymbol{\\mu}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H})$ and sample $\\tilde{\\boldsymbol{\\mu}} \\sim P(\\boldsymbol{\\mu}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H})$;\n",
    "4. Take the action $A_{j,t}$ such that $A_{j,t} = argmax_{a \\in \\mathcal{A}} \\tilde\\mu_{j,a}$;\n",
    "6. Receive reward $R_{j,t}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d0ce7-53c1-4cf5-b467-645fd69b0c5b",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a19e2739-6af0-4cda-966d-b87a8331cce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\GitHub\\CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac5d997-5cd8-43d6-8d1f-4fcbd9a2d9a8",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "befb3223-e2a5-46b4-88c3-5e64fb1436b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.Online.Meta_Bandits import MTTS_Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf40ee8-2a24-4b7b-9a7c-19139d44ebd5",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the MovieLens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b50ce9d-1fa7-4986-a95d-43e4bd8034bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Meta_Bandits import _env_realMultiTask as _env\n",
    "env = _env.MultiTask_Env(seed = 0, Binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2625ce7b-5949-4b75-9fb6-02c476718b4f",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "\n",
    "- `sigma`: the standard deviation of the reward distributions\n",
    "- `order`: = 'episodic', if a sequential schedule (i.e., a new task will not be interacted with until the preceding task is completed) is applied; ='concurrent', if a concurrent schedule (i.e., at every step $t$, the agent will interact with all $N$ tasks) is applied\n",
    "- `T`: number of total steps per task\n",
    "- `theta_prior_mean`: mean of the meta prior distribution\n",
    "- `theta_prior_cov`: covariance matrix of the meta prior distribution\n",
    "- `delta_cov`: covariance of $\\delta_j$\n",
    "- `Xs`: Baseline information for all Tasks, (N,K,p) matrix\n",
    "- `approximate_solution`: if `True`, we implement the Algorithm 2 in [1]. Specifically, if order = 'episodic', the meta-posterior distribution is updated once a task is finished; if order = 'concurrent, the meta-posterior distribution is updated once the agent finishes interacting with all tasks at each step $t$.\n",
    "- `update_freq`: if `approximate_solution = False`, then the meta-posterior is updated every `update_freq` steps\n",
    "- `seed`: random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c870db5a-6195-40a8-8417-f655bddee3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma = 1\n",
    "order=\"episodic\"\n",
    "T = 100\n",
    "theta_prior_mean = np.zeros(env.p)\n",
    "theta_prior_cov = np.identity(env.p)\n",
    "delta_cov = np.identity(env.K)\n",
    "Xs = env.Phi\n",
    "update_freq = 1\n",
    "approximate_solution = False\n",
    "seed = 42\n",
    "\n",
    "MTTS_Gaussian_agent = MTTS_Gaussian.MTTS_agent(sigma = sigma, order=order,T = T, theta_prior_mean = theta_prior_mean, theta_prior_cov = theta_prior_cov,\n",
    "                                               delta_cov = delta_cov, Xs = Xs, update_freq = update_freq, approximate_solution = approximate_solution, \n",
    "                                               seed = seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43b93c67-46b7-4c2c-9079-9f69c636df1e",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "\n",
    "Starting from i = 0, t = 0, for each (i,t), there are four steps:\n",
    "1. Observe the feature information\n",
    "<code> X, feature_info = env.get_Phi(i, t) </code>\n",
    "2. Recommend an action \n",
    "<code> A = MTTS_Gaussian_agent.take_action(i, t) </code>\n",
    "3. Get a reward from the environment \n",
    "<code> R = env.get_reward(i, t, A) </code>\n",
    "4. Update the posterior distribution of the mean reward of each arm\n",
    "<code> MTTS_Gaussian_agent.receive_reward(i, t, A, R, X) </code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9424bfe2-aad8-4c45-86fd-e075b7bfc083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1, 2.0, (25.0, 1.0, 'college/grad student'))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "t = 0\n",
    "X, feature_info = env.get_Phi(i, t)\n",
    "A = MTTS_Gaussian_agent.take_action(i, t)\n",
    "R = env.get_reward(i, t, A)\n",
    "MTTS_Gaussian_agent.receive_reward(i, t, A, R, X)\n",
    "i,t,A,R,feature_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d269a354-3cb2-4afd-91fc-b7a74bc44acc",
   "metadata": {},
   "source": [
    "**Interpretation**: Interacting with the first customer (25-year-old male who is a college/grad student), at step 0, the TS agent recommends a Thriller (arm 3) and receives a rate of 3 from the user.\n",
    "\n",
    "**Remark**: use `MTTS_Gaussian_agent.posterior_u[i]` to get the most up-to-date posterior mean of $\\mu_{i,a}$ and `MTTS_Gaussian_agent.posterior_cov_diag[0]` to get the corresponding posterior covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06078899-c3bd-49e6-bcae-ceb95a7e5e46",
   "metadata": {},
   "source": [
    "### Demo Code for Bernoulli Bandit\n",
    "The steps are similar to those previously performed with a Gaussian Bandit. Note that, when specifying the prior distribution of the expected reward, the mean-precision form of the Beta distribution is used here, i.e., Beta($\\mu$, $\\phi$), where $\\mu$ is the mean reward of each arm and $\\phi$ is the precision of the Beta distribution. By default, Algorithm 2 in [1] is applied to save computational time updating meta-posteriors. If `update_freq` is specified, then the meta-posterior will be updated every `update_freq` rounds of interactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "32bac736-87a7-4de5-b013-134f5c3489f4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 0, 1, (25.0, 1.0, 'college/grad student'))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from causaldm.learners.Online.Meta_Bandits import MTTS_Binary\n",
    "env = _env.MultiTask_Env(seed = 0, Binary = True)\n",
    "theta_prior_mean = np.zeros(env.p)\n",
    "theta_prior_cov = np.identity(env.p)\n",
    "phi_beta = 1/4\n",
    "order=\"episodic\"\n",
    "T = 100\n",
    "Xs = env.Phi\n",
    "update_freq = None\n",
    "seed = 42\n",
    "\n",
    "MTTS_Binary_agent = MTTS_Binary.MTTS_agent(T = T, theta_prior_mean = theta_prior_mean, theta_prior_cov = theta_prior_cov,\n",
    "                                           phi_beta = phi_beta, order = order, Xs = Xs, update_freq = update_freq, \n",
    "                                           seed = seed)\n",
    "i = 0\n",
    "t = 0\n",
    "X, feature_info = env.get_Phi(i, t)\n",
    "A = MTTS_Binary_agent.take_action(i, t)\n",
    "R = env.get_reward(i, t, A)\n",
    "MTTS_Binary_agent.receive_reward(i, t, A, R, X)\n",
    "i,t,A,R,feature_info"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281edbf7-618f-41e1-a76e-fa6a2618af23",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Interpretation**: Interacting with the first customer (25-year-old male who is a college/grad student), at step 0, the TS agent recommends a Comedy (arm 0) and receives a rate of 1 from the user.\n",
    "\n",
    "**Remark**: use `MTTS_Binary_agent.theta` to get the most up-to-date sampled $\\tilde{\\boldsymbol{\\gamma}}$; use `(MTTS_Binary_agent.posterior_alpha[i], MTTS_Binary_agent.posterior_beta[i])` to get the most up-to-date posterior Beta($\\alpha$,$\\beta$) distribution of $\\mu_{i,a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3f3eee1-f1f8-4cea-8759-3737e5397c97",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Wan, R., Ge, L., & Song, R. (2021). Metadata-based multi-task bandits with bayesian hierarchical models. Advances in Neural Information Processing Systems, 34, 29655-29668.\n",
    "\n",
    "[2] Basu, S., Kveton, B., Zaheer, M., & Szepesv√°ri, C. (2021). No regrets for learning the prior in bandits. Advances in Neural Information Processing Systems, 34, 28029-28041.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
