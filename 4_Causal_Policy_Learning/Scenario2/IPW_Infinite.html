

<!DOCTYPE html>


<html lang="en" data-content_root="" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Importance Sampling for Policy Evaluation (Infinite Horizon) &#8212; Causal Decision Making</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../../_static/styles/theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/bootstrap.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
<link href="../../_static/styles/pydata-sphinx-theme.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />

  
  <link href="../../_static/vendor/fontawesome/6.5.2/css/all.min.css?digest=dfe6caa3a7d634c4db9b" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../../_static/vendor/fontawesome/6.5.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../../_static/pygments.css" />
    <link rel="stylesheet" href="../../_static/styles/sphinx-book-theme.css?digest=14f4ca6b54d191a8c7657f6c759bf11a5fb86285" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/design-style.4045f2051d55cab465a707391d5b2007.min.css" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b" />
<link rel="preload" as="script" href="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b" />
  <script src="../../_static/vendor/fontawesome/6.5.2/js/all.min.js?digest=dfe6caa3a7d634c4db9b"></script>

    <script data-url_root="../../" id="documentation_options" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/_sphinx_javascript_frameworks_compat.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script src="../../_static/scripts/sphinx-book-theme.js?digest=5a5c038af52cf7bc1a1ec88eea08e6366ee68824"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../../_static/design-tabs.js"></script>
    <script async="async" src="https://www.googletagmanager.com/gtag/js?id=G-M7XK5P5ZHC"></script>
    <script>
                window.dataLayer = window.dataLayer || [];
                function gtag(){ dataLayer.push(arguments); }
                gtag('js', new Date());
                gtag('config', 'G-M7XK5P5ZHC');
            </script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '4_Causal_Policy_Learning/Scenario2/IPW_Infinite';</script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)" href="DR_Infinite.html" />
    <link rel="prev" title="Fitted-Q Evaluation" href="FQE.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-primary-sidebar-checkbox"/>
  <label class="overlay overlay-primary" for="pst-primary-sidebar-checkbox"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          id="pst-secondary-sidebar-checkbox"/>
  <label class="overlay overlay-secondary" for="pst-secondary-sidebar-checkbox"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  
    
  

<a class="navbar-brand logo" href="../../Intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../../_static/logo.png" class="logo__image only-light" alt="Causal Decision Making - Home"/>
    <script>document.write(`<img src="../../_static/logo.png" class="logo__image only-dark" alt="Causal Decision Making - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item"><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../../Intro.html">
                    Introduction
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Overview</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../Overview.html">Overview</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Motivating Examples</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../0_Motivating_Examples/CSL.html">Causal Structure Learning (CSL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../0_Motivating_Examples/CEL.html">Causal Effect Learning (CEL)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../0_Motivating_Examples/CPL.html">Causal Policy Learning (CPL)</a></li>

</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Preliminary</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../1_Preliminary/Causal%20Inference%20Preliminary.html">Causal Inference Preliminary</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Structure Learning (CSL)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../../2_Causal_Structure_Learning/Preliminaries%20of%20Causal%20Graphs.html">Preliminaries of Causal Graphs</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../2_Causal_Structure_Learning/Causal%20Discovery.html">Causal Discovery</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../2_Causal_Structure_Learning/Testing-based%20Learner.html">Testing-based Learner</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../2_Causal_Structure_Learning/Functional-based%20Learner.html">Functional-based Learner</a></li>
<li class="toctree-l2"><a class="reference internal" href="../../2_Causal_Structure_Learning/Score-based%20Learner.html">Score-based Learner</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../2_Causal_Structure_Learning/Causal%20Mediation%20Analysis.html">Causal Mediation Analysis</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Effect Learning (CEL)</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Single%20Stage.html"><strong>Single Stage – Paradigm 1</strong></a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/ATE.html">ATE Estimation</a></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/HTE.html">HTE Estimation</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/S-learner.html"><strong>1. S-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/T-learner.html"><strong>2. T-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/X-learner.html"><strong>3. X-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/R-Learner.html"><strong>4. R learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/DR-Learner.html"><strong>5. DR-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Lp-R-Learner.html"><strong>6. Lp-R-learner</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/GRF.html"><strong>7. Generalized Random Forest</strong></a></li>


<li class="toctree-l3"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Dragonnet.html"><strong>8. Dragon Net</strong></a></li>


</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l2"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%201/Mediation%20Analysis.html">Mediation Analysis</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%202/underMDP.html">Markov Decision Processes – Paradigm 2</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Panel%20Data.html">Panel Data  – Paradigm 3</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/DiD.html"><strong>Difference in Difference</strong></a></li>

<li class="toctree-l2"><a class="reference internal" href="../../3_Causal_Effect_Learning/Scenario%203/Synthetic%20Control.html"><strong>Synthetic Control</strong></a></li>




</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 1</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Scenario1/Single%20Stage.html">Single Stage</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario1/Discrete.html">Discrete Action Space</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Scenario1/Q-learning_Single.html">Q-Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario1/A-learning_Single.html">A-Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario1/Classification/O-Learning.html">Outcome Weighted Learning</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario1/Quantile/QuantileOTR_test.html">Quantile Optimal Treatment Regime</a></li>

</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario1/Continuous.html">Continuous Action Space</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Scenario1/Continuous/Deep%20Jump%20Learner.html">Deep Jump Learner for Continuous Actions</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 2</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preliminary_MDP-potential-outcome.html">Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes</a></li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="Evaluation.html">Policy Evaluation–Value Estimation</a><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="FQE.html">Fitted-Q Evaluation</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Importance Sampling for Policy Evaluation (Infinite Horizon)</a></li>
<li class="toctree-l2"><a class="reference internal" href="DR_Infinite.html">Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)</a></li>
<li class="toctree-l2"><a class="reference internal" href="Deeply_Debiased.html">Deeply-Debiased Off-Policy Evaluation</a></li>
</ul><details open="open"><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="MediationRL.html">Policy Evaluation--Mediation Analysis</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="Optimization.html">Policy Optimization</a><ul>
<li class="toctree-l2"><a class="reference internal" href="FQI.html">Fitted-Q Iteration</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 3</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Scenario3/Multi%20Stage.html">Multiple Stages (DTR)</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Scenario3/Q-learning_Multiple.html">Q-Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="../Scenario3/A-learning_Multiple.html">A-Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 4</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Scenario4/Bandits.html">Overview: Bandits ALgorithm</a></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario4/MAB/MAB.html">Multi-Armed Bandits (MAB)</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/MAB/Epsilon_Greedy.html"><span class="math notranslate nohighlight">\(\epsilon\)</span>-Greedy</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/MAB/UCB.html">UCB</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/MAB/TS.html">TS</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario4/Contextual_Bandits/Contextual_Bandits.html">Contextual Bandits</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/Contextual_Bandits/LinUCB.html">LinUCB</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/Contextual_Bandits/LinTS.html">LinTS</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario4/Meta_Bandits/Meta_Bandits.html">Meta Bandits</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/Meta_Bandits/Meta_TS.html">Meta Thompson Sampling</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/Meta_Bandits/MTTS.html">Multi-Task Thompson Sampling (MTTS)</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario4/Structured_Bandits/Structured_Bandit.html">Structured Bandit (Slate Recommendation)</a><ul>
<li class="toctree-l2 has-children"><a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/Learning%20to%20rank.html">Online Learning to Rank (Cascading Bandit)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/TS_Cascade.html">TS_Cascade</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/CascadeLinTS.html">CascadeLinTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/Cascade/MTSS_Cascade.html">MTSS_Cascade</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/Combinatorial%20Optimization.html">Online Combinatorial Optimization (Combinatorial Semi-Bandit)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/CombTS.html">CombTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/CombLinTS.html">CombLinTS</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/Combinatorial-Semi/MTSS_Comb.html">MTSS_Comb</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l2 has-children"><a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/Assortment%20Optimization.html">Dynamic Assortment Optimization (Multinomial Logit Bandit)</a><ul>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/TS_MNL_Beta.html">TS_MNL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/TS_Contextual_MNL.html">TS_Contextual_MNL</a></li>
<li class="toctree-l3"><a class="reference internal" href="../Scenario4/Structured_Bandits/MNL/MTSS_MNL.html">MTSS_MNL</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1 has-children"><a class="reference internal" href="../Scenario4/OnlineEval/Online%20Policy%20Evaluation.html">Online Policy Evaluation</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/OnlineEval/Direct%20Online%20Policy%20Evaluator.html">Direct Online Policy Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/OnlineEval/Inverse%20Probability%20Weighted%20Online%20Policy%20Evaluator.html">Inverse Probability Weighted Online Policy Evaluator</a></li>
<li class="toctree-l2"><a class="reference internal" href="../Scenario4/OnlineEval/Doubly%20Robust%20Online%20Policy%20Evaluator.html">Doubly Robust Online Policy Evaluator</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 5</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Scenario5/OnlineRL_Markov.html">Online Policy Learning and Evaluation in Markovian Environments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Causal Policy Learning (CPL)--Paradigm 6</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../Scenario6/OnlineRL_non_Markov.html">Ooline Policy Learning in Non-Markovian Environments</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Case Studies</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="../../5_Case_Study/MIMIC3/MIMIC3_intro.html">Mimic3</a><ul>
<li class="toctree-l2"><a class="reference internal" href="../../5_Case_Study/MIMIC3/MIMIC3-Demo-Ver2.html">Mimic3 Demo-Ver2</a></li>


<li class="toctree-l2"><a class="reference internal" href="../../5_Case_Study/MIMIC3/Single_Stage.html">MIMIC III (Single-Stage)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../5_Case_Study/MIMIC3/Longitudinal.html">MIMIC III (3-Stages)</a></li>

<li class="toctree-l2"><a class="reference internal" href="../../5_Case_Study/MIMIC3/Infinite_Horizon.html">MIMIC III (Infinite Horizon)</a></li>
</ul><details><summary><span class="toctree-toggle" role="presentation"><i class="fa-solid fa-chevron-down"></i></span></summary>
</details></li>
<li class="toctree-l1"><a class="reference internal" href="../../5_Case_Study/MovieLens/MovieLens.html">MovieLens</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/CausalDM/Causal-Decision-Making/tree/main" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/CausalDM/Causal-Decision-Making/tree/main/issues/new?title=Issue%20on%20page%20%2F4_Causal_Policy_Learning/Scenario2/IPW_Infinite.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../../_sources/4_Causal_Policy_Learning/Scenario2/IPW_Infinite.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="theme-switch fa-solid fa-sun fa-lg" data-mode="light"></i>
    <i class="theme-switch fa-solid fa-moon fa-lg" data-mode="dark"></i>
    <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"></i>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm pst-navbar-icon search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Importance Sampling for Policy Evaluation (Infinite Horizon)</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-idea">Main Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-the-curse-of-horizon-with-stationary-distribution">Breaking the curse of horizon with stationary distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note">Note</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="importance-sampling-for-policy-evaluation-infinite-horizon">
<h1>Importance Sampling for Policy Evaluation (Infinite Horizon)<a class="headerlink" href="#importance-sampling-for-policy-evaluation-infinite-horizon" title="Permalink to this heading">#</a></h1>
<p>Another important approach is importance sampling (IS), also known as inverse propensity score or inverse propensity weighting methods.
IS has been widely used in statistics, and the idea can be extended to OPE after appropriately handling the temporal dependency.</p>
<p><em><strong>Advantages</strong></em>:</p>
<ol class="arabic simple">
<li><p>Conceptually simple and easy to implement</p></li>
<li><p>Low bias. Specifically, with known propensity scores, the vanilla version is unbiased.</p></li>
</ol>
<p><em><strong>Appropriate application situations</strong></em>:</p>
<p>Due to the large variance and the curse of horizon, IS generally performs well in problems with</p>
<ol class="arabic simple">
<li><p>Short horizon</p></li>
<li><p>Sufficient policy match between the behaviour policy and the target policy.</p></li>
</ol>
<section id="main-idea">
<span id="section-ipw-rl-main-idea"></span><h2>Main Idea<a class="headerlink" href="#main-idea" title="Permalink to this heading">#</a></h2>
<p>IS estimates the value by reweighting the observed rewards with importance ratios between the target and behavior policy <span id="id1">[<a class="reference internal" href="#id8" title="Doina Precup. Eligibility traces for off-policy policy evaluation. Computer Science Department Faculty Publication Series, pages 80, 2000.">Pre00</a>]</span>. For simplicity, we assume the behaviour policy <span class="math notranslate nohighlight">\(b\)</span> is known.</p>
<p>To begin with, for every trajectory index <span class="math notranslate nohighlight">\(i\)</span> and any <span class="math notranslate nohighlight">\(t \in \{0, 1, \dots, T - 1\}\)</span>, we define the <span class="math notranslate nohighlight">\(t\)</span>-step cumulative <strong>importance ratio</strong> between the target policy <span class="math notranslate nohighlight">\(\pi\)</span> and the behaviour policy <span class="math notranslate nohighlight">\(b\)</span> as</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{align*}
    \rho^i_t = \prod_{t'=0}^{t} \frac{\pi(A_{i,t'}|S_{i,t'})}{b(A_{i,t'}|S_{i,t'})}. 
\end{align*}\]</div>
<p>Since the transition and reward generation probabilities are shared  between both policies, this ratio is equal to the probability ratio of observing the <span class="math notranslate nohighlight">\(i\)</span>th trajectory until time point <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>The standard <strong>(trajectory-wise) IS</strong> estimator <span id="id2">[<a class="reference internal" href="#id9" title="Philip S Thomas. Safe reinforcement learning. Doctoral Dissertations at University of Massachusetts Amherst, 2015.">Tho15</a>]</span>
regards each trajectory (and the corresponding observed cumulative reward, <span class="math notranslate nohighlight">\(\sum_{t=0}^{T-1} \gamma^t R_{i,t}\)</span>) as one realization, and it estimates <span class="math notranslate nohighlight">\(\eta^{\pi}\)</span> by</p>
<div class="amsmath math notranslate nohighlight" id="equation-6650009a-a973-4cca-9513-1c4271dc293e">
<span class="eqno">(88)<a class="headerlink" href="#equation-6650009a-a973-4cca-9513-1c4271dc293e" title="Permalink to this equation">#</a></span>\[\begin{align}\label{eqn:IS}
    \hat{\eta}^{\pi}_{IS} = \frac{1}{n} \sum_{i=1}^n \rho^i_T (\sum_{t=0}^{T-1} \gamma^t R_{i,t}). 
\end{align}\]</div>
<p>In contrast, the <strong>step-wise IS</strong> <span id="id3">[<a class="reference internal" href="#id9" title="Philip S Thomas. Safe reinforcement learning. Doctoral Dissertations at University of Massachusetts Amherst, 2015.">Tho15</a>]</span>
focuses on reweighting each immediate reward <span class="math notranslate nohighlight">\(R_{i,t}\)</span> and typically yields a lower variance than the trajectory-wise IS. It is defined as</p>
<div class="amsmath math notranslate nohighlight" id="equation-fbf4014c-4c34-4aa7-8413-8eaba18654c5">
<span class="eqno">(89)<a class="headerlink" href="#equation-fbf4014c-4c34-4aa7-8413-8eaba18654c5" title="Permalink to this equation">#</a></span>\[\begin{align}\label{eqn:stepIS}
    \hat{\eta}^{\pi}_{StepIS} = \frac{1}{n} \sum_{i=1}^n \Big[ \sum_{t=0}^{T-1} \rho^i_t  \gamma^t R_{i,t} \Big]. 
\end{align}\]</div>
<p>In addition to these two IS-type estimators, their <strong>self-normalized variants</strong> are also commonly considered <span id="id4">[<a class="reference internal" href="#id10" title="Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In International Conference on Machine Learning, 652–661. PMLR, 2016.">JL16</a>]</span>.
Specifically, we can define the normalization factor <span class="math notranslate nohighlight">\(\bar{\rho}_t = N^{-1} \sum_{i=1}^N \rho^i_t\)</span>, and replace the <span class="math notranslate nohighlight">\(\rho^i_t\)</span> term by <span class="math notranslate nohighlight">\(\rho^i_t / \bar{\rho}_t\)</span>.
The resulting estimators are biased but consistent, and they generally yield lower variance than their counterparts.
This comparison reflects the bias-variance trade-off.</p>
</section>
<section id="breaking-the-curse-of-horizon-with-stationary-distribution">
<h2>Breaking the curse of horizon with stationary distribution<a class="headerlink" href="#breaking-the-curse-of-horizon-with-stationary-distribution" title="Permalink to this heading">#</a></h2>
<p>Traditional IS methods (and related DR methods) have exponential variance with the number of steps and hence will soon become unstable when the trajectory is long.  To avoid this issue,  <span id="id5">Liu <em>et al.</em> [<a class="reference internal" href="#id11" title="Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: infinite-horizon off-policy estimation. In Advances in Neural Information Processing Systems, 5356–5366. 2018.">LLTZ18</a>]</span>
made an important step forward by proposing to utilize the stationary distributions of the Markov process to marginalize the importance ratio. We need to assume the stationarity assumption (SA), that the state process <span class="math notranslate nohighlight">\(\{S_{i,t}\}_{t \ge 0}\)</span> is strictly stationary.</p>
<p>Let <span class="math notranslate nohighlight">\(p_b(s)\)</span> and  <span class="math notranslate nohighlight">\(p_b(s, a)\)</span> denote the stationary density function of the state and the state-action pair under the policy <span class="math notranslate nohighlight">\(b\)</span>, respectively.
The key observation is that, under the stationary assumption and when the data is weakly dependent, we can consider the importance ratios computed on each state-action pair rather than on each  trajectory, and hence break the curse of horizon.
We introduce the average visitation distribution under a policy <span class="math notranslate nohighlight">\(\pi\)</span> as <span class="math notranslate nohighlight">\(d^{\pi}(s)= (1 - \gamma)^{-1} \sum_{t=0}^{+\infty} \gamma^{t} p_t^{\pi}(s)\)</span>, where <span class="math notranslate nohighlight">\(p_t^{\pi}(s)\)</span> denotes the probability of <span class="math notranslate nohighlight">\(\{S_t = s\}\)</span> following policy <span class="math notranslate nohighlight">\(\pi\)</span> with  <span class="math notranslate nohighlight">\(S_{0}\sim \mathbb{G}\)</span>.
Define <span class="math notranslate nohighlight">\(\widetilde{\omega}^{\pi}(s) = d^{\pi}(s) / d^{b}(s)\)</span>.
Therefore, <span class="math notranslate nohighlight">\(\widetilde{\omega}^{\pi}(s)\)</span> can be understood as a marginalized version of the importance ratio. With a similar change-of-measure trick as in IS, we can obtain the relationship that</p>
<div class="amsmath math notranslate nohighlight" id="equation-9f32d9be-f04c-4c6b-ac7b-80378db4221c">
<span class="eqno">(90)<a class="headerlink" href="#equation-9f32d9be-f04c-4c6b-ac7b-80378db4221c" title="Permalink to this equation">#</a></span>\[\begin{equation}\label{eqn:breaking}
    \eta^{\pi} =  \mathbb{E}_{(s,a) \sim p_b(s, a), r \sim \mathcal{R}(\cdot; s, a)} \widetilde{\omega}^{\pi}(s) \frac{\pi(a|s)}{b(a|s)} r. 
\end{equation}\]</div>
<p>According to this relationship, we can construct an estimator by replacing the nuisance functions  with their estimates and then approximating the expectation by its empirical mean over <span class="math notranslate nohighlight">\(\{(S_{i,t},A_{i,t},R_{i,t},S_{i,t+1})\}\)</span>.
The nuisance function <span class="math notranslate nohighlight">\(\widetilde{\omega}^{\pi}(s)\)</span> is typically learned by solving an optimization problem, which we will omit to save space.
The optimization is similar to a relevant task that we will discuss in the next section, which is more related with our proposal.</p>
</section>
<section id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this heading">#</a></h2>
<div class="docutils container" id="id6">
<div class="citation" id="id10" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id4">JL16</a><span class="fn-bracket">]</span></span>
<p>Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In <em>International Conference on Machine Learning</em>, 652–661. PMLR, 2016.</p>
</div>
<div class="citation" id="id11" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id5">LLTZ18</a><span class="fn-bracket">]</span></span>
<p>Qiang Liu, Lihong Li, Ziyang Tang, and Dengyong Zhou. Breaking the curse of horizon: infinite-horizon off-policy estimation. In <em>Advances in Neural Information Processing Systems</em>, 5356–5366. 2018.</p>
</div>
<div class="citation" id="id8" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span><a role="doc-backlink" href="#id1">Pre00</a><span class="fn-bracket">]</span></span>
<p>Doina Precup. Eligibility traces for off-policy policy evaluation. <em>Computer Science Department Faculty Publication Series</em>, pages 80, 2000.</p>
</div>
<div class="citation" id="id9" role="doc-biblioentry">
<span class="label"><span class="fn-bracket">[</span>Tho15<span class="fn-bracket">]</span></span>
<span class="backrefs">(<a role="doc-backlink" href="#id2">1</a>,<a role="doc-backlink" href="#id3">2</a>)</span>
<p>Philip S Thomas. Safe reinforcement learning. <em>Doctoral Dissertations at University of Massachusetts Amherst</em>, 2015.</p>
</div>
</div>
</div>
</section>
<section id="note">
<h2>Note<a class="headerlink" href="#note" title="Permalink to this heading">#</a></h2>
<ol class="arabic simple">
<li><p>When the behaviour policy is unknown, we can estimate it from data by regarding the task as a classification problem and using methods such as logistic regression.</p></li>
<li><p>We note that, in principle, IS-based methods (and doubly robust methods to be reviewed in the next section) only apply to the finite-horizon setting, where the  trajectory is truncated at a finite time step <span class="math notranslate nohighlight">\(T\)</span>.
The estimand is
<span class="math notranslate nohighlight">\(\mathbb{E}^{\pi}_{s \sim \mathbb{G}} (\sum_{t=0}^{T-1} \gamma^t R_{t}|S_{0}=s)\)</span> instead of
<span class="math notranslate nohighlight">\(\mathbb{E}^{\pi}_{s \sim \mathbb{G}} (\sum_{t=0}^{+\infty} \gamma^t R_{t}|S_{0}=s)\)</span>.
However, when <span class="math notranslate nohighlight">\(T\)</span> is relatively large and <span class="math notranslate nohighlight">\(\gamma\)</span> is not quite close to <span class="math notranslate nohighlight">\(1\)</span>, the difference between <span class="math notranslate nohighlight">\(\sum_{t=0}^{T-1} \gamma^t\)</span> and <span class="math notranslate nohighlight">\(\sum_{t=0}^{\infty} \gamma^t\)</span> is negligible and is usually ignored, and they are still commonly used as baselines.</p></li>
<li><p>We note that (SA) is not a strong assumption. Recall that <span class="math notranslate nohighlight">\(\{S_{i,t}\}_{t \ge 0}\)</span> is generated by following the stationary policy <span class="math notranslate nohighlight">\(b\)</span>. (SA) is automatically  satisfied when the initial distribution equals the stationary distribution. Besides, When the MDP is a Harris ergodic chain , the process will eventually mix well and we can replace the stationary distribution with its limiting assumption and the following discussions will continue to hold.</p></li>
</ol>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./4_Causal_Policy_Learning\Scenario2"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="FQE.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Fitted-Q Evaluation</p>
      </div>
    </a>
    <a class="right-next"
       href="DR_Infinite.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Doubly Robust Estimator for Policy Evaluation (Infinite Horizon)</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#main-idea">Main Idea</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#breaking-the-curse-of-horizon-with-stationary-distribution">Breaking the curse of horizon with stationary distribution</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#references">References</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#note">Note</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Causal Decision Making Team
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2022.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../_static/scripts/bootstrap.js?digest=dfe6caa3a7d634c4db9b"></script>
<script src="../../_static/scripts/pydata-sphinx-theme.js?digest=dfe6caa3a7d634c4db9b"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>