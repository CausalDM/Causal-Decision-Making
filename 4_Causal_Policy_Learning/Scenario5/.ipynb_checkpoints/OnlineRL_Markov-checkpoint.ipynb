{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7fef41c9",
   "metadata": {},
   "source": [
    "(section:online_RL)=\n",
    "# Ooline Policy Learning and Evaluation in Markovian Environments \n",
    "\n",
    "This chapter focuses on the online policy learning and evaluation problem in an Markov Decision Process (MDP), which is the most well-known and typically default setup of Reinforcement Learning (RL). \n",
    "From the causal perspective, the data dependency structure is the same with that in [offline RL](section:OPE_OPO_preliminary), with the major difference in that the data collection policy is now data-dependent and the objective is sometimes shifted from finding the optimal policy to maximizing the cumulative rewards. \n",
    "As this is a vast area with a huge literature, we do not aim to repeat the disucssions hear. Instead, we will focus on connecting online RL to the other parts of this paper. \n",
    "We refer interested readers to {cite:t}`sutton2018reinforcement` for more materials.\n",
    "\n",
    "## Model\n",
    "\n",
    "We first recap the infinite-horizon discounted MDP model that we introduced in [offline RL](section:OPE_OPO_preliminary). \n",
    "For any $t\\ge 0$, let $\\bar{a}_t=(a_0,a_1,\\cdots,a_t)^\\top\\in \\mathcal{A}^{t+1}$ denote a treatment history vector up to time $t$. \n",
    "Let $\\mathbb{S} \\subset \\mathbb{R}^d$ denote the support of state variables and $S_0$ denote the initial state variable. \n",
    "For any $(\\bar{a}_{t-1},\\bar{a}_{t})$, let $S_{t}^*(\\bar{a}_{t-1})$ and $Y_t^*(\\bar{a}_t)$ be the counterfactual state and counterfactual outcome, respectively,  that would occur at time $t$ had the agent followed the treatment history $\\bar{a}_{t}$. \n",
    "The set of potential outcomes up to time $t$ is given by\n",
    "\\begin{eqnarray*}\n",
    "\tW_t^*(\\bar{a}_t)=\\{S_0,Y_0^*(a_0),S_1^*(a_0),\\cdots,S_{t}^*(\\bar{a}_{t-1}),Y_t^*(\\bar{a}_t)\\}.\n",
    "\\end{eqnarray*}\n",
    "Let $W^*=\\cup_{t\\ge 0,\\bar{a}_t\\in \\{0,1\\}^{t+1}} W_t^*(\\bar{a}_t)$ be the set of all potential outcomes.\n",
    "\n",
    "The goodness of  a policy $\\pi$ is measured by its value functions, \n",
    "\\begin{eqnarray*}\n",
    "    V^{\\pi}(s)=\\sum_{t\\ge 0} \\gamma^t \\mathbb{E} \\{Y_t^*(\\pi)|S_0=s\\}, \\;\\; \tQ^{\\pi}(a,s)=\\sum_{t\\ge 0} \\gamma^t \\mathbb{E} \\{Y_t^*(\\pi)|S_0=s, A_0 = a\\}. \n",
    "\\end{eqnarray*}\n",
    "\n",
    "We need two critical assumptions for the MDP model. \n",
    "\n",
    "**(MA) Markov assumption**:  there exists a Markov transition kernel $\\mathcal{P}$ such that  for any $t\\ge 0$, $\\bar{a}_{t}\\in \\{0,1\\}^{t+1}$ and $\\mathcal{S}\\subseteq \\mathbb{R}^d$, we have \n",
    "$\\mathbb{P}\\{S_{t+1}^*(\\bar{a}_{t})\\in \\mathcal{S}|W_t^*(\\bar{a}_t)\\}=\\mathcal{P}(\\mathcal{S};a_t,S_t^*(\\bar{a}_{t-1})).$\n",
    "\n",
    "**(CMIA) Conditional mean independence assumption**: there exists a function $r$ such that  for any $t\\ge 0, \\bar{a}_{t}\\in \\{0,1\\}^{t+1}$, we have \n",
    "$\\mathbb{E} \\{Y_t^*(\\bar{a}_t)|S_t^*(\\bar{a}_{t-1}),W_{t-1}^*(\\bar{a}_{t-1})\\}=r(a_t,S_t^*(\\bar{a}_{t-1}))$.\n",
    "\n",
    "\n",
    "## Policy Evaluation\n",
    "\n",
    "To either purely evaluate a policy or improve over it, we need to understand its performance (ideally at every state-action tuple), which corresponds to the policy value function estimation and evaluation problem. \n",
    "We introduce two main appraoches in the section. \n",
    "\n",
    "**Monte Carlo (MC).** In an online environment, the most straightforward approach is to just sample trajectories and use the average observed cumulative reward from sub-trajectories that satisfy our conditions as the estimator. \n",
    "For example, to estimate the value $V^{\\pi}(s)$ for a given state $s$, we can sample $N$ trajectories following $\\pi$, then find time points where we visit state $s$, and finally use the returns from then on to construct an average as our value estimate. \n",
    "\n",
    "Singh, S. P., Sutton, R. S. (1996). Reinforcement learning with replacing eligibility traces. Machine Learning, 22(1-3):123â€“158.\n",
    "\n",
    "\n",
    "\n",
    "**Temporal-Difference (TD) Learning.** \n",
    "One limitation of MC is that one has to wait until the end of a trajectory to collect a data point, which makes it less online and incremental. \n",
    "An alternative is to leverage the Bellman equation and the dynamic optimization structure, as we have utilized in [Paradigm 2](section:FQE). \n",
    "The is known as the Temporal-Difference (TD) Learning {cite:p}`sutton1988learning`. \n",
    "The name is from the fact that it involves the estimate at time point $t$ and $t+1). \n",
    "We first recall the Bellman equation:\n",
    "\n",
    "\\begin{equation}\\label{eqn:bellman_Q}\n",
    "    Q^\\pi(a, s) = \\mathbb{E}^\\pi \\Big(R_t + \\gamma Q^\\pi(A_{t + 1}, S_{t+1})  | A_t = a, S_t = s \\Big). \n",
    "\\end{equation}\n",
    "\n",
    "Therefore, suppose we currently have an Q-function estimate $\\hat{Q}^{\\pi}$. \n",
    "Then, after collecting a trasition tuple $(s, a, r, s')$, we can then update the estimate of $\\hat{Q}^{\\pi}(s, a)$ as \n",
    "\\begin{equation}\n",
    "    \\hat{Q}^\\pi(a, s) + \\alpha \\Big[r + \\gamma \\hat{Q}^\\pi(\\pi(s'), s')  - \\hat{Q}^\\pi(a, s) \\Big], \n",
    "\\end{equation}\n",
    "where $\\alpha$ is a learning rate. \n",
    "\n",
    "\n",
    "\n",
    "**Statistical inference.** As discussed in [Paradigm 4](section:Direct Online Policy Evaluator), statistical inference with adaptively collected data is challenging. \n",
    "To address that issue, {cite:t}`shi2020statistical` leverages a carefully designed data splitting schema to provide valid asymptotic distribution (and hence the confidence interval). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Policy Optimization\n",
    "\n",
    "**reference.**\n",
    "\n",
    "\n",
    "At a high-level\n",
    "\n",
    "exploration. \n",
    "policy evaluation. \n",
    "\n",
    "Policy gradient\n",
    "\n",
    "Actor critic\n",
    "\n",
    "Value-based \n",
    "\n",
    "1. DQN\n",
    "\n",
    "More references!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4871ef2-de01-4755-bd15-b8353e8f0dda",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## References\n",
    "\n",
    "```{bibliography}\n",
    ":filter: docname in docnames\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
