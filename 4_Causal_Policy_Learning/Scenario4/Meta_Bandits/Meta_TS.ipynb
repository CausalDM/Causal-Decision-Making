{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4b115ac-4e61-4c5d-a5a8-a849fd9b46b6",
   "metadata": {},
   "source": [
    "# Meta Thompson Sampling\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: When task instances are sampled from the same unknown instance prior (i.e., the tasks are similar), it efficiently learns the prior distribution of the mean potential rewards to achieve a regret bound that is comparable to that of the TS algorithm with known priors. \n",
    "- **Disadvantage**: When there is a large number of different tasks, the algorithm is not scalable and inefficient.\n",
    "- **Application Situation**: Useful when there are multiple **similar** multi-armed bandit tasks, each with the same action space. The reward space can be either binary or continuous.\n",
    "\n",
    "## Main Idea\n",
    "The **Meta-TS**[1] assumes that the mean potential rewards, $\\mu_{j,a} = E(R_{j,t}(a))$, for each task $j$ are i.i.d sampled from some distribution parameterized by $\\boldsymbol{\\gamma}$. Specifically, it assumes that\n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(meta-Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Prior)} \\quad\n",
    "\\; \\quad\\quad\\quad\\quad   \\boldsymbol{\\mu}_j | \\boldsymbol{\\gamma} &&\\sim g(\\boldsymbol{\\mu}_j | \\boldsymbol{\\gamma})\\\\\n",
    "&\\text{(Reward)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&\\sim f(Y_{j,t}(a)|\\mu_{j,a}).\n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "To learn the prior distribution of $\\boldsymbol{\\mu}_{j}$, it introduces a meta-parameter $\\boldsymbol{\\gamma}$ with a meta-prior distribution $Q(\\boldsymbol{\\gamma})$. The **Meta-TS** efficiently leverages the knowledge received from different tasks to learn the prior distribution and to guide the exploration of each task by maintaining the meta-posterior distribution of $\\boldsymbol{\\gamma}$. Theoretically, it is demonstrated to have a regret bound comparable to that of the Thompson sampling method with known prior distribution of $\\mu_{j,a}$. Both the \n",
    "\n",
    "Considering a Gaussian bandits, we assume that\n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(meta-Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Prior)} \\quad\n",
    "\\;  \\quad\\quad\\quad\\quad \\boldsymbol{\\mu}_j |\\boldsymbol{\\gamma} &&\\sim g(\\boldsymbol{\\mu}_j |\\boldsymbol{\\gamma})=\\boldsymbol{\\gamma}+ \\boldsymbol{\\delta}_{j}, \\\\\n",
    "&\\text{(Reward)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&= \\mu_{j,a} + \\epsilon_{j,t}, \n",
    "      \\end{alignedat}\n",
    "\\end{equation} where $\\boldsymbol{\\delta}_{j} \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(\\boldsymbol{0}, \\boldsymbol{\\Sigma})$, and $\\epsilon_{j,t} \\stackrel{i.i.d.}{\\sim} \\mathcal{N}(0, \\sigma^{2})$. The $\\boldsymbol{\\Sigma}$ and $\\sigma$ are both supposed to be known. A Gaussian meta-prior is employed by default with explicit forms of posterior distributions for simplicity. However, d ifferent meta-priors are welcome, with only minor modifications needed, such as using the **Pymc3** to accomplish posterior updating instead if there is no explicit form.\n",
    "\n",
    "Similarly, considering the Bernoulli bandits, we assume that \n",
    "\\begin{equation}\n",
    "  \\begin{alignedat}{2}\n",
    "&\\text{(meta-Prior)} \\quad\n",
    "\\quad\\quad\\quad    \\boldsymbol{\\gamma} &&\\sim Q(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Prior)} \\quad\n",
    "\\;  \\quad\\quad\\quad\\quad \\boldsymbol{\\mu}_j |\\boldsymbol{\\gamma} &&\\sim Beta(\\boldsymbol{\\gamma}), \\\\\n",
    "&\\text{(Reward)} \\quad\n",
    "\\;    R_{j,t}(a) = Y_{j,t}(a) &&= Bernoulli(\\mu_{j,a}). \n",
    "      \\end{alignedat}\n",
    "\\end{equation}\n",
    "While various meta-priors can be used, by default, we consider a finite space of $\\boldsymbol{\\gamma}$,\n",
    "\\begin{equation}\n",
    "\\mathcal{P} = \\{(\\alpha_{i,j})_{i=1}^{K}, (\\beta_{i,j})_{i=1}^{K}\\}_{j=1}^{L},\n",
    "\\end{equation}\n",
    "which contains **L** potential instance priors and assume a categorical distribution over the $\\mathcal{P}$ as the meta-prior. See [1] for more information about the corresponding meta-posterior updating.\n",
    "\n",
    "**Remark.** While the original system only supported a sequential schedule of interactions (i.e., a new task will not be interacted with until the preceding task is completed), we adjusted the algorithm to accommodate different recommending schedules. \n",
    "\n",
    "## Key Steps\n",
    "For $(j,t) = (0,0),(0,1),\\cdots$:\n",
    "1. Approximate the meta-posterior distribution $P(\\boldsymbol{\\gamma}|\\mathcal{H})$ either by implementing **Pymc3** or by calculating the explicit form of the posterior distribution;\n",
    "2. Sample $\\tilde{\\boldsymbol{\\gamma}} \\sim P(\\boldsymbol{\\gamma}|\\mathcal{H})$;\n",
    "3. Update $P(\\boldsymbol{\\mu}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H})$ and sample $\\tilde{\\boldsymbol{\\mu}} \\sim P(\\boldsymbol{\\mu}|\\tilde{\\boldsymbol{\\gamma}},\\mathcal{H})$;\n",
    "4. Take the action $A_{j,t}$ such that $A_{j,t} = argmax_{a \\in \\mathcal{A}} \\tilde\\mu_{j,a}$;\n",
    "6. Receive reward $R_{j,t}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "605a00b1-fb6e-48e3-8833-016e73f4b844",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "923aa1da-333e-45fc-8042-fcdf4885440e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('D:\\GitHub\\CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9aa339-34b1-49ee-9969-76dbba3fc9cd",
   "metadata": {},
   "source": [
    "### Import the learner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c672610-d25b-4e92-b7bd-8530a0a57cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from causaldm.learners.Online.Meta_Bandits import meta_TS_Gaussian"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5721c7db-20d4-4fe9-9f96-a4c3cd59686c",
   "metadata": {},
   "source": [
    "### Generate the Environment\n",
    "\n",
    "Here, we imitate an environment based on the MovieLens data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "294ae36c-70ce-4aae-9bc4-321196731042",
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Meta_Bandits import _env_realMultiTask as _env\n",
    "env = _env.MultiTask_Env(seed = 0, Binary = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c258d0c1-0b15-406e-a3a2-84790fe18524",
   "metadata": {},
   "source": [
    "### Specify Hyperparameters\n",
    "\n",
    "- `K`: number of arms\n",
    "- `N`: number of tasks\n",
    "- `sigma`: the standard deviation of the reward distributions\n",
    "- `sigma_0`: the standard deviation of the prior distribution of the mean reward of each arm\n",
    "- `sigma_q`: the standard deviation of the meta prior distribution\n",
    "- `order`: = 'episodic', if a sequential schedule is applied (Note: When order = 'episodic', meta-posterior is updated once a task is finished; otherwise, meta-posterior will be updated at every step)\n",
    "- `seed`: random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bddbee2-1aab-4ee3-8b3e-c181f8c26462",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = env.K\n",
    "N = env.N\n",
    "seed = 42\n",
    "order = 'episodic'\n",
    "sigma = 1\n",
    "sigma_0 = 1 \n",
    "sigma_q = 1\n",
    "\n",
    "meta_TS_Gaussian_agent = meta_TS_Gaussian.meta_TS_agent(sigma = sigma, sigma_0 = sigma_0, sigma_q = sigma_q,\n",
    "                                                        N = N, K = K, order = order, seed = seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6371d0b8-9cda-497f-88d9-f38078931001",
   "metadata": {},
   "source": [
    "### Recommendation and Interaction\n",
    "\n",
    "Starting from t = 0, for each step t, there are three steps:\n",
    "1. Recommend an action \n",
    "<code> A = LinTS_Gaussian_agent.take_action(X) </code>\n",
    "2. Get a reward from the environment \n",
    "<code> R = env.get_reward(t,A) </code>\n",
    "3. Update the posterior distribution of the mean reward of each arm\n",
    "<code> meta_TS_Gaussian_agent.receive_reward(i, t, A, R, episode_finished = False) </code>\n",
    "    - if a sequential schedule is applied and a task is finished, we would update the meta posterior by setting  `episode_finished = True`;\n",
    "    - if the schedule is not sequential, no specification of the parameter ` episode_finished` is needed, and the meta-posterior distribution would be updated at every step automatically"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ccf6940-7cc7-4e24-8439-8fa31de14907",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 3, 2.0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "t = 0\n",
    "A = meta_TS_Gaussian_agent.take_action(i,t)\n",
    "R = env.get_reward(i,t,A)\n",
    "meta_TS_Gaussian_agent.receive_reward(i, t, A, R, episode_finished = False)\n",
    "i,t,A,R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcc337c-9757-45c5-a2e2-d799b05d0a91",
   "metadata": {},
   "source": [
    "**Interpretation**: Interacting with Task 0, at step 0, the TS agent recommends a Thriller (arm 3) and receives a rate of 2 from the user.\n",
    "\n",
    "**Remark**: use `meta_TS_Gaussian_agent.meta_post` to get the most up-to-date meta-posterior; use `meta_TS_Gaussian_agent.posterior_u[i][A]` to get the most up-to-date posterior mean of $\\mu_{i,a}$ and `meta_TS_Gaussian_agent.posterior_cov_diag[i][A]` to get the corresponding posterior covariance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c81a5d-03df-464c-aa44-100bde8c1c23",
   "metadata": {},
   "source": [
    "### Demo Code for Bernoulli Bandit\n",
    "The steps are similar to those previously performed with a Gaussian Bandit. Note that, when specifying the prior distribution of the expected reward, the mean-precision form of the Beta distribution is used here, i.e., Beta($\\mu$, $\\phi$), where $\\mu$ is the mean reward of each arm and $\\phi$ is the precision of the Beta distribution. As we discussed before, we consider using a meta prior with L potential instance priors. Therefore, in implementing the meta-TS algorithm under Bernoulli bandits, we need to specify a list of candidate means of Beta priors. Specifically, three additional parameters are introduced:\n",
    "- `phi_beta`: the precision of Beta priors\n",
    "- `candi_means`: the candidate means of Beta priors\n",
    "- `Q`: categorical distribution of `candi_means`, i.e., each entry is the probability of each candidate mean being the true mean ($\\gamma$)\n",
    "- `update_freq`: if the recommending schedule is not sequential, we will update the meta-posterior every `update_freq` steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dba0abe-de5f-46e7-92be-99e492219a13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0, 1, 0)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from causaldm.learners.Online.Meta_Bandits import meta_TS_Binary\n",
    "env = _env.MultiTask_Env(seed = 0, Binary = True)\n",
    "K = env.K\n",
    "N = env.N\n",
    "seed = 42\n",
    "phi_beta = 1/4\n",
    "candi_means = [np.ones(K)*.1,np.ones(K)*.2,np.ones(K)*.3,np.ones(K)*.4,np.ones(K)*.5]\n",
    "Q =  np.ones(len(candi_means)) / len(candi_means)\n",
    "update_freq = 1\n",
    "order = 'episodic'\n",
    "\n",
    "meta_TS_Binary_agent = meta_TS_Binary.meta_TS_agent(phi_beta=phi_beta, candi_means = candi_means, Q = Q,\n",
    "                                                    N = N, K = K, update_freq=update_freq, order = order,\n",
    "                                                    seed = seed)\n",
    "i = 0\n",
    "t = 0\n",
    "A = meta_TS_Binary_agent.take_action(i,t)\n",
    "R = env.get_reward(i,t,A)\n",
    "meta_TS_Binary_agent.receive_reward(i, t, A, R, episode_finished = False)\n",
    "i,t,A,R"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f75b8a8-a9fa-4122-b7ba-f8386d0b459f",
   "metadata": {},
   "source": [
    "**Interpretation**: Interacting with Task 0, at step 0, the TS agent recommends a Drama (arm 1) and receives a rate of 0 from the user.\n",
    "\n",
    "**Remark**: use `meta_TS_Binary_agent.Q` to get the most up-to-date meta-posterior; use `(meta_TS_Binary_agent.posterior_alpha[i], meta_TS_Binary_agent.posterior_beta[i])` to get the most up-to-date posterior Beta($\\alpha$,$\\beta$) distribution of $\\mu_{i,a}$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e15d2146-d82c-4c02-a2a5-a1c3ad605cad",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Kveton, B., Konobeev, M., Zaheer, M., Hsu, C. W., Mladenov, M., Boutilier, C., & Szepesvari, C. (2021, July). Meta-thompson sampling. In International Conference on Machine Learning (pp. 5884-5893). PMLR.\n",
    "\n",
    "[2] Basu, S., Kveton, B., Zaheer, M., & Szepesvári, C. (2021). No regrets for learning the prior in bandits. Advances in Neural Information Processing Systems, 34, 28029-28041.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
