{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abbd3625",
   "metadata": {},
   "source": [
    "# Fitted-Q Evaluation\n",
    "\n",
    "The most straightforward approach for OPE is the direct method (DM). \n",
    "As suggested by the name, \n",
    "methods belonging to this category will first directly impose a model for either the environment or the Q-function, and then learn the model by regarding the task as a regression (or classification) problem, and finally calculate the value of the target policy via a plug-in estimator according to the definition of $\\eta^\\pi$\n",
    "The Q-function based approach and the environment-based approach are also called as model-free and  model-based, respectively. \n",
    "\n",
    "Among the many model-free DM estimators, we will focus on the most classic one, the fitted-Q evaluation (FQE) [1]. \n",
    "It is observed to perform consistently well in a large-scale empirical study [2]. \n",
    "\n",
    "***Advantages***:\n",
    "\n",
    "1. Conceptually simple and easy to implement\n",
    "2. Good numerical results when the the model class is chosen appropriately \n",
    "\n",
    "***Appropriate application situations***:\n",
    "\n",
    "Due to the potential bias, FQE generally performs well in problems where\n",
    "\n",
    "1. The model class can be chosen appropriately \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55cd7ae3",
   "metadata": {},
   "source": [
    "## Main Idea\n",
    "\n",
    "**Q-function.**\n",
    "The Q-function-based approach aims to direct learn the state-action value function (referred to as the Q-function) \n",
    "\\begin{eqnarray}\n",
    "Q^\\pi(a,s)&= \\mathbb{E}^{\\pi} (\\sum_{t=0}^{+\\infty} \\gamma^t R_{t}|A_{0}=a,S_{0}=s)   \n",
    "\\end{eqnarray}\n",
    "of the policy $\\pi$ that we aim to evaluate. \n",
    "\n",
    "The final estimator can then be constructed by plugging $\\hat{Q}^{\\pi}$ in the definition $\\eta^{\\pi} = \\mathbb{E}_{s \\sim \\mathbb{G}, a \\sim \\pi(\\cdot|s)} Q^{\\pi}(a, s)$. \n",
    "\n",
    "\n",
    "**Bellman equations.**\n",
    "The Q-learning-type evaluation is commonly based on the Bellman equation for the Q-function of a given policy $\\pi$ \n",
    "\\begin{equation}\\label{eqn:bellman_Q}\n",
    "    Q^\\pi(a, s) = \\mathbb{E}^\\pi \\Big(R_t + \\gamma Q^\\pi(A_{t + 1}, S_{t+1})  | A_t = a, S_t = s \\Big).  \\;\\;\\;\\;\\; \\text{(1)} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "**FQE.**\n",
    "FQE is mainly motivated by the fact that, the true value function $Q^\\pi$ is the unique solution to the Bellman equation (1). \n",
    "Besides, the right-hand side of (1) is a contraction mapping. \n",
    "Therefore, we can consider a fixed-point method: \n",
    "with an initial estimate $\\widehat{Q}^{0}$, \n",
    "FQE iteratively solves the following optimization problem, \n",
    "\n",
    "\n",
    "\\begin{eqnarray}\n",
    "\t\\widehat{Q}^{{\\ell}}=\\arg \\min_{Q} \n",
    "\t\\sum_{\\substack{i \\le n}}\\sum_{t<T}\n",
    "\t\\Big\\{\n",
    "\t\\gamma \\mathbb{E}_{a' \\sim \\pi(\\cdot| S_{i, t+1})} \\widehat{Q}^{\\ell-1}(a',S_{i, t+1}) \n",
    "\t+R_{i,t}- Q(A_{i, t}, S_{i, t})  \n",
    "\\Big\\}^2,\n",
    "\\end{eqnarray}\n",
    "\n",
    "\n",
    "for $\\ell=1,2,\\cdots$, until convergence. \n",
    "The final estimator is denoted as $\\widehat{Q}_{FQE}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc05347",
   "metadata": {},
   "source": [
    "## Demo [TODO]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "73f10eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('..')\n",
    "os.chdir('../CausalDM')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b43d451",
   "metadata": {},
   "source": [
    "## References\n",
    "[1] Le, H. M., Voloshin, C., and Yue, Y. Batch policy learning under constraints. arXiv preprint arXiv:1903.08738, 2019.\n",
    "\n",
    "[2] Voloshin C, Le H M, Jiang N, et al. Empirical study of off-policy policy evaluation for reinforcement learning[J]. arXiv preprint arXiv:1911.06854, 2019."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
