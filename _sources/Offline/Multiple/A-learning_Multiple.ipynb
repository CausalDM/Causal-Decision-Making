{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A-Learning (Multiple Stages)\n",
    "\n",
    "## Main Idea\n",
    "A-Learning, also known as Advantage Learning, is one of the main approaches to learning the optimal regime and works similarly to Q-learning. However, while Q-learning requires positing regression models to fit the expected outcome, A-learning models the contrasts between treatments and control, directly informing the optimal decision. For example, in the case of **Personalized Incentives**, A-learning aims to find the optimal incentive ($A$) for each user by modeling the difference in expected return-on-investment ($Y$) between treatments. A detailed comparison between Q-learning and A-learning can be found in [1]. While [1] mainly focus on the case with binary treatment options, a complete review of A-learning with multiple treatment options can be found in [2]. Here, following the algorithm in [1], we consider contrast-based A-learning. However, there is an alternative regret-based A-learning introduced in [3]. Some recent extensions to conventional A-learning, such as deep A-learning [4] and high-dimensional A-Learning [5], will be added soon. Overall, A-learning is doubly-robust. In other words, it is less sensitive and more robust to model misspecification. \n",
    "\n",
    "Note that, we assume the action space is either **binary** (i.e., 0,1) or **multinomial** (i.e., 0,1,2,3,4, where 0 stands for the control group by convention), and the outcome of interest Y is **continuous** and **non-negative**, where the larger the $Y$ the better. \n",
    "\n",
    "## Algorithm Details\n",
    "Suppose there are $m_t$ number of options at decision point $t$, and the corresponding action space $\\mathcal{A}_t=\\{0,1,\\dots,m_t-1\\}$. At each decision point $t$, contrast-based A-learning aims to learn and estimate the constrast function, $C_{tj}(h_{t}), j=1,2,\\dots,m_t-1$. Here, $h_{t}=\\{X_{1i},A_{1i},\\cdots,X_{ti}\\})$ includes all the information observed till step t. Furthermore, we also need to posit a model for the conditional expected outcome for the control option (treatment $0$), $Q_t(h_t,0)$, and the propensity function $\\omega(h_{t},a_{t})$. Detailed definitions are provided in the following:\n",
    "*   Q-function:\n",
    "    For the final step $T$, \n",
    "    \\begin{align}\n",
    "    Q_T(h_T,a_{T})=E[Y|H_{T}=h_{T}, A_{T}=a_{T}],\n",
    "    \\end{align}\n",
    "    \n",
    "    If there is a multi-stage case with total step $T>1$, for the step $t=1,\\cdots,T-1$,\n",
    "    \\begin{align}\n",
    "    Q_t(h_t,a_{t})=E[V_{t+1}|H_{t}=h_{t}, A_{t}=a_{t}],\n",
    "    \\end{align}\n",
    "    where \n",
    "    \\begin{align}\n",
    "    V_{t}(h_{t}) = \\max_{j\\in\\mathcal{A}_t}Q_{t}(h_t,j)\n",
    "    \\end{align}\n",
    "    Alternatively, with the contrast function $C_{tj}(h_t)$, which will be defined later,\n",
    "    \\begin{align}\n",
    "    Q_t(h_t,j) = Q_t(h_t,0) + C_{tj}(h_t),\\quad j=0,\\dots,m_t-1,\\quad t=1,\\dots,T.\n",
    "    \\end{align}\n",
    "*   Contrast functions (optimal blip to zero functions)\n",
    "    \\begin{align}\n",
    "    C_{tj}(h_t)=Q_t(h_t,j)-Q_t(h_t,0),\\quad j=0,\\dots,m_t-1,\\quad t=1,\\dots,T,\n",
    "    \\end{align}\n",
    "    where $C_{t0}(h_t) = 0$.\n",
    "*   Propensity score\n",
    "    \\begin{align}\n",
    "    \\omega_{t}(h_t,a_t)=P(A_t=a_t|H_t=h_t)\n",
    "    \\end{align}\n",
    "*   Optimal regime\n",
    "    \\begin{align}\n",
    "    d_t^{opt}(h_t)=\\arg\\max_{j\\in\\mathcal{A}_t}C_{tj}(h_t)\n",
    "    \\end{align}\n",
    "\n",
    "A backward approach was proposed to find the optimized treatment regime at each decision point. \n",
    "\n",
    "At Decision $T$, similar as what we did previously with single decision point, we estimate the $\\psi_{Tj}$, $\\phi_T$ and $\\gamma_T$ by solving the eqautions in A.1 jointly, and the optimal decision at time $T$ is calculated accordingly. \n",
    "\n",
    "Then, at Decision $t=T-1,\\dots,1$, we use similar trick as decision $T$, except for changing $Y$ in the estimating eqautions to some pseudo outcome $\\tilde{Y}_{t+1,i}$, such that:\n",
    "\\begin{align}\n",
    "\\tilde{Y}_{ti}=\\tilde{Y}_{t+1,i}+\\max_{j=0,\\dots,m_t-1}C_{tj}(h_{ti},\\hat{\\psi}_{tj})-\\sum_{j=1}^{m_k-1}\\mathbb{I}\\{A_{ti}=j\\}C_{tj}(h_{ti},\\hat{\\psi}_{tj}),\n",
    "\\end{align}\n",
    "where $\\tilde{Y}_{T+1,i} = Y_{i}$.\n",
    "    \n",
    "Estimating $\\psi_{tj}$, $\\phi_t$ and $\\gamma_t$ iteratively for $t=T-1,\\cdots,1$, we calculated the optimal decision at time $t$, $d^{opt}_{t}(h_{ti})$ as\n",
    "\\begin{align}\n",
    "d^{opt}_{t}(h_{ti})=\\arg\\max_{j=0,\\dots,m_t-1} C_{tj}(h_{ti};\\hat{\\psi}_{tj}).\n",
    "\\end{align}\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "**Policy Learning:**\n",
    "1. At the final decision point $t=T$, fitted a model $\\omega_{T}(h_{T},a,\\hat{\\gamma}_{T})$, and estimating $\\psi_{Tj}$, $\\phi_{T}$ by solving the equations in A.2 jointly;\n",
    "2. For each individual $i$, calculated the pseudo-outcome $\\tilde{Y}_{Ti}$, and the optimal action $a_{Ti}$;\n",
    "3. For decision point $t = T-1,\\cdots, 1$,\n",
    "    1. fitted a model $\\omega_{t}(h_{t},a,\\hat{\\gamma}_{t})$, and estimating $\\psi_{tj}$, $\\phi_{t}$ by solving the equations in A.2 jointly with the pseudo-outcome $\\tilde{Y}_{t+1}$\n",
    "    2. For each individual $i$, calculated the pseudo-outcome $\\tilde{Y}_{ti}$, and the optimal action $d_t^{opt}(h_ti)$;\n",
    "\n",
    "**Policy Evaluation:**    \n",
    "We use the backward iteration as what we did in policy learning. However, here for each round, the pseudo outcome is not the maximum of Q values. Instead, the pseudo outcome at decision point t is defined as below:\n",
    "\\begin{align}\n",
    "\\tilde{Y}_{ti}=\\tilde{Y}_{t+1,i}+C_{tj*}(h_{ti},\\hat{\\psi}_{tj*})-\\sum_{j=1}^{m_k-1}\\mathbb{I}\\{A_{ti}=j\\}C_{tj}(h_{ti},\\hat{\\psi}_{tj}),\n",
    "\\end{align}\n",
    "where $j*=d(H_{ti})$, and $d$ is the fixed regime that we want to evaluate.\n",
    "The estimated value of the policy is then the average of $\\tilde{Y}_{1}$.\n",
    "\n",
    "**Note** we also provide an option for bootstrapping. Particularly, for a given policy, we utilze the boostrap resampling to get the estimated value of the regime and the corresponding estimated standard error. \n",
    "\n",
    "## Demo Code\n",
    "In the following, we exhibit how to apply the learner on real data to do policy learning and policy evaluation, respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Policy Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A demo with code on how to use the package\n",
    "from causaldm.learners import ALearning\n",
    "from causaldm.test import shared_simulation\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataMDP = pd.read_csv(\"dataMDP_feasible.txt\", sep=',')\n",
    "Y = np.array(dataMDP['Y'])\n",
    "X = np.hstack([np.ones((len(Y),1)),np.array(dataMDP[['CD4_0','CD4_6','CD4_12']])])\n",
    "A = np.array(dataMDP[['A1','A2','A3']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ALearn = ALearning.ALearning()\n",
    "model_info = [{'X_prop': list(range(2)),\n",
    "              'X_q0': list(range(2)),\n",
    "               'X_C':{1:list(range(2))},\n",
    "              'action_space': {'A1':[0,1]}},\n",
    "             {'X_prop': list(range(3)),\n",
    "              'X_q0': list(range(3)),\n",
    "               'X_C':{1:list(range(3))},\n",
    "              'action_space': {'A2':[0,1]}},\n",
    "             {'X_prop': list(range(4)),\n",
    "              'X_q0': list(range(4)),\n",
    "               'X_C':{1:list(range(4))},\n",
    "              'action_space': {'A3':[0,1]}}]\n",
    "# train the policy\n",
    "ALearn.train(X, A, Y, model_info, T=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitted contrast model: {2: {1: array([ 3.5872e+02, -1.0493e+00,  4.9347e-03, -5.2010e-02])}, 1: {1: array([-214.568 ,    1.1057,   -0.62  ])}, 0: {1: array([-9.8412e+01,  9.2479e-02])}}\n",
      "opt regime:    A3  A2  A1\n",
      "0   0   0   0\n",
      "1   0   0   0\n",
      "2   0   0   0\n",
      "3   0   0   0\n",
      "4   0   0   0\n",
      "opt value: 1162.4662578531918\n"
     ]
    }
   ],
   "source": [
    "# recommend action\n",
    "opt_d = ALearn.recommend_action(X).head()\n",
    "# get the estimated value of the optimal regime\n",
    "V_hat = ALearn.predict_value(X)\n",
    "print(\"fitted contrast model:\",ALearn.fitted_model['contrast'])\n",
    "print(\"opt regime:\",opt_d)\n",
    "print(\"opt value:\",V_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 1162.3156513295758 Value_std: 4.559003921896037\n",
      "estimated_contrast: {1:          Mean         std\n",
      "0 -120.245571  198.595014\n",
      "1    0.143433    0.440232}\n"
     ]
    }
   ],
   "source": [
    "# Optional: we also provide a bootstrap standard deviaiton of the optimal value estimation\n",
    "# Warning: results amay not be reliable\n",
    "ALearn = ALearning.ALearning()\n",
    "model_info = [{'X_prop': list(range(2)),\n",
    "              'X_q0': list(range(2)),\n",
    "               'X_C':{1:list(range(2))},\n",
    "              'action_space': {'A1':[0,1]}},\n",
    "             {'X_prop': list(range(3)),\n",
    "              'X_q0': list(range(3)),\n",
    "               'X_C':{1:list(range(3))},\n",
    "              'action_space': {'A2':[0,1]}},\n",
    "             {'X_prop': list(range(4)),\n",
    "              'X_q0': list(range(4)),\n",
    "               'X_C':{1:list(range(4))},\n",
    "              'action_space': {'A3':[0,1]}}]\n",
    "ALearn.train(X, A, Y, model_info, T=3, bootstrap = True, n_bs = 100)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.predict_value_boots(X)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)\n",
    "##estimated contrast model at t = 0\n",
    "print('estimated_contrast:',params[0]['contrast'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1162.4662578531918"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#specify the fixed regime to be tested\n",
    "# For example, regime d = 0 for all subjects\n",
    "N, p = X.shape\n",
    "ALearn = ALearning.ALearning()\n",
    "# regime should be in the same format as A, which is a dict\n",
    "regime = pd.DataFrame({'A1':np.array([0]*N),'A2':np.array([0]*N),'A3':np.array([0]*N)})\n",
    "model_info = [{'X_prop': list(range(2)),\n",
    "              'X_q0': list(range(2)),\n",
    "               'X_C':{1:list(range(2))},\n",
    "              'action_space': {'A1':[0,1]}},\n",
    "             {'X_prop': list(range(3)),\n",
    "              'X_q0': list(range(3)),\n",
    "               'X_C':{1:list(range(3))},\n",
    "              'action_space': {'A2':[0,1]}},\n",
    "             {'X_prop': list(range(4)),\n",
    "              'X_q0': list(range(4)),\n",
    "               'X_C':{1:list(range(4))},\n",
    "              'action_space': {'A3':[0,1]}}]\n",
    "ALearn.train(X, A, Y, model_info, T=3, regime = regime, evaluate = True)\n",
    "ALearn.predict_value(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value_hat: 1162.3966192806022 Value_std: 4.626837283714682\n"
     ]
    }
   ],
   "source": [
    "# bootstrap average and the std of estimate value\n",
    "ALearn.train(X, A, Y, model_info, T=3, regime = regime, evaluate = True, bootstrap = True, n_bs = 200)\n",
    "fitted_params,fitted_value,value_avg,value_std,params=ALearn.predict_value_boots(X)\n",
    "print('Value_hat:',value_avg,'Value_std:',value_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "💥 Placeholder for C.I."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "1. Schulte, P. J., Tsiatis, A. A., Laber, E. B., & Davidian, M. (2014). Q-and A-learning methods for estimating optimal dynamic treatment regimes. Statistical science: a review journal of the Institute of Mathematical Statistics, 29(4), 640.\n",
    "2. Robins, J. M. (2004). Optimal structural nested models for optimal sequential decisions. In Proceedings of the second seattle Symposium in Biostatistics (pp. 189-326). Springer, New York, NY.\n",
    "3. Murphy, S. A. (2003). Optimal dynamic treatment regimes. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 65(2), 331-355.\n",
    "4. Liang, S., Lu, W., & Song, R. (2018). Deep advantage learning for optimal dynamic treatment regime. Statistical theory and related fields, 2(1), 80-88.\n",
    "5. Shi, C., Fan, A., Song, R., & Lu, W. (2018). High-dimensional A-learning for optimal dynamic treatment regimes. Annals of statistics, 46(3), 925."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A.1\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial C_{Tj}(H_{Ti};\\psi_{Tj})}{\\partial \\psi_{Tj}}\\{\\mathbb{I}\\{A_{Ti}=j\\}-\\omega_T(H_{Ti},j;\\gamma_T)\\}\\times \\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_{T})\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial Q_T(H_{Ti},0;\\phi_T)}{\\partial \\phi_T}\\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_T)\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial \\omega_T(H_{Ti},j;\\gamma_T)}{\\partial \\gamma_T}\\Big\\{Y_i-\\sum_{j'=1}^{m_T-1} \\mathbb{I}\\{A_{Ti}=j'\\}C_{Tj'}(H_{Ti};\\psi_{Tj'})-Q_T(H_{Ti},0;\\phi_T)\\Big\\}\\right]=0\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "## A.2\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial C_{tj}(H_{ti};\\psi_{tj})}{\\partial \\psi_{tj}}\\{\\mathbb{I}\\{A_{ti}=j\\}-\\omega_T(H_{ti},j;\\gamma_t)\\}\\times \\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_{t})\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial Q_t(H_{ti},0;\\phi_t)}{\\partial \\phi_t}\\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_t)\\Big\\}\\right]=0\\\\\n",
    "&\\sum_{i=1}^n \\left[\\frac{\\partial \\omega_t(H_{ti},j;\\gamma_t)}{\\partial \\gamma_t}\\Big\\{\\tilde{Y}_{t+1,i}-\\sum_{j'=1}^{m_t-1} \\mathbb{I}\\{A_{ti}=j'\\}C_{tj'}(H_{ti};\\psi_{tj'})-Q_t(H_{ti},0;\\phi_t)\\Big\\}\\right]=0\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
