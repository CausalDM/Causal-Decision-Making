{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# TS_MNL\n",
    "\n",
    "## Overview\n",
    "- **Advantage**: In practice, it always outperforms algorithms that also do not use features but are based on other frameworks, such as UCB.\n",
    "- **Disadvantage**: When there are a large number of items, it is not scalable.\n",
    "- **Application Situation**: Useful when a list of items is presented, each with a matching price or income, and only one is chosen for each interaction. Binary responses from users include click/don't-click and buy/don't-buy.\n",
    "\n",
    "## Main Idea\n",
    "The first TS-based algorithm is developed by [1]. Noticing that direct inference under the standard multinomila logit model is intractable due to the complex dependency of the reward distribution on action slate $a$, an epoch-based algorithmic structure is introduced and is being more popular in recent bandit literature [1,2,3]. Under the epoch-type offering framework,  \n",
    "\\begin{align}\n",
    "    Y_{i}^l(a) &\\sim Geometric(\\theta_i), \\forall i \\in a,\\\\\n",
    "    R^l(a) &= \\sum_{i\\in a}Y_{i}^l(a)\\eta_{i}.\n",
    "\\end{align} \n",
    "Taking the advantage of the nice conjugate relationship between the geometric distribution and the Beta distribution, the TS-based algorithm **TS_MNL** [1] is tractable and computationally efficient. Assuming a Beta prior over parameters $\\theta_{i}$, at each epoch $l$, **TS_MNL** updates the posterior distribution of $\\theta_{i}$ according to the property of the Beta-Geometric conjugate pair, from which we then sample a $\\tilde{\\theta}_{i}^{l}$, and $\\tilde{v}_{i}^{l}$ is calculated directly as $\\tilde{v}_{i}^{l}=1/\\tilde{\\theta}_{i}^{l}-1$. Finally, the optimal assortment $A^{l}$ is determined efficiently through linear programming [1], such that\n",
    "\\begin{equation}\n",
    "    A^{l} = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid\\tilde{\\boldsymbol{v}})=argmax_{a \\in \\mathcal{A}} \\frac{\\sum_{i\\in a}\\eta_{i}\\tilde{v}_{i}}{1+\\sum_{j\\in a} \\tilde{v}_{j}},\n",
    "\\end{equation} where $t$ is the first round of epoch $l$.  It should be noted that the posterior updating step differs for different pairs of the prior distribution of $\\theta_i$ and the reward distribution, and the code can be easily modified to different prior/reward distribution specifications if necessary.\n",
    "\n",
    "\n",
    "## Key Steps\n",
    "1. Specifying a prior distirbution of each $\\theta_i$, i.e., Beta(1,1).\n",
    "2. For l = $0, 1,\\cdots$:\n",
    "    - sample a $\\tilde{\\theta}^{l}$ from the posterior distribution of $\\theta$ or prior distribution if in epoch $0$\n",
    "    - compute the utility $\\tilde{v}_{i}^{l} = \\frac{1}{\\tilde{\\theta}_{i}^{l}}-1$;\n",
    "    - at the first round $t$ of epoch $l$ select top $K$ items by linear programming such that $A^{l} = A_t = arg max_{a \\in \\mathcal{A}} E(R_t(a) \\mid \\tilde{\\boldsymbol{v}}^{l})$\n",
    "    - keep offering $A^{l}$ untile no-purchase appears\n",
    "    - receive the rewad $R^l$, and update the posterior distirbution accordingly.\n",
    "    \n",
    "*Notations can be found in either the inroduction of the chapter \"Structured Bandits\" or the introduction of the Multinomial Logit Bandit problems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# After we publish the pack age, we can directly import it\n",
    "# TODO: explore more efficient way\n",
    "# we can hide this cell later\n",
    "import os\n",
    "os.getcwd()\n",
    "os.chdir('/nas/longleaf/home/lge/CausalDM')\n",
    "# code used to import the learner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from causaldm.learners.Online.Slate.MNL import MTSS_MNL\n",
    "from causaldm.learners.Online.Slate.MNL import _env_MNL\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "T = 20000\n",
    "L = 1000\n",
    "update_freq = 500\n",
    "update_freq_linear = 500\n",
    "\n",
    "phi_beta = 1/4\n",
    "n_init = 500\n",
    "with_intercept = True\n",
    "same_reward = True\n",
    "p=3\n",
    "K=5\n",
    "X_mu = np.zeros(p-1)\n",
    "X_sigma = np.identity(p-1)\n",
    "Sigma_gamma = sigma_gamma = np.identity(p)\n",
    "mu_gamma = np.zeros(p)\n",
    "seed = 0\n",
    "\n",
    "env = _env_MNL.MNL_env(L, K, T, mu_gamma, sigma_gamma, X_mu, X_sigma,                                       \n",
    "                        phi_beta, same_reward = same_reward, \n",
    "                        seed = seed, p = p, with_intercept = with_intercept)\n",
    "MTSS_agent = MTSS_MNL.MTSS_MNL(L, env.r, K, env.Phi, phi_beta = phi_beta,n_init = n_init,\n",
    "                                    gamma_prior_mean = mu_gamma, gamma_prior_cov = Sigma_gamma,\n",
    "                                    update_freq=update_freq, seed = seed, pm_core = 1, same_reward = same_reward, clip = True)\n",
    "S = MTSS_agent.take_action(env.Phi)\n",
    "t = 1\n",
    "c, exp_R, R = env.get_reward(S)\n",
    "MTSS_agent.receive_reward(S, c, R, exp_R)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([426, 394, 715, 213, 285])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "[1] Agrawal, S., Avadhanula, V., Goyal, V., & Zeevi, A. (2017, June). Thompson sampling for the mnl-bandit. In Conference on Learning Theory (pp. 76-78). PMLR.\n",
    "\n",
    "[2] Agrawal, S., Avadhanula, V., Goyal, V., & Zeevi, A. (2019). Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5), 1453-1485.\n",
    "\n",
    "[3] Dong, K., Li, Y., Zhang, Q., & Zhou, Y. (2020, November). Multinomial logit bandit with low switching cost. In International Conference on Machine Learning (pp. 2607-2615). PMLR."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
