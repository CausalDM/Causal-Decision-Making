{"cells":[{"cell_type":"markdown","source":["### **5. DR-learner**\n","\n","DR-learner is a two-stage doubly robust estimator for HTE estimation. Before Kennedy et al. 2020 [4], there are several related approaches trying to extend the doubly robust procedure to HTE estimation, such as [5, 6, 7]. Compared with the above three estimators, DR-learner is proved to be oracle efficient under some mild assumptions detailed in Theorem 2 of [4].\n","\n","The basic steps of DR-learner is given below:\n","\n","**Step 1**: Nuisance training: \\\\\n","(a)  Using $I_{1}^n$ to construct estimates $\\hat{\\pi}$ for the propensity scores $\\pi$; \\\\\n","(b)  Using $I_{1}^n$ to construct estimates $\\hat\\mu_a(s)$ for $\\mu_a(s):=\\mathbb{E}[R|S=s,A=a]$;\n","\n","**Step 2**: Pseudo-outcome regression: \\\\\n","Define $\\widehat{\\phi}(Z)$ as the pseudo-outcome where \n","\\begin{equation}\n","\\widehat{\\phi}(Z)=\\frac{A-\\hat{\\pi}(S)}{\\hat{\\pi}(S)\\{1-\\hat{\\pi}(S)\\}}\\Big\\{R-\\hat{\\mu}_A(S)\\Big\\}+\\hat{\\mu}_1(S)-\\hat{\\mu}_0(S),\n","\\end{equation}\n","and regress it on covariates $S$ in the test sample $I_2^n$, yielding \n","\\begin{equation}\n","\\widehat{\\tau}_{\\text{DR-learner}}(s)=\\widehat{\\mathbb{E}}_n[\\widehat{\\phi}(Z)|S=s].\n","\\end{equation}\n"],"metadata":{"id":"J2z2JRumRzdo"},"id":"J2z2JRumRzdo"},{"cell_type":"code","source":["import sys\n","!{sys.executable} -m pip install scikit-uplift"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676134075357,"user_tz":300,"elapsed":4287,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"9e32343f-1b89-4ab6-eca3-8a02680d80af","id":"VBreyjCSYdHy"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting scikit-uplift\n","  Downloading scikit_uplift-0.5.1-py3-none-any.whl (42 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.16 in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.21.6)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.3.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (3.2.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (2.25.1)\n","Requirement already satisfied: scikit-learn>=0.21.0 in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (1.0.2)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.8/dist-packages (from scikit-uplift) (4.64.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (3.1.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.2.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-learn>=0.21.0->scikit-uplift) (1.7.3)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (3.0.9)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (0.11.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.8/dist-packages (from matplotlib->scikit-uplift) (1.4.4)\n","Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->scikit-uplift) (2022.7.1)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (2022.12.7)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->scikit-uplift) (2.10)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.1->matplotlib->scikit-uplift) (1.15.0)\n","Installing collected packages: scikit-uplift\n","Successfully installed scikit-uplift-0.5.1\n"]}],"id":"VBreyjCSYdHy"},{"cell_type":"code","source":["# import related packages\n","from matplotlib import pyplot as plt\n","from lightgbm import LGBMRegressor\n","from sklearn.linear_model import LinearRegression\n","from sklearn.linear_model import LogisticRegression \n","from causaldm._util_causaldm import *\n","from causaldm.learners.Causal_Effect_Learning.Single_Stage.DRlearner import DRlearner"],"metadata":{"id":"7_g6QQ8wXcb2"},"execution_count":null,"outputs":[],"id":"7_g6QQ8wXcb2"},{"cell_type":"code","source":["n = 10**3  # sample size in observed data\n","n0 = 10**5 # the number of samples used to estimate the true reward distribution by MC\n","seed=223"],"metadata":{"id":"vq-ZNkGjXcb7"},"execution_count":null,"outputs":[],"id":"vq-ZNkGjXcb7"},{"cell_type":"code","source":["# Get data\n","data_behavior = get_data_simulation(n, seed, policy=\"behavior\")\n","#data_target = get_data_simulation(n0, seed, policy=\"target\")\n","\n","# The true expected heterogeneous treatment effect\n","HTE_true = get_data_simulation(n, seed, policy=\"1\")['R']-get_data_simulation(n, seed, policy=\"0\")['R']\n","\n"],"metadata":{"id":"E4tdbzu6Xcb8"},"execution_count":null,"outputs":[],"id":"E4tdbzu6Xcb8"},{"cell_type":"code","source":["# DR-learner for HTE estimation\n","outcome = 'R'\n","treatment = 'A'\n","controls = ['S1','S2']\n","n_folds = 5\n","y_model = LGBMRegressor(max_depth=2)\n","ps_model = LogisticRegression()\n","Rlearner_model = LGBMRegressor(max_depth=2)\n","\n","HTE_DR_learner = DRlearner(data_behavior, outcome, treatment, controls, y_model, ps_model)\n","HTE_DR_learner = HTE_DR_learner.to_numpy()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675479895278,"user_tz":300,"elapsed":540,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"3383f83c-5015-4dd5-a756-529361a17346","id":"i_F-3H7NFBHZ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["estimate with DR-learner\n","fold 1, testing r2 baselearner: 0.980, pslearner: 0.943\n","fold 2, testing r2 baselearner: 0.978, pslearner: 0.947\n","fold 3, testing r2 baselearner: 0.975, pslearner: 0.942\n","fold 4, testing r2 baselearner: 0.978, pslearner: 0.946\n","fold 5, testing r2 baselearner: 0.978, pslearner: 0.940\n"]}],"id":"i_F-3H7NFBHZ"},{"cell_type":"code","source":["print(\"DR-learner:  \",HTE_DR_learner[0:8])\n","print(\"true value: \",HTE_true[0:8].to_numpy())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675479896943,"user_tz":300,"elapsed":158,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"2bda9075-e9e8-48e2-dd06-fbab8b7ff0f2","id":"0-u2xNvpFBHZ"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DR-learner:   [-1.2566  0.0408 -0.8131 -0.0906 -0.5665 -0.7341 -0.6459 -1.272 ]\n","true value:  [ 1.2961 -0.4475  0.731   0.2863  0.4471 -0.1839 -3.3869 -1.238 ]\n"]}],"id":"0-u2xNvpFBHZ"},{"cell_type":"code","source":["Bias_DR_learner = np.sum(HTE_DR_learner-HTE_true)/n\n","Variance_DR_learner = np.sum((HTE_DR_learner-HTE_true)**2)/n\n","print(\"The overall estimation bias of DR-learner is :     \", Bias_DR_learner, \", \\n\", \"The overall estimation variance of DR-learner is :\",Variance_DR_learner,\". \\n\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1675479898442,"user_tz":300,"elapsed":163,"user":{"displayName":"Yang Xu","userId":"12270366590264264299"}},"outputId":"926b5a81-e86d-4d5f-f9da-9f5f0b61f258","id":"Yvb360k8FBHa"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["The overall estimation bias of DR-learner is :      0.29436318987432813 , \n"," The overall estimation variance of DR-learner is : 4.011818461500106 . \n","\n"]}],"id":"Yvb360k8FBHa"},{"cell_type":"markdown","metadata":{"id":"1098b550"},"source":["## References\n","\n","2. Xinkun Nie and Stefan Wager. Quasi-oracle estimation of heterogeneous treatment effects. Biometrika, 108(2):299–319, 2021.\n","\n","3. Peter M Robinson. Root-n-consistent semiparametric regression. Econometrica: Journal of the Econometric Society, pages 931–954, 1988.\n","\n","4. Edward H Kennedy. Optimal doubly robust estimation of heterogeneous causal effects. arXiv preprint arXiv:2004.14497, 2020\n","\n","5. M. J. van der Laan. Statistical inference for variable importance. The International Journal of Biostatistics, 2(1), 2006.\n","\n","6. S. Lee, R. Okui, and Y.-J. Whang. Doubly robust uniform confidence band for the conditional average treatment effect function. Journal of Applied Econometrics, 32(7):1207–1225, 2017.\n","\n","7. D. J. Foster and V. Syrgkanis. Orthogonal statistical learning. arXiv preprint arXiv:1901.09036, 2019."],"id":"1098b550"}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"colab":{"provenance":[],"collapsed_sections":["1098b550"]}},"nbformat":4,"nbformat_minor":5}