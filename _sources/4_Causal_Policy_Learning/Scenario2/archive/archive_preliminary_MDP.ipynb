{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d51efee",
   "metadata": {},
   "source": [
    "# Preliminary: Off-policy Evaluation and Optimization in Markov Decision Processes\n",
    "\n",
    "\n",
    "## Markov Decision Process\n",
    "As the underlying data generation model for RL, \n",
    "we consider an infinite-horizon discounted Markov Decision Process MDP [1] defined by a tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)$. Here, $\\mathcal{S}$ is the state space, $\\mathcal{A}$ is the action space, \n",
    "$\\mathcal{P}$ is the transition kernel with $\\mathcal{P}(s'| s, a)$ giving the mass function (or probability density) of entering state $s'$ by taking action $a$ in state $s$, \n",
    "$\\mathcal{R}$ is the reward kernel with $\\mathcal{R}(r|s, a)$ denoting the mass function (or probability density) of receiving a reward $r$ after taking action $a$ in state $s$, \n",
    "and $\\gamma \\in (0, 1)$ is a discounted factor that balances the immediate and future rewards. \n",
    "To simplify the presentation, we assume the spaces are discrete throughout this report. \n",
    "Meanwhile, most discussions continue to hold with continuous spaces as well. \n",
    "Following a given policy $\\pi$, when the current state is $s$, \n",
    "the agent will select action $a$ with probability $\\pi(a|s)$. \n",
    "\n",
    "\n",
    "Let $\\{(S_t,A_t,R_t)\\}_{t\\ge 0}$ denote a trajectory generated from the MDP model where $(S_t,A_t,R_t)$ denotes the state-action-reward triplet at time $t$. \n",
    "A trajectory following policy $\\pi$ is generated as follows. \n",
    "The agent starts from a state $S_0$ sampled from the initial state distribution,  denoted by $\\mathbb{G}$. \n",
    "At each time point $t \\ge 0$, the agent will select an action $A_{t}$ following $\\pi(\\cdot|S_{t})$, and then receive a random reward $R_{t}$ following  $\\mathcal{R}(\\cdot|S_{t}, A_{t})$, and finally moves to the next state  $S_{t+1}$ following $\\mathcal{P}(\\cdot; S_{t}, A_{t})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2f878d",
   "metadata": {},
   "source": [
    "## Value functions\n",
    "\n",
    "\n",
    "\n",
    "The state value function (referred to as the V-function) and state-action value function (referred to as the Q-function) for a policy $\\pi$ are given as follows:\n",
    "\\begin{align}\n",
    "\tV^\\pi(s)&=\\mathbb{E}^{\\pi} (\\sum_{t=0}^{+\\infty} \\gamma^t R_{t}|S_{0}=s),\\\\\n",
    "\tQ^\\pi(a,s)&= \\mathbb{E}^{\\pi} (\\sum_{t=0}^{+\\infty} \\gamma^t R_{t}|A_{0}=a,S_{0}=s) \\label{eqn:Q},\n",
    "\\end{align}\n",
    "where the expectation $\\mathbb{E}^{\\pi}$ is defined by assuming the system follows the policy $\\pi$. \n",
    "The observed discounted cumulative reward for a trajectory $\\{(S_t,A_t,R_t)\\}_{t\\ge 0}$ is $\\sum_{t=0}^{\\infty} \\gamma^t R_t$. \n",
    "The optimal policy is defined as $\\pi^* = \\arg \\max_{\\pi} V^\\pi(s), \\forall s \\in \\mathcal{S}$. \n",
    "\n",
    "Finally, we are ready to introduce the well-known Bellman equation [2]. Note that the definition of the Q function implies that \n",
    "\\begin{align*}\n",
    "    Q^\\pi(a,s)\n",
    "    = \\mathbb{E}^{\\pi} (R_{0} + \\sum_{t=1}^{+\\infty} \\gamma^t R_{t}|A_{0}=a,S_{0}=s)\n",
    "    &= \\mathbb{E}^{\\pi} (R_{0} + \\gamma \\sum_{t=0}^{+\\infty} \\gamma^t R_{t+1}|A_{0}=a,S_{0}=s)\\\\\n",
    "    &=  \\mathbb{E}^\\pi \\Big(R_t + \\gamma Q^{\\pi}(A_{t + 1}, S_{t+1})  | A_t = a, S_t = s \\Big), \n",
    "\\end{align*}\n",
    "where the last equality follows from the stationarity of the MDP. \n",
    "Motivated by this fact, \n",
    "the Bellman equation for the Q-function is defined as\n",
    "\\begin{equation}\\label{eqn:bellman_Q}\n",
    "    Q(a, s) = \\mathbb{E}^\\pi \\Big(R_t + \\gamma Q(A_{t + 1}, S_{t+1})  | A_t = a, S_t = s \\Big), \n",
    "\\end{equation}\n",
    "for which $Q^\\pi$ is the unique solution [2]. \n",
    "The Bellman equation relates different components of the MDP via a recursive form and is the foundation for lots of RL algorithms. \n",
    "\n",
    "Similarly, we have the Bellman optimality equation, which characterizes the optimal policy $\\pi^*$ and is commonly used in policy optimization. \n",
    "Specifically, its value function $Q^*$ is the unique solution of \n",
    "\\begin{equation}\n",
    "    Q(a, s) = \\mathbb{E} \\Big(R_t + \\gamma \\arg \\max_{a'} Q(a, S_{t+1})  | A_t = a, S_t = s \\Big).  \\;\\;\\;\\;\\; \\text{(2)} \n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "## Off-policy Evaluation and Optimization\n",
    "\n",
    "In the off-policy setting, the observed data consists of $n$ i.i.d. trajectories $\\{(S_{i,t},A_{i,t},R_{i,t},S_{i,t+1})\\}_{0\\le t<T_i,1\\le i\\le n}$, where $T_i$ denotes the length of the $i$th trajectory. Without loss of generality, we assume $T_1=\\cdots=T_n=T$ and the immediate rewards are uniformly bounded. \n",
    "The dataset is collected by following a stationary policy $b$, known as the *behavior policy*. \n",
    "\n",
    "**Off-Policy Evaluation(OPE).** The goal of OPE is to estimate the value of a given *target policy* $\\pi$ with respect to the initial state distribution $\\mathbb{G}$, defined as \n",
    "\\begin{eqnarray}\\label{eqn:def_value}\n",
    "\t\\eta^{\\pi} =  \\mathbb{E}_{s \\sim \\mathbb{G}} V^{\\pi}(s). \n",
    "\\end{eqnarray} \n",
    "By definition, we directly have $\\eta^{\\pi} = \\mathbb{E}_{s \\sim \\mathbb{G}, a \\sim \\pi(\\cdot|s)} Q^{\\pi}(a, s)$. \n",
    "\n",
    "In addition to a point estimator, many applications would benefit from having a CI for $\\eta^{\\pi}$. \n",
    "We refer to an interval $[\\hat{\\eta}^{\\pi}_l, \\hat{\\eta}^{\\pi}_u]$ as an $(1-\\alpha)$-CI for $\\eta^{\\pi}$ if and only if $P(\\hat{\\eta}^{\\pi}_l \\le \\eta^{\\pi} \\le \\hat{\\eta}^{\\pi}_u) \\ge 1 - \\alpha$, for any $\\alpha \\in (0, 1)$.  \n",
    "\n",
    "**Off-Policy Optimization(OPO).** The goal of OPO is to solve the optimal policy $\\pi^*$, or in other words, to learn a policy $\\hat{\\pi}$ so as to minimize the regret $\\eta^{\\pi^*} - \\eta^{\\hat{\\pi}}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e57a2dc-2118-44f2-9a68-355b485c5145",
   "metadata": {},
   "source": [
    "## Reference\n",
    "\n",
    "[1] Puterman M L. Markov decision processes: discrete stochastic dynamic programming[M]. John Wiley & Sons, 2014.\n",
    "\n",
    "[2] Sutton R S, Barto A G. Reinforcement learning: An introduction[M]. MIT press, 2018."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21af44a9-137e-43b3-8c68-e92a3ac56b8d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
